{"paperId": "9225cea96b18566ed714264e4102b8c65743779b", "publicationVenue": {"id": "2633f5b2-c15c-49fe-80f5-07523e770c26", "name": "IEEE Access", "type": "journal", "issn": "2169-3536", "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"]}, "title": "C2FHAR: Coarse-to-Fine Human Activity Recognition With Behavioral Context Modeling Using Smart Inertial Sensors", "abstract": "Smart sensing devices are furnished with an array of sensors, including locomotion sensors, which enable continuous and passive monitoring of human activities for the ambient assisted living. As a result, sensor-based human activity recognition has earned significant popularity in the past few years. A lot of successful research studies have been conducted in this regard. However, the accurate recognition of in-the-wild human activities in real-time is still a fundamental challenge to be addressed as human physical activity patterns are adversely affected by their behavioral contexts. Moreover, it is essential to infer a user\u2019s behavioral context along with the physical activity to enable context-aware and knowledge-driven applications in real-time. Therefore, this research work presents \u201cC2FHAR\u201d, a novel approach for coarse-to-fine human activity recognition in-the-wild, which explicitly models the user\u2019s behavioral contexts with activities of daily living to learn and recognize the fine-grained human activities. For addressing real-time activity recognition challenges, the proposed scheme utilizes a multi-label classification model for identifying in-the-wild human activities at two different levels, i.e., coarse or fine-grained, depending upon the real-time use-cases. The proposed scheme is validated with extensive experiments using heterogeneous sensors, which demonstrate its efficacy.", "venue": "IEEE Access", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-01-06", "journal": {"name": "IEEE Access", "pages": "7731-7747", "volume": "8"}, "authors": [{"authorId": "1404961030", "name": "M. Ehatisham-ul-Haq"}, {"authorId": "33738614", "name": "M. A. Azam"}, {"authorId": "103092106", "name": "Y. Amin"}, {"authorId": "1888283", "name": "U. Naeem"}], "citations": [{"paperId": "d198659537d5c1ebcb272fa459b3a854bd331e81", "title": "Multimodal Sensor Data Fusion and Ensemble Modeling for Human Locomotion Activity Recognition"}, {"paperId": "5313333592d218bbd54f47e281bb66a6f2696fe1", "title": "Human Action Recognition in Smart Living Services and Applications: Context Awareness, Data Availability, Personalization, and Privacy"}, {"paperId": "d382ebd278c6d7b2eba8eda384cdb709bf0f6fa9", "title": "A Framework for Daily Living Activity Recognition using Fusion of Smartphone Inertial Sensors Data"}, {"paperId": "25dad04ee87e974b23fdfcf507fc42e2d85fdd69", "title": "Human activity recognition based on multi\u2010instance learning"}, {"paperId": "be4a1159136078f2c2ce17e0827455334eeb8f63", "title": "Human Activity Recognition Berdasarkan Tangkapan Webcam Menggunakan Metode Convolutional Neural Network (CNN) Dengan Arsitektur MobileNet"}, {"paperId": "9a402d186261c0b3bfadb5e1f0ddd37c6c5210c4", "title": "Human activity recognition from sensor data using spatial attention-aided CNN with genetic algorithm"}, {"paperId": "1237f5738ca777e28ee6910cec4d4c9f733990cd", "title": "An Activity Management System for Office Workers Using Multimodal Data *"}, {"paperId": "16318df5de8ef02c05e5b363f0f3d19189db13d2", "title": "Context-Aware Edge-Based AI Models for Wireless Sensor Networks\u2014An Overview"}, {"paperId": "caf73741a53da460dd10467c5349fd3e6abbd6fa", "title": "Simultaneous Recognition Algorithm of Human Activity and Phone Position Based on Multi-sensor Data Fusion"}, {"paperId": "95eb866b9bdf186cd4e1a4dc2b56d92693accc8b", "title": "FL-PMI: Federated Learning-Based Person Movement Identification through Wearable Devices in Smart Healthcare Systems"}, {"paperId": "ccd561625ae82694965d6cbc724086d5f0e00db9", "title": "Human activity recognition in artificial intelligence framework: a narrative review"}, {"paperId": "4d02e382c3b8669c4c02e6d49cd3bbb95f5b5524", "title": "Studi Literatur Human Activity Recognition (HAR) Menggunakan Sensor Inersia"}, {"paperId": "5bf22aa73616ddb5766ab61de179c43a82806327", "title": "Recognizing Human-Object Interaction (HOI) Using Wrist-Mounted Inertial Sensors"}, {"paperId": "e074eab6d64842a1bde3a74a028182a5ba2a91e5", "title": "S-THAD: a framework for sensor-based temporal human activity detection from continuous data streams"}, {"paperId": "c0aa919677d318e8ee3ed2a67e4a300333fac3f9", "title": "Fall Detection of Riders using Inertial Sensors: A Smart Helmet"}, {"paperId": "b465c7b0412d3dd0167da24d794eb04e0b5fe5ef", "title": "Particle Swarm Optimization of Convolutional Neural Networks for Human Activity Prediction"}, {"paperId": "c28bb99f81feb1519165e6a10195a510a5d00a8d", "title": "A Smart Chair Design for Recognizing Human-Object Interactions using Pressure Sensors"}, {"paperId": "82f79388994492e49446e92ba29cd4760d55bbaf", "title": "Dynamic User Activity Prediction using Contextual Service Matching Mechanism"}, {"paperId": "0da8ed93e42c106c99aea4e95b730f03a7562dd7", "title": "What Did Our Model Just Learn? Hard Lessons in Applying Deep Learning to Human Factors Data"}]}
