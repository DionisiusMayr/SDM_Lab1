{"paperId": "42005450eb70475d38d757c40025fc2b908c4a5a", "publicationVenue": null, "title": "Hyperscale Data Processing with Network-centric Designs", "abstract": "One of the most fundamental tasks in computer science is to process data in a timely manner. It is critical for nearly every computing workload and affects aspects of our lives as diverse as health (e.g., tracking outbreaks in a pandemic), finance (e.g., fast trading and quantitative analysis), education (e.g., large-scale resource sharing), and entertainment (e.g., massive online services and recommendations). Unfortunately, unprecedented data growth has meant that achieving good performance is increasingly challenging for all of the components involved: the infrastructure must provide massive amounts of resources, data processing systems must utilize the resources efficiently, and applications sometimes need different algorithm designs. My long-term goal is to make the processing of data in any form efficient at any scale. To that end, I have researched and published across the data processing stack, from novel data science applications [1] to large-scale data processing systems [2, 3, 4, 5] and their underlying infrastructure [6, 7, 8, 9]. These papers have appeared at database, networking, and systems conferences. My dissertation research focuses on cloud data centers, where today\u2019s largest data processing workloads are hosted. Over the years, these workloads have ballooned to hyperscale level, encompassing billions to trillions of data items and hundreds to thousands of machines per query. Enabling and expanding with these workloads are highly scalable data center networks that connect up to hundreds of thousands of networked servers. At hyperscale, many traditional design principles break down. One example is the classic principle of layering, in which different components of a network, from applications to the transport and hardware, are built independently as layers and connected by well-specified protocols. Layering has allowed people working on different components to focus on their own systems with clear optimization goals. However, layered designs are no longer sustainable at hyperscale. For instance, without knowing how their data is transferred in the network, applications can make egregious decisions in their execution models; the cloud infrastructure also performs poorly without rethinking the interfaces and services exposed to its applications. In fact, most cloud providers already break layering in their current architectures, e.g., creating custom networking stacks and hardware that are tailored to their most common applications. The research community has not kept up, due to lack of systematic investigations on applying cross-layer designs to address hyperscale challenges in data processing. My research aims to fix this by bridging data processing systems and data center networks. My approach is principled and concerns three questions and associated challenges as follows. How do data processing systems perform in current networks? Developers of data processing systems treat the network as a black box that simply delivers messages from point A to point B. This assumption greatly simplifies the design of distributed data processing: the execution engine can ignore details like the physical placement of distributed workers. The network has also traditionally tried to support this assumption with a \u201cone big switch\u201d abstraction that it provides to applications. It incurs a great cost to maintain the assumption at scale because a data processing job that requires hundreds of machines must necessarily span multiple racks or even clusters. Instead, today\u2019s data center networks are commonly oversubscribed due to cost considerations, meaning that the cross-rack and cross-cluster network performance is significantly worse than that within racks and clusters. Such data center network characteristics are important for large-scale data processing but rarely considered in prior systems. In an effort to address these challenges, I built GraphRex [3], the first system that processes hyperscale data by systematically adopting network awareness. These ideas demonstrated orders of magnitude better performance than the state of the art. How will they perform in future networks? Disaggregated data centers (DDCs) are a promising new cloud proposal that decouples different types of resources from monolithic servers into resource pools, e.g., compute/CPU pool, memory pool, and storage pool, and connects these pools by a high-speed network. Compared to traditional server architectures, disaggregation offers vast operational benefits. It solves the traditional binpacking problem when packing virtual machines (VMs) to physical machines by making resource allocation independent, potentially saving billion-dollar resource waste. DDCs also make data center expansion easier as different resources can be added and managed independently. In addition, DDCs achieve better failure isolation and provide easy elasticity for applications. Despite these benefits, this new cloud architecture can potentially", "venue": "", "year": 2021, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2112197162", "name": "Qizhen Zhang"}], "citations": []}
