{"paperId": "32f65dd751e5c491d83c2f46e41f50afa1c6d2bb", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Scaling Law of Large Sequential Recommendation Models", "abstract": "Scaling of neural networks has recently shown great potential to improve the model capacity in various fields. Specifically, model performance has a power-law relationship with model size or data size, which provides important guidance for the development of large-scale models. However, there is still limited understanding on the scaling effect of user behavior models in recommender systems, where the unique data characteristics (e.g. data scarcity and sparsity) pose new challenges to explore the scaling effect in recommendation tasks. In this work, we focus on investigating the scaling laws in large sequential recommendation models. Specially, we consider a pure ID-based task formulation, where the interaction history of a user is formatted as a chronological sequence of item IDs. We don't incorporate any side information (e.g. item text), because we would like to explore how scaling law holds from the perspective of user behavior. With specially improved strategies, we scale up the model size to 0.8B parameters, making it feasible to explore the scaling effect in a diverse range of model sizes. As the major findings, we empirically show that scaling law still holds for these trained models, even in data-constrained scenarios. We then fit the curve for scaling law, and successfully predict the test loss of the two largest tested model scales. Furthermore, we examine the performance advantage of scaling effect on five challenging recommendation tasks, considering the unique issues (e.g. cold start, robustness, long-term preference) in recommender systems. We find that scaling up the model size can greatly boost the performance on these challenging tasks, which again verifies the benefits of large recommendation models.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-19", "journal": {"name": "ArXiv", "volume": "abs/2311.11351"}, "authors": [{"authorId": "2267502484", "name": "Gaowei Zhang"}, {"authorId": "151472453", "name": "Yupeng Hou"}, {"authorId": "2266804776", "name": "Hongyu Lu"}, {"authorId": "2266788301", "name": "Yu Chen"}, {"authorId": "2257376413", "name": "Wayne Xin Zhao"}, {"authorId": "153693432", "name": "Ji-rong Wen"}], "citations": [{"paperId": "13965d8d68217308ff8c7e738ced637739c6a1b8", "title": "Bridging Language and Items for Retrieval and Recommendation"}]}
