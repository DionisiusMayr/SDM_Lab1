{"paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e", "publicationVenue": {"id": "bdc2e585-4e48-4e36-8af1-6d859763d405", "name": "AAAI Conference on Artificial Intelligence", "type": "conference", "alternate_names": ["National Conference on Artificial Intelligence", "National Conf Artif Intell", "AAAI Conf Artif Intell", "AAAI"], "url": "http://www.aaai.org/"}, "title": "Preference Ranking Optimization for Human Alignment", "abstract": "Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT. (2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise contrast, thus lacking contrasts from a macro perspective. In this paper, we propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast to accommodate preference rankings of any length. By iteratively contrasting candidates, PRO instructs the LLM to prioritize the best response while progressively ranking the rest responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses through automatic-based, reward-based, GPT-4, and human evaluations.", "venue": "AAAI Conference on Artificial Intelligence", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-06-30", "journal": {"name": "ArXiv", "volume": "abs/2306.17492"}, "authors": [{"authorId": "66947198", "name": "Feifan Song"}, {"authorId": "48613402", "name": "Yu Bowen"}, {"authorId": "123545597", "name": "Minghao Li"}, {"authorId": "46493167", "name": "Haiyang Yu"}, {"authorId": "143857288", "name": "Fei Huang"}, {"authorId": "1527090216", "name": "Yongbin Li"}, {"authorId": "1781885", "name": "Houfeng Wang"}], "citations": [{"paperId": "3714bb3aac3f0429cd62e315da0ffb3f79655f98", "title": "Regularized Conditional Diffusion Model for Multi-Task Preference Alignment"}, {"paperId": "d9ef2a095d49f738cb7b538ffb86f79b31f32213", "title": "Robust Preference Optimization with Provable Noise Tolerance for LLMs"}, {"paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641", "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"}, {"paperId": "8115ffbbadd1055424d18369dba66ce32a572800", "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback"}, {"paperId": "d2be1c0f537adc9ffec42510bc84d2f478ee2402", "title": "Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model"}, {"paperId": "279dbec2194f05ee0e9d49818eff5fbe5f61b2ee", "title": "Understanding the Learning Dynamics of Alignment with Human Feedback"}, {"paperId": "4aa8b64c51e987d703045023cd72b3ba5e115f99", "title": "Assessment of Multimodal Large Language Models in Alignment with Human Values"}, {"paperId": "ce13bba037382e240411dcb16105012f42335387", "title": "CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment"}, {"paperId": "c9e4af8b55db19f9bc9359806ca9283053366fe5", "title": "Improving the Robustness of Large Language Models via Consistency Alignment"}, {"paperId": "b3cdde45d87a3a90d86ebe20c1ee8c4ef0150d1c", "title": "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment"}, {"paperId": "e33bd615b021ba90f87e02c036c3acae834d3893", "title": "Authorship Style Transfer with Policy Optimization"}, {"paperId": "973814cd535facbf4f27c3de477b05bf19366030", "title": "ORPO: Monolithic Preference Optimization without Reference Model"}, {"paperId": "4146b447187e1a09b736564854007c403f986c69", "title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling"}, {"paperId": "66e7edf09589527ebb58418632418758cee668cd", "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"}, {"paperId": "5678a1ee9d5542785115555b856e51a1dd9eb0e9", "title": "\"In Dialogues We Learn\": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning"}, {"paperId": "8c785ebee1f34464dbc85ab4113bccafd7a74b0a", "title": "DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling"}, {"paperId": "8be81d531dfc4a1145474a1bb2f9c0cf15e19f45", "title": "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration"}, {"paperId": "405daa547e62fd5a0d0c69e06908324f3bc74893", "title": "A Language Model's Guide Through Latent Space"}, {"paperId": "8832f073c4d5d7ae26d6b5252b00bf8d8531f2e6", "title": "LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study"}, {"paperId": "2df14dafc8486ff51575f8327159da1a021054b5", "title": "Large Language Models for Data Annotation: A Survey"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "de6ddb30b07f192f2be142062c4c6c817e508d96", "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation"}, {"paperId": "08436b3ddafd2edc798753ebc87f6ceffed6e8df", "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models"}, {"paperId": "041ab8f72343db5d50769eeb725398c689b2850c", "title": "ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization"}, {"paperId": "37a00c43bd1099a8bdef978ab0aa6d2566cefad0", "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards"}, {"paperId": "35b142ea69598e6241f0011312128031df55895c", "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"}, {"paperId": "b899a28eb553800ce558cf8974a697f65103e591", "title": "DeAL: Decoding-time Alignment for Large Language Models"}, {"paperId": "abdb6e912fe86a60600b438b0e36b502f6412b24", "title": "Vaccine: Perturbation-aware Alignment for Large Language Model"}, {"paperId": "f977dac98cc603bfccae6ea991cf4b1f83bf139c", "title": "LiPO: Listwise Preference Optimization through Learning-to-Rank"}, {"paperId": "329a949f3d67fa345db5f7b807faef022e92d364", "title": "Preference-free Alignment Learning with Regularized Relevance Reward"}, {"paperId": "5edcbf3ea678a7d9977555ecd59370452667597a", "title": "Towards Efficient and Exact Optimization of Language Model Alignment"}, {"paperId": "ca47c63a97848e60389037c93a4feb2daf849c3e", "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF"}, {"paperId": "1491e2d5d8ba63132ff157e47e824af76c422450", "title": "ARGS: Alignment as Reward-Guided Search"}, {"paperId": "6d9d552af11f333b56158b4c4a3ccc236820eca1", "title": "GRATH: Gradual Self-Truthifying for Large Language Models"}, {"paperId": "de4dfb773ab455081e5fb1862e08f581c58d43bc", "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems"}, {"paperId": "cdd0e94e51a02bac22ca5e94fa95daa18f36e226", "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles"}, {"paperId": "485f8a429cf5f70c558181187f2d62e31784deaa", "title": "Reasons to Reject? Aligning Language Models with Judgments"}, {"paperId": "1f1638ca845881545f364ca23b1fae46f729e72f", "title": "ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference"}, {"paperId": "ce316ce4671e38019582f7be7a6e87b4c3909b57", "title": "RLHF and IIA: Perverse Incentives"}, {"paperId": "0f9995ec08e95bea09d512c59e40d19f0f44d7bb", "title": "Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language"}, {"paperId": "215de09ac6e5de81187c85065b5ace8bc01f2862", "title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models"}, {"paperId": "9331818a22f1b80b397b4de8f0742403e2588436", "title": "Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values"}, {"paperId": "a8568bac56c24b5d25e373a117f947171d5f97be", "title": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering"}, {"paperId": "abf1ddd4f48b2cee7e5c71ee4609f07209189c75", "title": "Large Language Model based Long-tail Query Rewriting in Taobao Search"}, {"paperId": "c0d698950a4560fc2a63acb30a91aa2deb042ed3", "title": "Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions"}, {"paperId": "9c0102443a1b5adc0c2235fab23a80bf8122ce72", "title": "CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment"}, {"paperId": "42016f91e5b1da63174d45acb96bc89b64aa124d", "title": "Knowledge Editing for Large Language Models: A Survey"}, {"paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1", "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"}, {"paperId": "2be910eb19f2f8f2e8038d2a835bc48f868ccbf1", "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models"}, {"paperId": "9bf00afb0efb02a263fa3ddea1e768677498536c", "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"}, {"paperId": "6f217d984f36499d88ab8a3d89572171552e6f3f", "title": "Evaluating Large Language Models at Evaluating Instruction Following"}, {"paperId": "ad8182bfb70b3b67a60571ba720cdfc00b8e1392", "title": "Constructive Large Language Models Alignment with Diverse Feedback"}, {"paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b", "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"}, {"paperId": "ba015c5d3f5b44e36363b90070bb3301d21ae57e", "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?"}, {"paperId": "b1e2abc63630f26be54a1931041e0f4eeb0434e0", "title": "Enabling Language Models to Implicitly Learn Self-Improvement"}, {"paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3", "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "749d59f887c8ac83fd4f5178465e8b03e463358c", "title": "Large Language Model Alignment: A Survey"}, {"paperId": "0fb61be60088e80e565b84f44e49ba30630b6126", "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal"}, {"paperId": "64ad8e62544cca34a9714cbc79af8c56807310fa", "title": "Reward Engineering for Generating Semi-structured Explanation"}, {"paperId": "22ab4219371366a4e890382bc0ca606130840ca7", "title": "Statistical Rejection Sampling Improves Preference Optimization"}, {"paperId": "09e55d5223104aa5726ff0fcf4043a1beeee410e", "title": "Beyond Traditional Teaching: The Potential of Large Language Models and Chatbots in Graduate Engineering Education"}, {"paperId": "74b4b993babe99bc5f5c589c27fef0f1baba606b", "title": "Making Large Language Models Better Reasoners with Alignment"}, {"paperId": "c12db2e67d1fb289266faa5507ff112c9a062465", "title": "Efficient RLHF: Reducing the Memory Usage of PPO"}, {"paperId": "b931b242f40a032b9ae7dae9d9fc10c6ab90695e", "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models"}, {"paperId": "ac1788e9a168a6455beb6316f316950842297c11", "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities"}, {"paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7", "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"}, {"paperId": "dd3fb89d1201d46fa80b6a9519114599c01c11ac", "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"}, {"paperId": "29288d4f7f7e2e0b18d88c6ce0681fd181a18c68", "title": "Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length"}, {"paperId": "c74a13b251b6af6dfce49eeb128b1c0e2ddf955d", "title": "A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment"}, {"paperId": "91206346edbe28abb606d7b3425cd455d4019d4f", "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"}, {"paperId": "3705919b880f4f8dc37483a704e14dd078cb9ac4", "title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators"}, {"paperId": "a53a747ed6c7c8090d9542ed51d4f65103ac35ad", "title": "Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges"}, {"paperId": "ca31b8584b6c022ef15ddfe994fe361e002b7729", "title": "A Comprehensive Overview of Large Language Models"}, {"paperId": "60d90e96e7c434861697194fa47f1978d86b9d28", "title": "Leftover-Lunch: Advantage-based Offline Reinforcement Learning for Language Models"}, {"paperId": "19c222d1f18317d58cc85491f37479bc0dc49f41", "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs"}, {"paperId": "ac771182d1780c863954243809d1e144433919f9", "title": "Aligning Large Language Models with Human: A Survey"}, {"paperId": "24de1048791bac4972ecc16d1c3c1de23691407d", "title": "Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"}]}
