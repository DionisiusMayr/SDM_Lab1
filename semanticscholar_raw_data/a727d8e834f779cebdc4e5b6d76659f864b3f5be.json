{"paperId": "a727d8e834f779cebdc4e5b6d76659f864b3f5be", "publicationVenue": {"id": "d2e111a6-dd60-474c-ae8a-2b5773cca07d", "name": "International Conference on Parallel and Distributed Systems", "type": "conference", "alternate_names": ["Int Conf Parallel Distrib Syst", "ICPADS"], "url": "http://www.wikicfp.com/cfp/program?id=1443"}, "title": "Distributed Training of Large Language Models", "abstract": "The advent of large language models (LLMs), like ChatGPT ushers in revolutionary opportunities that bring a vast variety of applications (such as healthcare, law, and education) across various disciplines. The research report pointed out that the model showcases excellent performance often closely related to the parameter scale of the model, so how to train an LLM? This is a question that everyone is more concerned about. At present, there are several commonly used distributed training frameworks including Megatron-LM, DeepSpeed, etc. In this paper, we first provide a brief introduction, which refers to the current development status of LLM. Second, we start from the status, introducing the current common parallel strategies of LLM distributed training. Next, we briefly introduce the underlying technologies and frameworks that LLM relies on nowadays, describing the current popular ones and types of large models. Then we introduce the optimization techniques used in the LLMs. Finally, we summarize the problems and challenges encountered in the current LLM training and describe the possible future development direction of LLM.", "venue": "International Conference on Parallel and Distributed Systems", "year": 2023, "fieldsOfStudy": null, "publicationTypes": ["Conference"], "publicationDate": "2023-12-17", "journal": {"name": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)", "pages": "840-847"}, "authors": [{"authorId": "2266389161", "name": "Fanlong Zeng"}, {"authorId": "2248178756", "name": "Wensheng Gan"}, {"authorId": "2266421251", "name": "Yongheng Wang"}, {"authorId": "2261463959", "name": "Philip S. Yu"}], "citations": [{"paperId": "49de32756c194402bd0a4dd3344120af7ca3e220", "title": "Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining1"}]}
