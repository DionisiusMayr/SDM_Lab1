{"paperId": "c3af18fbc0ed47f07f1bbae075aa97bbc08b89e8", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models", "abstract": "Large language models (LLMs) exhibit positional bias in how they use context, which especially complicates listwise ranking. To address this, we propose permutation self-consistency, a form of self-consistency over ranking list outputs of black-box LLMs. Our key idea is to marginalize out different list orders in the prompt to produce an order-independent ranking with less positional bias. First, given some input prompt, we repeatedly shuffle the list in the prompt and pass it through the LLM while holding the instructions the same. Next, we aggregate the resulting sample of rankings by computing the central ranking closest in distance to all of them, marginalizing out prompt order biases in the process. Theoretically, we prove the robustness of our method, showing convergence to the true ranking in the presence of random perturbations. Empirically, on five list-ranking datasets in sorting and passage reranking, our approach improves scores from conventional inference by up to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previous state of the art in passage reranking. Our code is at https://github.com/castorini/perm-sc.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-12", "journal": {"name": "ArXiv", "volume": "abs/2310.07712"}, "authors": [{"authorId": "26917433", "name": "Raphael Tang"}, {"authorId": "2118895402", "name": "Xinyu Crystina Zhang"}, {"authorId": "2461713", "name": "Xueguang Ma"}, {"authorId": "2257085301", "name": "Jimmy Lin"}, {"authorId": "52416773", "name": "Ferhan Ture"}], "citations": [{"paperId": "0b220041eb83c23b7b10d32a5d08c0309d528071", "title": "Large Language Models for Information Retrieval: A Survey"}]}
