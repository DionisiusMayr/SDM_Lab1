{"paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models", "abstract": "Alignment is crucial for training large language models. The predominant strategy is Reinforcement Learning from Human Feedback (RLHF), with Proximal Policy Optimization (PPO) as the de-facto algorithm. Yet, PPO is known to struggle with computational inefficiency, a challenge that this paper aims to address. We identify three important properties of RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on these properties, we develop ReMax, a new algorithm tailored for RLHF. The design of ReMax builds on the celebrated algorithm REINFORCE but is enhanced with a new variance-reduction technique. ReMax offers threefold advantages over PPO: first, it is simple to implement with just 6 lines of code. It further eliminates more than 4 hyper-parameters in PPO, which are laborious to tune. Second, ReMax reduces memory usage by about 50%. To illustrate, PPO runs out of memory when fine-tuning a Llama2-7B model on A100-80GB GPUs, whereas ReMax can support the training. Even though memory-efficient techniques (e.g., ZeRO and offload) are employed for PPO to afford training, ReMax can utilize a larger batch size to increase throughput. Third, in terms of wall-clock time, PPO is about twice as slow as ReMax per iteration. Importantly, these improvements do not sacrifice task performance. We hypothesize that these advantages can be maintained in larger-scale models.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-16", "journal": {"name": "ArXiv", "volume": "abs/2310.10505"}, "authors": [{"authorId": "25841722", "name": "Ziniu Li"}, {"authorId": "40084973", "name": "Tian Xu"}, {"authorId": "2164118584", "name": "Yushun Zhang"}, {"authorId": "2258789112", "name": "Yang Yu"}, {"authorId": "2257157840", "name": "Ruoyu Sun"}, {"authorId": "2114939672", "name": "Zhimin Luo"}], "citations": [{"paperId": "42ee90ce864f1cb2a865f554fc3c6531d0ea34d3", "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF"}, {"paperId": "bb94aef621b67ef56c796151ab31724ee8f59ed0", "title": "A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference"}, {"paperId": "f65db0fc08e01bb41a7a69197b090765a7063f95", "title": "Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning"}, {"paperId": "44a9d8b0314d34aff91ccff9207d38eed37216ed", "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint"}]}
