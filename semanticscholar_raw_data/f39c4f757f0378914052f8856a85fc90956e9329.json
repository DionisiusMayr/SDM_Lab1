{"paperId": "f39c4f757f0378914052f8856a85fc90956e9329", "publicationVenue": {"id": "7bb54772-a70c-44df-9b9d-9f3b5354c0e2", "name": "IEEE International Parallel and Distributed Processing Symposium", "type": "conference", "alternate_names": ["IEEE Int Parallel Distrib Process Symp", "International Parallel and Distributed Processing Symposium", "IPDPS", "Int Parallel Distrib Process Symp"], "url": "http://www.ipdps.org/"}, "title": "PCGCN: Partition-Centric Processing for Accelerating Graph Convolutional Network", "abstract": "Inspired by the successes of convolutional neural networks (CNN) in computer vision, the convolutional operation has been moved beyond low-dimension grids (e.g., images) to high-dimensional graph-structured data (e.g., web graphs, social networks), leading to graph convolutional network (GCN). And GCN has been gaining popularity due to its success in real-world applications such as recommendation, natural language processing, etc. Because neural network and graph propagation have high computation complexity, GPUs have been introduced to both neural network training and graph processing. However, it is notoriously difficult to perform efficient GCN computing on data parallel hardware like GPU due to the sparsity and irregularity in graphs. In this paper, we present PCGCN, a novel and general method to accelerate GCN computing by taking advantage of the locality in graphs. We experimentally demonstrate that real-world graphs usually have the clustering property that can be used to enhance the data locality in GCN computing. Then, PCGCN proposes to partition the whole graph into chunks according to locality and process subgraphs with a dual-mode computing strategy which includes a selective and a full processing methods for sparse and dense subgraphs, respectively. Compared to existing state-of-the-art implementations of GCN on real-world and synthetic datasets, our implementation on top of TensorFlow achieves up to 8.8\u00d7 speedup over the fastest one of the baselines.", "venue": "IEEE International Parallel and Distributed Processing Symposium", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-05-01", "journal": {"name": "2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "pages": "936-945"}, "authors": [{"authorId": "2072927082", "name": "Chao Tian"}, {"authorId": "2492241", "name": "Lingxiao Ma"}, {"authorId": "98256743", "name": "Zhi Yang"}, {"authorId": "34889832", "name": "Yafei Dai"}], "citations": [{"paperId": "a48d60f216c9c7714f14bd6196ee8dc4b3070269", "title": "GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System"}, {"paperId": "85691014466d48647a01d8726c68d0e9258fc667", "title": "Single-GPU GNN Systems: Traps and Pitfalls"}, {"paperId": "2a72d9a7f8a3c4740fc75b8e62a6648189fb8f19", "title": "Barad-dur: Near-Storage Accelerator for Training Large Graph Neural Networks"}, {"paperId": "0cdf3d81aed199265a20012af77857c321d16774", "title": "STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs"}, {"paperId": "8ff6afb036dbf6f587c90e0d0603f0fc5bfaa555", "title": "A Survey of Graph Pre-processing Methods: From Algorithmic to Hardware Perspectives"}, {"paperId": "e5859eca194340a1295057e049c204f60a0abd82", "title": "Accelerating Graph Convolutional Networks Through a PIM-Accelerated Approach"}, {"paperId": "e52549fdb6be6d36071143764b8d8862aabb930c", "title": "Optimizing GPU-Based Graph Sampling and Random Walk for Efficiency and Scalability"}, {"paperId": "116c1eab038d7c5d3f2ff3d1103cdd1fefdd2ef8", "title": "Graph Neural Networks for Intelligent Transportation Systems: A Survey"}, {"paperId": "54a863e3155637681e9330c04c7c1928c2d7455e", "title": "Serving Graph Neural Networks With Distributed Fog Servers for Smart IoT Services"}, {"paperId": "894d61c709ec6f61899703458d90b09c663d7b11", "title": "A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware"}, {"paperId": "548449b18b248f08f851d1ac7123b9d793c526c9", "title": "AdaptGear: Accelerating GNN Training via Adaptive Subgraph-Level Kernels on GPUs"}, {"paperId": "2e24c192aa130183188b67f4ba4450605b6dbc6d", "title": "Software-hardware co-design for accelerating large-scale graph convolutional network inference on FPGA"}, {"paperId": "c6becf9b67bd7abe6e9c8c3e0074a2e86422e554", "title": "A survey of field programmable gate array (FPGA)-based graph convolutional neural network accelerators: challenges and opportunities"}, {"paperId": "1e79e33c77b2d8eaf643af0e1f5003057d7356b2", "title": "Distributed Graph Neural Network Training: A Survey"}, {"paperId": "1b2576c824a8c71e559eea72486092575e5dbbde", "title": "GNN at the Edge: Cost-Efficient Graph Neural Network Processing Over Distributed Edge Servers"}, {"paperId": "ef12b71e7c7cc7483e3c2147ebf9753a31554266", "title": "TLPGNN: A Lightweight Two-Level Parallelism Paradigm for Graph Neural Network Computation on GPU"}, {"paperId": "9c05a1223f7f900d3dcd52ec03ac13a9ccd9d713", "title": "ProGNNosis: A Data-driven Model to Predict GNN Computation Time Using Graph Metrics"}, {"paperId": "5be7d06ee1647b4b257036e8d1f3b4a0ecb0b05c", "title": "Hyperscale FPGA-as-a-service architecture for large-scale distributed graph neural network"}, {"paperId": "8018a561d2ed821cfb4be7ce04993cabf9932c2f", "title": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis"}, {"paperId": "f5cb7f12d56969fec01af3037af3b61918ab901e", "title": "COIN: Communication-Aware In-Memory Acceleration for Graph Convolutional Networks"}, {"paperId": "bcfd5c42af568bd4e15ee3b6be518490a6d7e371", "title": "EC-Graph: A Distributed Graph Neural Network System with Error-Compensated Compression"}, {"paperId": "e4b81c6d782d4da6f4168298f4861f410a7a4f9f", "title": "Fograph: Enabling Real-Time Deep Graph Inference with Fog Computing"}, {"paperId": "9ae5cc2b23df6247d3a16f60bc1a6fd377198885", "title": "Understanding the Design Space of Sparse/Dense Multiphase Dataflows for Mapping Graph Neural Networks on Spatial Accelerators."}, {"paperId": "b40352e71bae7ecb1dfe93cdad044749202eb6cb", "title": "PCGraph: Accelerating GNN Inference on Large Graphs via Partition Caching"}, {"paperId": "0c2d35908e1af4fd3ec48db93333f82d9a09c4de", "title": "Motif Prediction with Graph Neural Networks"}, {"paperId": "6c756260e4f07b3adf1b531a2b073451cc8dea20", "title": "Accelerating Large Scale Real-Time GNN Inference using Channel Pruning"}, {"paperId": "96c953826918394468ba4791749274ac966faf5f", "title": "ADGraph: Accurate, Distributed Training on Large Graphs"}, {"paperId": "99016bc147399e86cdd0775bf7d7031de223570b", "title": "A Taxonomy for Classification and Comparison of Dataflows for GNN Accelerators"}, {"paperId": "c674139862eb662b674969413f2a9af5b02c5f14", "title": "BoostGCN: A Framework for Optimizing GCN Inference on FPGA"}, {"paperId": "c3d9e591e230b253a82352b70e529203b7ee9f19", "title": "On the Equivalence of Decoupled Graph Convolution Network and Label Propagation"}, {"paperId": "2e92728e393538304555e48a8d7532daf5ebabd4", "title": "Computing Graph Neural Networks: A Survey from Algorithms to Accelerators"}, {"paperId": "c68cd169e8aa67943c43138f9eacc8c05b4c3178", "title": "IPDPS 2020 TOC"}, {"paperId": "c238d932615f92613788e42fbe8c062f52d2d122", "title": "Analyzing GCN Aggregation on GPU"}]}
