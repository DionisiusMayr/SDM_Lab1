{"paperId": "e3f7ad05b1652c6ada78cffbe405bceb723bc70c", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "MoDS: Model-oriented Data Selection for Instruction Tuning", "abstract": "Instruction tuning has become the de facto method to equip large language models (LLMs) with the ability of following user instructions. Usually, hundreds of thousands or millions of instruction-following pairs are employed to fine-tune the foundation LLMs. Recently, some studies show that a small number of high-quality instruction data is enough. However, how to select appropriate instruction data for a given LLM is still an open problem. To address this problem, in this paper we present a model-oriented data selection (MoDS) approach, which selects instruction data based on a new criteria considering three aspects: quality, coverage and necessity. First, our approach utilizes a quality evaluation model to filter out the high-quality subset from the original instruction dataset, and then designs an algorithm to further select from the high-quality subset a seed instruction dataset with good coverage. The seed dataset is applied to fine-tune the foundation LLM to obtain an initial instruction-following LLM. Finally, we develop a necessity evaluation model to find out the instruction data which are performed badly in the initial instruction-following LLM and consider them necessary instructions to further improve the LLMs. In this way, we can get a small high-quality, broad-coverage and high-necessity subset from the original instruction datasets. Experimental results show that, the model fine-tuned with 4,000 instruction pairs selected by our approach could perform better than the model fine-tuned with the full original dataset which includes 214k instruction data.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-27", "journal": {"name": "ArXiv", "volume": "abs/2311.15653"}, "authors": [{"authorId": "8134471", "name": "Qianlong Du"}, {"authorId": "2064100826", "name": "Chengqing Zong"}, {"authorId": "2124819243", "name": "Jiajun Zhang"}], "citations": [{"paperId": "c00c3fd40e4ab89faa14c0d2d34e0ea5de8e3608", "title": "Exploring the Mystery of Influential Data for Mathematical Reasoning"}, {"paperId": "48a26fc1ad809f42afb0786beb0dd13fac490e8f", "title": "What's in Your\"Safe\"Data?: Identifying Benign Data that Breaks Safety"}, {"paperId": "dd334c161aad0e459e8980b362f9a3c7fd6ae51f", "title": "PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "2565b1ae47fb8e47017bee18ef1c602d6e1f4e44", "title": "Rethinking Data Selection for Supervised Fine-Tuning"}, {"paperId": "95898b1f82cf7ad7d96fcc85b4def7f086325af5", "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning"}, {"paperId": "c991dedacba67949a28640cd8755de4c8ae297b0", "title": "A Survey on Data Selection for LLM Instruction Tuning"}, {"paperId": "e9aec062906c7fb16e540dc9fb7ed2cbcf129407", "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning"}, {"paperId": "2b9fdc60647701c9c79c3203c04fded35fe2d7ec", "title": "An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models"}, {"paperId": "83d12fa58743015f8abe4097fb58088f2d13c7f0", "title": "Rethinking the Instruction Quality: LIFT is What You Need"}]}
