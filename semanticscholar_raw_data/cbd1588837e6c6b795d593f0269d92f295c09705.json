{"paperId": "cbd1588837e6c6b795d593f0269d92f295c09705", "publicationVenue": {"id": "b7aa40ac-729b-49d6-9064-4d1a9480e9a9", "name": "International Symposium on High-Performance Computer Architecture", "type": "conference", "alternate_names": ["HPCA", "High Perform Comput Appl", "Int Symp High-performance Comput Archit", "High Performance Computing and Applications"], "url": "https://web.archive.org/web/*/http://www.hpcaconf.org/"}, "title": "Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems", "abstract": "Modern deep learning (DL) training is memory-consuming, constrained by the memory capacity of each computation component and cross-device communication bandwidth. In response to such constraints, current approaches include increasing parallelism in distributed training and optimizing inter-device communication. However, model parameter communication is becoming a key performance bottleneck in distributed DL training. To improve parameter communication performance, we propose COARSE, a disaggregated memory extension for distributed DL training. COARSE is built on modern cache-coherent interconnect (CCI) protocols and MPI-like collective communication for synchronization, to allow low-latency and parallel access to training data and model parameters shared among worker GPUs. To enable high bandwidth transfers between GPUs and the disaggregated memory system, we propose a decentralized parameter communication scheme to decouple and localize parameter synchronization traffic. Furthermore, we propose dynamic tensor routing and partitioning to fully utilize the non-uniform serial bus bandwidth varied across different cloud computing systems. Finally, we design a deadlock avoidance and dual synchronization to ensure high-performance parameter synchronization. Our evaluation shows that COARSE achieves up to 48.3% faster DL training compared to the state-of-the-art MPI AllReduce communication.", "venue": "International Symposium on High-Performance Computer Architecture", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-04-01", "journal": {"name": "2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "pages": "126-140"}, "authors": [{"authorId": "2013869802", "name": "Zixuan Wang"}, {"authorId": "30937904", "name": "Joonseop Sim"}, {"authorId": "1405984505", "name": "Euicheol Lim"}, {"authorId": "2109858015", "name": "Jishen Zhao"}], "citations": [{"paperId": "f5a8c9a7cdc90251e1c423401b01931fee284445", "title": "High-Level Data Abstraction and Elastic Data Caching for Data-Intensive AI Applications on Cloud-Native Platforms"}, {"paperId": "b90191d391b5b12a3b24f9fc3c163f91157e9c88", "title": "\u0100pta: Fault-tolerant object-granular CXL disaggregated memory for accelerating FaaS"}, {"paperId": "c386df36f579bd08972dfeb3b31c14a327770090", "title": "EdgeMove: Pipelining Device-Edge Model Training for Mobile Intelligence"}, {"paperId": "4ba3679794efaeaf95432aabaf7eea75d05211e1", "title": "CAESAR: Coherence-Aided Elective and Seamless Alternative Routing via on-chip FPGA"}, {"paperId": "4cd2754e14cff11a258130bc0334f9503cbd3a38", "title": "Evaluating Emerging CXL-enabled Memory Pooling for HPC Systems"}, {"paperId": "4cfc35aafa2a343101bbffefbe7f8e79b95f811e", "title": "Hello bytes, bye blocks: PCIe storage meets compute express link for memory expansion (CXL-SSD)"}]}
