{"paperId": "c22d63ac2bf295e7c31f9a5b263b9b0360730327", "publicationVenue": {"id": "d135477d-8b21-429b-8564-b80f938a0147", "name": "International Conference on High Performance Computing", "type": "conference", "alternate_names": ["HiPC", "High Performance Computing Symposium", "IEEE International Conference on High Performance Computing, Data, and Analytics", "HPC", "Int Adv Res Workshop High Perform Comput", "Int Conf High Perform Comput", "International Advanced Research Workshop on High Performance Computing", "High Perform Comput Symp", "IEEE Int Conf High Perform Comput Data Anal"], "url": "http://www.wikicfp.com/cfp/program?id=1192"}, "title": "Accelerating Time to Science using CRADLE: A Framework for Materials Data Science", "abstract": "Modern materials science research problems present a challenge to data science and analytics as experiments generate Petabyte-scale spatiotemporal datasets that span a number of modalities and formats. Creating computing infrastructure and frameworks that support the scale and diversity of materials science data while remaining accessible for materials scientists to use is a non-trivial task. We have developed the Common Research Analytics and Data Lifecycle Environment (CRADLE) to solve the challenges of materials data science through a scalable research computing framework and cyber infrastructure that can (1) handle large-scale, heterogeneous datasets (2) provide a flexible toolbox for building machine learning pipelines that span from ingestion to model deployment (3) be accessible to research scientists with limited to extensive computational backgrounds and (4) utilize a myriad of low performance to high performance computer systems. CRADLE is a framework that integrates distributed systems like Hadoop and High-Performance Computing (HPC) infras-tructure to handle materials data at scale. This all enables the general materials data scientist to query Petabytes of data and train thousands of models in a parallel, distributed environment. We demonstrate three use cases for CRADLE to benchmark its capability to ingest and analyze spatiotemporal materials data at scale. These tasks span three data modalities: transforming 2.6 billion Photovoltaic time-series power measurements, training hundreds of deep learning models on Atomic Force Microscopy images, and ingesting 27 billion geospatial data points. CRADLE exemplifies an overarching framework that accelerates time to science, extends to other domains with similar challenges, and expands the horizon of data science and research.", "venue": "International Conference on High Performance Computing", "year": 2023, "fieldsOfStudy": null, "publicationTypes": ["Conference"], "publicationDate": "2023-12-18", "journal": {"name": "2023 IEEE 30th International Conference on High Performance Computing, Data, and Analytics (HiPC)", "pages": "234-245"}, "authors": [{"authorId": "2115137329", "name": "A. Nihar"}, {"authorId": "2287573195", "name": "Thomas G. Ciardi"}, {"authorId": "2295155464", "name": "Rounak Chawla"}, {"authorId": "2284398465", "name": "Olatunde D. Akanbi"}, {"authorId": "2280017035", "name": "Vipin Chaudhary"}, {"authorId": "2261079881", "name": "Yinghui Wu"}, {"authorId": "2280016838", "name": "Roger H. French"}], "citations": []}
