{"paperId": "c4d3b9a87295db0b9f6466b7f3ce2f175c6d3157", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length", "abstract": "The evolving sophistication and intricacies of Large Language Models (LLMs) yield unprecedented advancements, yet they simultaneously demand considerable computational resources and incur significant costs. To alleviate these challenges, this paper introduces a novel, simple, and effective method named ``\\growlength'' to accelerate the pretraining process of LLMs. Our method progressively increases the training length throughout the pretraining phase, thereby mitigating computational costs and enhancing efficiency. For instance, it begins with a sequence length of 128 and progressively extends to 4096. This approach enables models to process a larger number of tokens within limited time frames, potentially boosting their performance. In other words, the efficiency gain is derived from training with shorter sequences optimizing the utilization of resources. Our extensive experiments with various state-of-the-art LLMs have revealed that models trained using our method not only converge more swiftly but also exhibit superior performance metrics compared to those trained with existing methods. Furthermore, our method for LLMs pretraining acceleration does not require any additional engineering efforts, making it a practical solution in the realm of LLMs.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-01", "journal": {"name": "ArXiv", "volume": "abs/2310.00576"}, "authors": [{"authorId": "1791983892", "name": "Hongye Jin"}, {"authorId": "2249766229", "name": "Xiaotian Han"}, {"authorId": "2250031863", "name": "Jingfeng Yang"}, {"authorId": "47653902", "name": "Zhimeng Jiang"}, {"authorId": "2166879577", "name": "Chia-yuan Chang"}, {"authorId": "2249844820", "name": "Xia Hu"}], "citations": [{"paperId": "db7498f569be9852a04b2bb5bd68bd2885820bea", "title": "World Model on Million-Length Video And Language With Blockwise RingAttention"}, {"paperId": "23b09ed66024fdd04d6713b9ba621b866f033d20", "title": "The What, Why, and How of Context Length Extension Techniques in Large Language Models - A Detailed Survey"}, {"paperId": "24de1048791bac4972ecc16d1c3c1de23691407d", "title": "Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"}, {"paperId": "80da74d006d9e9ac1252c4092bb72b9878d8bec7", "title": "W ORLD M ODEL ON M ILLION -L ENGTH V IDEO A ND L ANGUAGE W ITH R ING A TTENTION"}]}
