{"paperId": "5f58863dd6474d6f127be995b5871e7c60f2792f", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Video Understanding with Large Language Models: A Survey", "abstract": "With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of Large Language Models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into four main types: LLM-based Video Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods. Furthermore, this survey presents a comprehensive study of the tasks, datasets, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-12-29", "journal": {"name": "ArXiv", "volume": "abs/2312.17432"}, "authors": [{"authorId": "2119309562", "name": "Yunlong Tang"}, {"authorId": "2066776633", "name": "Jing Bi"}, {"authorId": "2186266050", "name": "Siting Xu"}, {"authorId": "2242154602", "name": "Luchuan Song"}, {"authorId": "2153545235", "name": "Susan Liang"}, {"authorId": "2277415304", "name": "Teng Wang"}, {"authorId": "2266412651", "name": "Daoan Zhang"}, {"authorId": "2277235738", "name": "Jie An"}, {"authorId": "2277246502", "name": "Jingyang Lin"}, {"authorId": "2277251955", "name": "Rongyi Zhu"}, {"authorId": "82946757", "name": "A. Vosoughi"}, {"authorId": "2277419004", "name": "Chao Huang"}, {"authorId": "2155280541", "name": "Zeliang Zhang"}, {"authorId": "2146483847", "name": "Feng Zheng"}, {"authorId": "2277204434", "name": "Jianguo Zhang"}, {"authorId": "2277217764", "name": "Ping Luo"}, {"authorId": "2257204717", "name": "Jiebo Luo"}, {"authorId": "2242467374", "name": "Chenliang Xu"}], "citations": [{"paperId": "e8f12eb28184dfd0b2e3de58636ca0e566db4a6d", "title": "AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue"}, {"paperId": "d8415838384db8c056622c81922a47a68b9cdb26", "title": "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies"}, {"paperId": "c44393114047e0f9c32e45b381970d50ed503260", "title": "LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs"}, {"paperId": "271f08345dab179670da94ea67f1e0862db2dac1", "title": "VideoPrism: A Foundational Visual Encoder for Video Understanding"}]}
