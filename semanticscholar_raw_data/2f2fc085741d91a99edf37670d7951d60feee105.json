{"paperId": "2f2fc085741d91a99edf37670d7951d60feee105", "publicationVenue": {"id": "911e7332-8ea8-4e9d-bc20-5572a2523f92", "name": "International Middleware Conference", "type": "conference", "alternate_names": ["Middleware", "ACM/IFIP/USENIX int conf Middlew", "ACM/IFIP/USENIX international conference on Middleware", "Int Middlew Conf"], "url": "https://dl.acm.org/conference/middleware/proceedings"}, "title": "Mitigating Stragglers in the Decentralized Training on Heterogeneous Clusters", "abstract": "Decentralized algorithms, e.g., AllReduce, have been widely applied as the synchronization strategy for data-parallel distributed deep learning due to its superior performance over centralized ones. The synchronous Stochastic Gradient Descent (SGD) approach guarantees accuracy for various deep learning models, but its performance suffers from stragglers, i.e., \"long-tail effects.\" The straggler can be caused by the inherent load imbalance from workloads or system heterogeneity. Despite existing optimizations to support centralized algorithms against stragglers, little effort has been explored in decentralized training algorithms. This paper proposes a Randomized Non-blocking AllReduce (RNA) protocol to mitigate the straggler problem. To avoid \"long-tail effects\" brought by the strict barrier in the AllReduce, we propose a decentralized, relaxed, and randomized sampling approach to implement partial AllReduce operation. To handle heterogeneity at a large scale, we combine the traditional Parameter Servers (PS) with AllReduce to implement a hierarchical synchronization mechanism. We theoretically demonstrate the convergence analysis and detail the system implementation. The experiment results on representative deep learning models show nearly 1.8\u00d7 speedup over the state-of-the-art Horovod and 1.3\u00d7 speedup over AD-PSGD on a heterogeneous cluster.", "venue": "International Middleware Conference", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2020-12-07", "journal": {"name": "Proceedings of the 21st International Middleware Conference"}, "authors": [{"authorId": "2007708782", "name": "Donglin Yang"}, {"authorId": "39164201", "name": "Wei Rang"}, {"authorId": "1778706", "name": "Dazhao Cheng"}], "citations": [{"paperId": "b37f3cf718c119b36474b7776e6735ddbc7e38c9", "title": "Heter-Train: A Distributed Training Framework Based on Semi-Asynchronous Parallel Mechanism for Heterogeneous Intelligent Transportation Systems"}, {"paperId": "a31fad4ebe1957b4fea92b6fbd0bed5df98b7df6", "title": "Optimizing Aggregation Frequency for Hierarchical Model Training in Heterogeneous Edge Computing"}, {"paperId": "1c7d3a359f4029e5384d784ce118ab53e71877b6", "title": "DropCompute: simple and more robust distributed synchronous training via compute variance reduction"}, {"paperId": "864dd48ed2caa0ba0d1b04895754d87bd23eda81", "title": "Chronica: A Data-Imbalance-Aware Scheduler for Distributed Deep Learning"}, {"paperId": "34e14fdf011987ecf8abac4dc18c776e1554c51c", "title": "FSP: Towards Flexible Synchronous Parallel Frameworks for Distributed Machine Learning"}, {"paperId": "61cb0123a3e66a79fa69f69e84e11d6ac993bc79", "title": "Forseti: Dynamic chunk-level reshaping for data processing on heterogeneous clusters"}, {"paperId": "7b75e579544a2f4c07577dbf28db28e555cc2a88", "title": "Profiling Deep Learning Workloads at Scale using Amazon SageMaker"}]}
