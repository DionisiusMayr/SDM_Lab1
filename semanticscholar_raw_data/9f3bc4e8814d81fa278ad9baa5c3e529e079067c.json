{"paperId": "9f3bc4e8814d81fa278ad9baa5c3e529e079067c", "publicationVenue": {"id": "d4610af5-85e0-480b-8773-5c71d92a7b99", "name": "International Conference on Architectural Support for Programming Languages and Operating Systems", "type": "conference", "alternate_names": ["ASPLOS", "Int Conf Archit Support Program Lang Oper Syst", "Archit Support Program Lang Oper Syst", "Architectural Support for Programming Languages and Operating Systems"], "url": "http://www.acm.org/sigplan/"}, "title": "Towards a Machine Learning-Assisted Kernel with LAKE", "abstract": "The complexity of modern operating systems (OSes), rapid diversification of hardware, and steady evolution of machine learning (ML) motivate us to explore the potential of ML to improve decision-making in OS kernels. We conjecture that ML can better manage tradeoff spaces for subsystems such as memory management and process and I/O scheduling that currently rely on hand-tuned heuristics to provide reasonable average-case performance. We explore the replacement of heuristics with ML-driven decision-making in five kernel subsystems, consider the implications for kernel design, shared OS-level components, and access to hardware acceleration. We identify obstacles, address challenges and characterize tradeoffs for the benefits ML can provide that arise in kernel-space. We find that use of specialized hardware such as GPUs is critical to absorbing the additional computational load required by ML decisioning, but that poor accessibility of accelerators in kernel space is a barrier to adoption. We also find that the benefits of ML and acceleration for OSes is subsystem-, workload- and hardware-dependent, suggesting that using ML in kernels will require frameworks to help kernel developers navigate new tradeoff spaces. We address these challenge by building a system called LAKE for supporting ML and exposing accelerators in kernel space. LAKE includes APIs for feature collection and management across abstraction layers and module boundaries. LAKE provides mechanisms for managing the variable profitability of acceleration, and interfaces for mitigating contention for resources between user and kernel space. We show that an ML-backed I/O latency predictor can have its inference time reduced by up to 96% with acceleration.", "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2023-01-27", "journal": {"name": "Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2"}, "authors": [{"authorId": "3215063", "name": "Henrique Fingler"}, {"authorId": "31871632", "name": "Isha Tarte"}, {"authorId": "7279070", "name": "Hangchen Yu"}, {"authorId": "1644635852", "name": "Ariel Szekely"}, {"authorId": "146267713", "name": "Bodun Hu"}, {"authorId": "152426179", "name": "Aditya Akella"}, {"authorId": "1692790", "name": "C. Rossbach"}], "citations": [{"paperId": "9b580b32de576d76ed6522c5e478c33ca9ced953", "title": "On a Foundation Model for Operating Systems"}]}
