{"paperId": "a4c921bdef167ae54cc3a40643e6e3ed13d49a61", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions", "abstract": "Training large language models to follow instructions makes them perform better on a wide range of tasks and generally become more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning. We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3% safety examples (a few hundred demonstrations) when fine-tuning a model like LLaMA can substantially improve its safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find exaggerated safety behaviours, where too much safety-tuning makes models refuse perfectly safe prompts if they superficially resemble unsafe ones. As a whole, our results illustrate trade-offs in training LLMs to be helpful and training them to be safe.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-14", "journal": {"name": "ArXiv", "volume": "abs/2309.07875"}, "authors": [{"authorId": "49224924", "name": "Federico Bianchi"}, {"authorId": "51903517", "name": "Mirac Suzgun"}, {"authorId": "1481857041", "name": "Giuseppe Attanasio"}, {"authorId": "2043232919", "name": "Paul R\u00f6ttger"}, {"authorId": "1746807", "name": "Dan Jurafsky"}, {"authorId": "2117567142", "name": "Tatsunori Hashimoto"}, {"authorId": "2240530524", "name": "James Zou"}], "citations": [{"paperId": "d862aae89c364eb97442372979a0d185e95ae3bb", "title": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety"}, {"paperId": "9fd65c623d319dd21b99d67fcca2b4b4f2717ec5", "title": "Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation"}, {"paperId": "d95bf07bf21d8c17810da9c1f87190257d757b05", "title": "Developing Safe and Responsible Large Language Models -- A Comprehensive Framework"}, {"paperId": "6fd5dbea7588ee6bca703aa3fea9a487006dba29", "title": "Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order"}, {"paperId": "8b1378a728ac223309e5e4c6d2006654b2d469bf", "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models"}, {"paperId": "6fe7e6ce3cc0ebd038caa456d73fd7472e7d6c38", "title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment"}, {"paperId": "9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2", "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates"}, {"paperId": "c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a", "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning"}, {"paperId": "05e0c57f912cec9597021855bac28306c97e36fd", "title": "Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content"}, {"paperId": "e79671a83e25288fedd897e1c9e6152f70f7f52e", "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey"}, {"paperId": "490e815b3be11ba97631783d9ae946b8f8517fd6", "title": "Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues"}, {"paperId": "8ee0d8f6a35f66d8bb97e3388b85dba10d8d22d2", "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science"}, {"paperId": "2d83b615f989c8d1e9860a8a2d82628c95e40d22", "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models"}, {"paperId": "f21d0177e9374bb8579c1d9c71319f212f62b3d5", "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance"}, {"paperId": "3500dbcea3de9880b195c6114c6959fcfa9719f7", "title": "Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models"}, {"paperId": "0e0ea3593dda3039cb93d2ec795a87420006ec08", "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents"}, {"paperId": "19174942f77857fc9308a877781ce9d4d1944ba9", "title": "Large Language Models for Social Networks: Applications, Challenges, and Solutions"}, {"paperId": "accb2fab67c76d5668908107cd50cbb81110c389", "title": "The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness"}, {"paperId": "9763c526f667de1f853fdcf59b29f1ac5ddd33f1", "title": "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack"}, {"paperId": "1a5a79b393b3f00eb5a47243ee031ad799d2f641", "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"}, {"paperId": "267ef391ebaa4fc3c8ec62cb92107e8efe82a5db", "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering"}, {"paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2", "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"}, {"paperId": "a95f8c0228b9d8940632c982e0c6106b701a105b", "title": "Making Harmful Behaviors Unlearnable for Large Language Models"}, {"paperId": "a64067c6c4286fc60f4430829ae6b18519c088e3", "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models"}, {"paperId": "0e0e706e13f160e74cac9556f28ab9a358c148d2", "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"}, {"paperId": "84b7c486c56bd3880cb8eb01de9ae90ba3ebdaed", "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models"}, {"paperId": "6f75e8b61f13562237851d8119cb2f9d49e073fb", "title": "Can LLM-Generated Misinformation Be Detected?"}, {"paperId": "897940fb5dd4d739b88c4659c4565d05f48d06b8", "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"}, {"paperId": "b67eb8213a63be8a4b0274728ffdc50bfa109e10", "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models"}, {"paperId": "e51d38bbabca719cd9c845d1e004c724e9cbdfa4", "title": "How is ChatGPT's behavior changing over time?"}]}
