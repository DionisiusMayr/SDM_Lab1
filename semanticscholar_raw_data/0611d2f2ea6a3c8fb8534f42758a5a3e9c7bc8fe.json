{"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "title": "Hash Layers For Large Sparse Models", "abstract": "We investigate the training of sparse layers that use different parameters for different inputs based on hashing in large Transformer models. Specifically, we modify the feedforward layer to hash to different sets of weights depending on the current token, over all tokens in the sequence. We show that this procedure either outperforms or is competitive with learning-to-route mixture-of-expert methods such as Switch Transformers and BASE Layers, while requiring no routing parameters or extra terms in the objective function such as a load balancing loss, and no sophisticated assignment algorithm. We study the performance of different hashing techniques, hash sizes and input features, and show that balanced and random hashes focused on the most local features work best, compared to either learning clusters or using longer-range context. We show our approach works well both on large language modeling and dialogue tasks, and on downstream fine-tuning tasks.", "venue": "Neural Information Processing Systems", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-08", "journal": {"pages": "17555-17566"}, "authors": [{"authorId": "3849208", "name": "Stephen Roller"}, {"authorId": "2265067", "name": "Sainbayar Sukhbaatar"}, {"authorId": "3149531", "name": "Arthur Szlam"}, {"authorId": "145183709", "name": "J. Weston"}], "citations": [{"paperId": "c67c4c81beed122d7f94580d8816a6dc68867ec4", "title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models"}, {"paperId": "3940c3071e343e00d0a1d8c129854eee9430e3fb", "title": "Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts"}, {"paperId": "95dff42e685e35bec18b58e33fadbe25a35782fc", "title": "Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts"}, {"paperId": "5c5adfbeb53b2e7668e60b4dd9612d671e707f81", "title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts"}, {"paperId": "07894aeadab9158fdb97647c4792816ede1b60b9", "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"}, {"paperId": "5c2f4a0cb9b02bc21e2508228e1bef3abe8d64aa", "title": "Enabling Large Dynamic Neural Network Training with Learning-based Memory Management"}, {"paperId": "3041f6f7882a6b59c9a51e58dfd33790821475e7", "title": "Enhancing Efficiency in Sparse Models with Sparser Selection"}, {"paperId": "5c1ffa6c5e73e278d6047843df74f9a4b75a553e", "title": "Towards an empirical understanding of MoE design choices"}, {"paperId": "af6aa336c25ead669da0df560376a32314e08006", "title": "MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models"}, {"paperId": "2fe05b1f953da5dcf6ec5fe7bc72bfb3dbd9ea30", "title": "Model Compression and Efficient Inference for Large Language Models: A Survey"}, {"paperId": "81738232a8275b65cba357150a50b1856c6f97fe", "title": "Eliciting Personality Traits in Large Language Models"}, {"paperId": "9548bacc4c7714151b674748dc86e2cc185a4955", "title": "Scaling Laws for Fine-Grained Mixture of Experts"}, {"paperId": "a74a20be53e5767648b5970e30b2d81a9ba8293f", "title": "A Survey on Transformer Compression"}, {"paperId": "a12bc2ddaa3a91edab682f6822e381ce9323e8fc", "title": "Routers in Vision Mixture of Experts: An Empirical Study"}, {"paperId": "37ac7683543f0e039197a56e71e752a9ebe5998e", "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models"}, {"paperId": "a5001fd96ee93464791072904818d90f739a4662", "title": "LocMoE: A Low-overhead MoE for Large Language Model Training"}, {"paperId": "eb5affd55edfccfa0c8cbaded214a913b9d97af2", "title": "Enhancing diversity for logical table\u2010to\u2010text generation with mixture of experts"}, {"paperId": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda", "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"}, {"paperId": "7d011d6a9e1704acc29bab88d616089089ea1006", "title": "PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation"}, {"paperId": "13261129251c9e8891cff02c3aee15c4df6a5630", "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"}, {"paperId": "e9aa2df101192ff0df10910bab0640ff8ac16ec5", "title": "Mixture-of-Linear-Experts for Long-term Time Series Forecasting"}, {"paperId": "7a2cc2fdb0f0df3752bb4668224489535bdd9a06", "title": "Learning to Skip for Language Modeling"}, {"paperId": "2209672a9d57668e5f0a53b3b3998a8943eff649", "title": "Design of a data processing method for the farmland environmental monitoring based on improved Spark components"}, {"paperId": "525cd5d5c7d823d0b2ad4eaf086622940d45ed6d", "title": "SiRA: Sparse Mixture of Low Rank Adaptation"}, {"paperId": "c4ae033f14a89db8b6b6b4da598375f177a1fac4", "title": "Memory Augmented Language Models through Mixture of Word Experts"}, {"paperId": "25067493e2a969217e6c8b8115462184390bb2bd", "title": "Prophet: Fine-grained Load Balancing for Parallel Training of Large-scale MoE Models"}, {"paperId": "3629b7d15a70299499d51c077e70f48825edf1f2", "title": "SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models"}, {"paperId": "b136e8de996b95f2295b40bf367e5512c937b61e", "title": "QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models"}, {"paperId": "66129521108ddf3db1377f425cc846ee5aa36a1e", "title": "Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation"}, {"paperId": "3570221d38b3d29b5a2d007a9ea825ec10e0f026", "title": "Image Super-resolution Via Latent Diffusion: A Sampling-space Mixture Of Experts And Frequency-augmented Decoder Approach"}, {"paperId": "f8294487f9cabd82f1772d81ed69f46556a60117", "title": "G-SPEED: General SParse Efficient Editing MoDel"}, {"paperId": "9d2860bd00652b08f5be1cd77c2934d3be98df21", "title": "Diversifying the Mixture-of-Experts Representation for Language Models with Orthogonal Optimizer"}, {"paperId": "59472fe8bba99c998fb119fa684a80423d1f2f09", "title": "Adaptive Gating in Mixture-of-Experts based Language Models"}, {"paperId": "86f034a83c04e6364681f9ac865746958e36662e", "title": "SADMoE: Exploiting Activation Sparsity with Dynamic-k Gating"}, {"paperId": "a4382a9b1edba07e0af4df2ff6a4bce22f4e55b5", "title": "AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts"}, {"paperId": "9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b", "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization"}, {"paperId": "352244ac7602f13e16a08424db322364d0a2cef1", "title": "ConPET: Continual Parameter-Efficient Tuning for Large Language Models"}, {"paperId": "5aae7d84f8eaa55f3386cee41d94769e7ab01e9d", "title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"}, {"paperId": "026f9d3ff90588d0fa686d1e4428a8685c6b1bb1", "title": "Task-Based MoE for Multitask Multilingual Machine Translation"}, {"paperId": "dc7b27b0a1d891e16f80dd9b8eec0aa8baf95b36", "title": "EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models"}, {"paperId": "3ed178316be914658a80e561bf00576577f34389", "title": "Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"}, {"paperId": "079a430070c5b01cff1d2ab11638306d1db0d6a6", "title": "Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts"}, {"paperId": "6f88133bc591cd964667a626a06debad17775757", "title": "Experts Weights Averaging: A New General Training Scheme for Vision Transformers"}, {"paperId": "2edccb8fa562ed52cd49ea6fc67ed32db6218247", "title": "From Sparse to Soft Mixtures of Experts"}, {"paperId": "a03a940371c44daf1ff0373aa8bab3a79581db7c", "title": "ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks"}, {"paperId": "9618aa98729670f74418d2087f5e47ab137856b4", "title": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models\u2019 Memories"}, {"paperId": "7a816dd242c4c3f652a448dba54daa53f89a9e4f", "title": "Soft Merging of Experts with Adaptive Routing"}, {"paperId": "38c5a43e3807ca40a34744ce6ed33488ae266cab", "title": "COMET: Learning Cardinality Constrained Mixture of Experts with Trees and Local Search"}, {"paperId": "4b5cbb924f06763a3c785d0ccfb3bc8bd765f4a5", "title": "Brainformers: Trading Simplicity for Efficiency"}, {"paperId": "dbfd154190667087ed1cd6c7f75a81858c2f397e", "title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models"}, {"paperId": "50bf60b1439368caa941b386d1ed0c364dd7fe38", "title": "Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model"}, {"paperId": "b2ec81b572fd5f0a5f5de843e3c62985b7d9c5a1", "title": "Lifting the Curse of Capacity Gap in Distilling Language Models"}, {"paperId": "c17b71f31fa708eb01310ff65ab660f2676a12a1", "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models"}, {"paperId": "ad2f06f572281068609dfb0881c8f95c61447fb8", "title": "Learning Language-Specific Layers for Multilingual Machine Translation"}, {"paperId": "09d3a354d22d75deddb0dcc99870f307aae2fd3c", "title": "Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity"}, {"paperId": "148644bf4ccef7e022b965304e8b3178be8af0fa", "title": "Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference"}, {"paperId": "dbbc5003af690799fa4fe6330fb795311cde106f", "title": "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement"}, {"paperId": "7b97979ebc76d9709ad516ee86236f64c4950356", "title": "Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling"}, {"paperId": "464770587aece80cc9e3451050058e30c2aa6666", "title": "Scaling Expert Language Models with Unsupervised Domain Discovery"}, {"paperId": "8045973dfc73ba945bb47c2181e955669e6270f9", "title": "Sparse Distributed Memory is a Continual Learner"}, {"paperId": "362cbfd0d05e139cd6cf049754098a6e1520b910", "title": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"}, {"paperId": "9d12916dd46df7a6446cbec0bc4d054f7dafcdab", "title": "Scaling Vision-Language Models with Sparse Mixture of Experts"}, {"paperId": "174ae9800f6359520d900d19890acfcf46709107", "title": "Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference"}, {"paperId": "0ce0dca6dfe98dbfc748143cd5607d5ccb01b016", "title": "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering"}, {"paperId": "1462a0e5b7db47301bb0995db56426e1f4a0ac7d", "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers"}, {"paperId": "1f346f74e8eabececa4896d734ab9b261f30830d", "title": "Modular Deep Learning"}, {"paperId": "a34384389f74b7b2c31c696b0db0bf813e8bb301", "title": "TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training"}, {"paperId": "712573cc74633ec2283724e868328fd2d319c091", "title": "Ten Lessons We Have Learned in the New \"Sparseland\": A Short Handbook for Sparse Neural Network Researchers"}, {"paperId": "b369b05f5386ce22dc7fa33c6f912d5c1cd27f14", "title": "The Power of External Memory in Increasing Predictive Model Capacity"}, {"paperId": "c9d46cfcf0211d11356c295ecd9584c84c19c8f8", "title": "Alternating Updates for Efficient Transformers"}, {"paperId": "857ba15bba68934b8a23de8c2c3564bb86a59f22", "title": "Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual Machine Translation"}, {"paperId": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e", "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"}, {"paperId": "43014fc85c4860487336579ec98f509fec1803f7", "title": "MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"}, {"paperId": "9a6d83c836ce6389b526b941d971eee775aa573e", "title": "ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts"}, {"paperId": "a99fe84851b3cd030e27ab0c9b3d673e69866c05", "title": "M3ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design"}, {"paperId": "5165de3cd4f8dc9d88e82d55f4798013d57cc0f1", "title": "AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation"}, {"paperId": "a27d5e6140269f292da02c3e88b4235c83f49e9e", "title": "Sparsity-Constrained Optimal Transport"}, {"paperId": "ca086f4c09cf8de705830ac2b70951737fab93ca", "title": "A Review of Sparse Expert Models in Deep Learning"}, {"paperId": "3f60b7b86908ee2a6024539675b7c0b05a33871b", "title": "A Theoretical View on Sparsely Activated Networks"}, {"paperId": "8b3a67c7e5289eed160d2acfd04d71cfb552c67d", "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models"}, {"paperId": "33d97417aa0e5c6fc4a58716003b02c09736e782", "title": "Towards Understanding Mixture of Experts in Deep Learning"}, {"paperId": "1c1eaea99e0cbbd4616125049bae8c6787bd368c", "title": "MoEC: Mixture of Expert Clusters"}, {"paperId": "170132622cc2772d6c6a7a3167f386a447341198", "title": "Neural Implicit Dictionary Learning via Mixture-of-Expert Training"}, {"paperId": "2e700ff36108119f5ed19a53bd2eaa22b42ec3d8", "title": "Tutel: Adaptive Mixture-of-Experts at Scale"}, {"paperId": "499d3bb3acbc10730dd6582bd9b8f646bf22ccd5", "title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts"}, {"paperId": "0fb8e89e34f00365a56c3e67f7e862fdcf935f2c", "title": "Task-Specific Expert Pruning for Sparse Mixture-of-Experts"}, {"paperId": "30d4380143a0e1de925a78017d0494c23b6304b3", "title": "Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers"}, {"paperId": "7618f17179bb316002cb6cc472d61382776af6b7", "title": "Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT"}, {"paperId": "eb4d54651c4f610749caf2bf401af3ce28ddc439", "title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"}, {"paperId": "e47da75675b9a3fe02ef1efadca39bc8cdfcdc17", "title": "Designing Effective Sparse Expert Models"}, {"paperId": "7947124631cf9a02aeb2980475455598e92478cc", "title": "Sparsely-gated Mixture-of-Expert Layers for CNN Interpretability"}, {"paperId": "c26bb68806a992bf4fc85b5639e1657a445c4781", "title": "On the Representation Collapse of Sparse Mixture of Experts"}, {"paperId": "c9550f0d1940ee1adf1549c9a0d699ef896dbefd", "title": "StableMoE: Stable Routing Strategy for Mixture of Experts"}, {"paperId": "51b950bfcaba4bad321e7342b32833d42f42c914", "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners"}, {"paperId": "9377423732cf4cae109bd1a069eca1736ab0c84c", "title": "Mixture of Experts for Biomedical Question Answering"}, {"paperId": "df434c1289f3c7243b585cb9982afac3c5bf0439", "title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation"}, {"paperId": "54b8bc5be8bbffae333dd73f2cb9d93a492d438e", "title": "HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System"}, {"paperId": "3ff00d83a2dac26b5885f312e7ff8dedcdadf71c", "title": "Efficient Language Modeling with Sparse all-MLP"}, {"paperId": "d05141dc0900140f7146bb71e1f7402cf896ea87", "title": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation"}, {"paperId": "f677ef460670e63a0e9a0bd048cd881b4b55d92f", "title": "Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models"}, {"paperId": "bbc57e1b3cf90e09b64377f13de455793bc81ad5", "title": "Mixture-of-Experts with Expert Choice Routing"}, {"paperId": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3", "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models"}, {"paperId": "802a5d24c78f713e282b003d99b4afd924bd7568", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing"}, {"paperId": "c2536182c010c41941e8a031071a1880c34cec60", "title": "Unified Scaling Laws for Routed Language Models"}, {"paperId": "cf4f4b69b76dc58dc8c0d443ab88ceb461eec719", "title": "EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate"}, {"paperId": "a3184d40d390793232c99c89b57b8f65c16320b2", "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170", "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"}, {"paperId": "0e23cc159fd2fb34550600d60dd9148c93636183", "title": "Taming Sparsely Activated Transformer with Stochastic Experts"}, {"paperId": "24e775b20adf21e9b5b95c6a9b7a5c164d055849", "title": "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining"}, {"paperId": "561f9f5abb2c0960a886ab6221c821295f0461a1", "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"}, {"paperId": "6b95a0c36683025ff38c66128b817d3640a7e03a", "title": "Unbiased Gradient Estimation with Balanced Assignments for Mixtures of Experts"}, {"paperId": "917c63f2186119166b3379f5d2816bb1a2f39b09", "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "520711a1e93e6c4221f2a7c97c27a508379e8e37", "title": "Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts"}, {"paperId": "afa1857de5383e42b7baaaa05c6946d463b17daf", "title": "SWITCH-NERF: LEARNING SCENE DECOMPOSITION"}, {"paperId": "d73247905abdea4edc7ff23f8152e6c1bcee8ce1", "title": "SCoMoE: Efficient Mixtures of Experts with Structured Communication"}, {"paperId": "1a220cad4f150036086f8e1ec437dfa11209c10b", "title": "MMNMT: Modularizing Multilingual Neural Machine Translation with Flexibly Assembled MoE and Dense Blocks"}, {"paperId": "38338207e9ee2591e67c926b7da2294318a0dec2", "title": "Models with Conditional Computation Learn Suboptimal Solutions"}, {"paperId": "7e2530784eeae241e997627795819cf42ba8562f", "title": "AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large Language Models"}, {"paperId": "ed4bd341b9652ab281ec84172f88b931ff2d0648", "title": "AutoMoE: Neural Architecture Search for Efficient Sparsely Activated Transformers"}, {"paperId": "9dd7bc01f5c45ba2a0119db3c6be2b09475fe4fc", "title": "Knowledge Distillation for Mixture of Experts Models in Speech Recognition"}, {"paperId": "e0babb4eb3e631712afb702135898736c23a1e6b", "title": "Sparsely-gated MoE Layers for CNN Interpretability"}, {"paperId": "02ded8fd5415556babb817c6a439eb5257047b2c", "title": "Towards Understanding the Mixture-of-Experts Layer in Deep Learning"}, {"paperId": "8c4e2a1c421d91a75961e1a91a378cbaf4289b0f", "title": ": Conditional Computation of Transformer Models for Ef\ufb01cient Inference"}, {"paperId": "e58d8b717dfce4f37c5a4009c74c15915411442d", "title": "Dense-to-Sparse Gate for Mixture-of-Experts"}, {"paperId": "ed0b674a1e31f8756bdb5a6d97a6c3282a4e3fb6", "title": "MoEfication: Conditional Computation of Transformer Models for Efficient Inference"}, {"paperId": "4d3e6f5d8d75445be3e8137c368d4b677eecdc14", "title": "Learning Self-supervised User Representations Using Contextualized Mixture of Experts in Transformers"}, {"paperId": "19020fa54c47bb82aaa1428a0b408b26d85d4067", "title": "Soft-routing Mixture of Experts for Autoregressive Language Model Pre-training"}, {"paperId": "4b879f069d023e03bf537309a99bdaeb39916ea5", "title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training"}, {"paperId": "ed1d274ef73f640361f0614835e46468fe489386", "title": "Hash3D: Training-free Acceleration for 3D Generation"}, {"paperId": "40891680185dd23b1270cb814ea555811c3b4618", "title": "CR-MoE: Consistent Routed Mixture-of-Experts for Scaling Contrastive Learning"}]}
