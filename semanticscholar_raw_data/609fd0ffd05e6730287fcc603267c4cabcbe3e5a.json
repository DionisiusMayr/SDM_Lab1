{"paperId": "609fd0ffd05e6730287fcc603267c4cabcbe3e5a", "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "title": "Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback", "abstract": "Frozen models trained to mimic static datasets can never improve their performance. Models that can employ internet-retrieval for up-to-date information and obtain feedback from humans during deployment provide the promise of both adapting to new information, and improving their performance. In this work we study how to improve internet-driven conversational skills in such a learning framework. We collect deployment data, which we make publicly available, of human interactions, and collect various types of human feedback \u2013 including binary quality measurements, free-form text feedback, and fine-grained reasons for failure. We then study various algorithms for improving from such feedback, including standard supervised learning, rejection sampling, model-guiding and reward-based learning, in order to make recommendations on which type of feed- back and algorithms work best. We find the recently introduced DIRECTOR model (Arora et al., 2022) shows significant improvements over other existing approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-08-05", "journal": {"pages": "13557-13572"}, "authors": [{"authorId": "2155954521", "name": "Jing Xu"}, {"authorId": "1666356365", "name": "Megan Ung"}, {"authorId": "100653935", "name": "M. Komeili"}, {"authorId": "153695382", "name": "Kushal Arora"}, {"authorId": "90841478", "name": "Y-Lan Boureau"}, {"authorId": "145183709", "name": "J. Weston"}], "citations": [{"paperId": "0bc0168f2c92861f6711cd5a2e54c027c9c024d5", "title": "Learning from Emotions, Demographic Information and Implicit User Feedback in Task-Oriented Document-Grounded Dialogues"}, {"paperId": "facbc60bc55f5e548031ce1e382a1403c3c16901", "title": "PersonalityChat: Conversation Distillation for Personalized Dialog Modeling with Facts and Traits"}, {"paperId": "485f8a429cf5f70c558181187f2d62e31784deaa", "title": "Reasons to Reject? Aligning Language Models with Judgments"}, {"paperId": "d0b5d78e562e426519469e1f4ec84ee3b34e6fbe", "title": "What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception"}, {"paperId": "5cc33a8226f733ccff559dab44f84be50c1c2235", "title": "Cognitively Inspired Components for Social Conversational Agents"}, {"paperId": "653fb0fca0d436428f10e4e6186788eff609fe39", "title": "Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task"}, {"paperId": "65c6bf21f089a9226e2ade7990f8a1cda35ec944", "title": "Learning From Free-Text Human Feedback - Collect New Datasets Or Extend Existing Ones?"}, {"paperId": "8a7dcc4699b803b5eb10a6db797406b3106c424b", "title": "Social Commonsense-Guided Search Query Generation for Open-Domain Knowledge-Powered Conversations"}, {"paperId": "dd7a74a09fc29cadcd47fafc4f7812bb8d2d7208", "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values"}, {"paperId": "f4583c9d0f7330ee237a1a3e285af58ed6eda9bf", "title": "Hexa: Self-Improving for Knowledge-Grounded Dialogue System"}, {"paperId": "16342fd115398bb1bcf2f16baf94f9d4d122d480", "title": "Let Me Teach You: Pedagogical Foundations of Feedback for Language Models"}, {"paperId": "d1fb9013ed41c67b18755f01f0b7c7c5ef0a047f", "title": "System-Level Natural Language Feedback"}, {"paperId": "4bd35d344c635b05f97f4d749741d196ff541bf3", "title": "A Primer on Seq2Seq Models for Generative Chatbots"}, {"paperId": "0b6edce3dde7e502c6b7c6d83bac0230ec912482", "title": "Improving Open Language Models by Learning from Organic Interactions"}, {"paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc", "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"}, {"paperId": "627964b3dc7211f81902afeaa79d9357b7b2440c", "title": "Continually Improving Extractive QA via Human Feedback"}, {"paperId": "74b05bba46db21e589a2cc0f916f81069b0368ef", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"}, {"paperId": "c11810fa8887b678facea62da4607c4898360308", "title": "Training Language Models with Language Feedback at Scale"}, {"paperId": "e5174aeda1baa67c17f4ac630ae2e44453954cc3", "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback"}, {"paperId": "2029349c55c1dba3493c5b3bd25152f18ba21ae2", "title": "Augmented Language Models: a Survey"}, {"paperId": "5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691", "title": "PLACES: Prompting Language Models for Social Conversation Synthesis"}, {"paperId": "4ea413e5a21a743d68c92e7f169535d0543f6051", "title": "On Improving Summarization Factual Consistency from Natural Language Feedback"}, {"paperId": "83562af413d730b9321efe8bea24058514ac940b", "title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation"}, {"paperId": "4d81c33b295c092016ac236cfd32020a5bb70b97", "title": "Optimizing Prompts for Text-to-Image Generation"}, {"paperId": "795736777f08e92a80c95dab7f205d1d7c28a10b", "title": "The CRINGE Loss: Learning what language not to model"}, {"paperId": "2fa11f97ae084591d79013c0cbc65d9931d976df", "title": "When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels"}, {"paperId": "cd6652fe413d57d05b44e0f3aa036c54f0eef464", "title": "Towards Boosting the Open-Domain Chatbot with Human Feedback"}, {"paperId": "a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd", "title": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage"}, {"paperId": "4c868a92c615df3859433e28b2441bbce9e65fb3", "title": "Reward Reports for Reinforcement Learning"}, {"paperId": "4fa7eeccfb0b13418db34b158cd6095251d75f1b", "title": "Efficient Latent Variable Modeling for Knowledge-Grounded Dialogue Generation"}]}
