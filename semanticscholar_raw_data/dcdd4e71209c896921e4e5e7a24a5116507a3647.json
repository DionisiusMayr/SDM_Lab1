{"paperId": "dcdd4e71209c896921e4e5e7a24a5116507a3647", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Accelerating Deep Learning Inference via Learned Caches", "abstract": "Deep Neural Networks (DNNs) are witnessing increased adoption in multiple domains owing to their high accuracy in solving real-world problems. However, this high accuracy has been achieved by building deeper networks, posing a fundamental challenge to the low latency inference desired by user-facing applications. Current low latency solutions trade-off on accuracy or fail to exploit the inherent temporal locality in prediction serving workloads. We observe that caching hidden layer outputs of the DNN can introduce a form of late-binding where inference requests only consume the amount of computation needed. This enables a mechanism for achieving low latencies, coupled with an ability to exploit temporal locality. However, traditional caching approaches incur high memory overheads and lookup latencies, leading us to design learned caches - caches that consist of simple ML models that are continuously updated. We present the design of GATI, an end-to-end prediction serving system that incorporates learned caches for low-latency DNN inference. Results show that GATI can reduce inference latency by up to 7.69X on realistic workloads.", "venue": "arXiv.org", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-01-18", "journal": {"name": "ArXiv", "volume": "abs/2101.07344"}, "authors": [{"authorId": "50826488", "name": "Arjun Balasubramanian"}, {"authorId": "2125424319", "name": "Adarsh Kumar"}, {"authorId": "2108173451", "name": "Yuhan Liu"}, {"authorId": null, "name": "Han Cao"}, {"authorId": "2697906", "name": "S. Venkataraman"}, {"authorId": "152426179", "name": "Aditya Akella"}], "citations": [{"paperId": "41c5e8eeafde70bdfc51aef5ea4a3e66b7572ccc", "title": "Serving Deep Learning Model in Relational Databases"}, {"paperId": "e11bd8c43a4d9f18a59e06a6ffd5e0c7f3eec95a", "title": "Fast Video Classification based on unidirectional temporal differences based dynamic spatial selection with custom loss function and new class suggestion"}, {"paperId": "37b7a2b57d8a5c75d89c35a4839b207e787f9f7e", "title": "GRADES: Gradient Descent for Similarity Caching"}, {"paperId": "47993d0dcca76d194cf1c2382f5f4e3c9207f823", "title": "Improving the Performance of DNN-based Software Services using Automated Layer Caching"}, {"paperId": "c9de8b052562e03ebe8445a14d9cb3cb08e85fd6", "title": "Enabling All In-Edge Deep Learning: A Literature Review"}, {"paperId": "e15a1d52b3702366fa915ff62018f9f2a74f463a", "title": "Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing"}, {"paperId": "a173c0d0d6cd7581f5e9c0b95171ffe21f67396a", "title": "Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning"}, {"paperId": "304935aef59d535238d48d935280aafe4a53ff68", "title": "Serving DNN Models with Multi-Instance GPUs: A Case of the Reconfigurable Machine Scheduling Problem"}, {"paperId": "b3c5bc18c6eb1cc0d8142f30e9fa4ad524ab2fb2", "title": "GRADES: Gradient Descent for Similarity Caching"}, {"paperId": "987d599084f580313ac269b608dc165cf3c37460", "title": "A Survey of Machine Learning-Based System Performance Optimization Techniques"}, {"paperId": "d473c4de05951f135d947ae8b32933c933c8abeb", "title": "Metadata Representations for Queryable Repositories of Machine Learning Models"}, {"paperId": "9b8aabefe970826aa3c9e5ea21586ebb45748c0e", "title": "Enabling Deep Learning for All-in EDGE paradigm"}, {"paperId": "2cd75e5e91294ddaab0abcd153b70211e556ddfb", "title": "Learning Future Reference Patterns for Efficient Cache Replacement Decisions"}]}
