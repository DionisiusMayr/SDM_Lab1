{"paperId": "94f02394a8f019d7ece7eb9612e96253ba97f30c", "publicationVenue": null, "title": "Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents", "abstract": "At the heart of improving conversational AI is the open problem of how to evaluate conversations. Issues with automatic metrics are well known (Liu et al., 2016), with human evaluations still considered the gold standard. Unfortunately, how to perform human evaluations is also an open problem: differing data collection methods have varying levels of human agreement and statistical sensitivity, resulting in differing amounts of human annotation hours and labor costs. In this work we compare five different crowdworker-based human evaluation methods and find that different methods are best depending on the types of models compared, with no clear winner across the board. While this highlights the open problems in the area, our analysis leads to advice of when to use which one, and possible future directions.", "venue": "NLP4CONVAI", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-01-12", "journal": {"name": "ArXiv", "volume": "abs/2201.04723"}, "authors": [{"authorId": "51324296", "name": "Eric Michael Smith"}, {"authorId": "2149798298", "name": "Orion Hsu"}, {"authorId": "2149798086", "name": "Rebecca Qian"}, {"authorId": "3849208", "name": "Stephen Roller"}, {"authorId": "90841478", "name": "Y-Lan Boureau"}, {"authorId": "145183709", "name": "J. Weston"}], "citations": [{"paperId": "3add49104cb143ab85c0ce1614aa2701a5ebfcf2", "title": "Large Language Model based Situational Dialogues for Second Language Learning"}, {"paperId": "facbc60bc55f5e548031ce1e382a1403c3c16901", "title": "PersonalityChat: Conversation Distillation for Personalized Dialog Modeling with Facts and Traits"}, {"paperId": "a9ef5e89ecb9c77e21fad62e8092a9072cd23256", "title": "Dialogue Quality and Emotion Annotations for Customer Support Conversations"}, {"paperId": "6ade002e90f638b86c18578e55dcd25e3490a5b9", "title": "DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment"}, {"paperId": "8dd279a7efc611906d5547e29eb19fc25a5505fd", "title": "Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation"}, {"paperId": "b64b2d28e00e1bdf35393856707cbd133058abab", "title": "xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark"}, {"paperId": "40159a2ea0b33aba994c235be7e8a85da9869055", "title": "Predicting Interaction Quality Aspects Using Level-Based Scores for Conversational Agents"}, {"paperId": "83383f54e8820ad90679ac403e73bf098249bccf", "title": "Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features"}, {"paperId": "f1eeba530be0ac537bcdf633ee6c462045c97fb5", "title": "Towards Credible Human Evaluation of Open-Domain Dialog Systems Using Interactive Setup"}, {"paperId": "4bd35d344c635b05f97f4d749741d196ff541bf3", "title": "A Primer on Seq2Seq Models for Generative Chatbots"}, {"paperId": "0b6edce3dde7e502c6b7c6d83bac0230ec912482", "title": "Improving Open Language Models by Learning from Organic Interactions"}, {"paperId": "49ae59b1375cde60c1da89aee7bad569c4683e23", "title": "The HCI Aspects of Public Deployment of Research Chatbots: A User Study, Design Recommendations, and Open Challenges"}, {"paperId": "4178de4fff1eadccf552aaa944639e027e901c8d", "title": "Psychological Metrics for Dialog System Evaluation"}, {"paperId": "4f480bae3196dbbc27ab383bce33478ea963f9b3", "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models"}, {"paperId": "f2b3483aa5995c33b7a290869dc6402b33963e18", "title": "Hint of Thought prompting: an explainable and zero-shot approach to reasoning tasks with LLMs"}, {"paperId": "417df88083ece44fc6641478543a834063a75d13", "title": "Talking with Machines: A Comprehensive Survey of Emergent Dialogue Systems"}, {"paperId": "700da3f3758e053c379f905bebee261ba69f1073", "title": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation"}, {"paperId": "82beb8a86d438e85a134182128d47607b1b04004", "title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models"}, {"paperId": "4cbdafe5aa262f259bf03003949bbb78ddb54dce", "title": "Diving Deep into Modes of Fact Hallucinations in Dialogue Systems"}, {"paperId": "f78fe02f681a0a9a6867b007bd39e3884de64a91", "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization"}, {"paperId": "a640cdafc10181517b7694ab589db515595b3490", "title": "Evaluating Human-Language Model Interaction"}, {"paperId": "e8059434aa997cf486e6ae83cfbf355d4829a95c", "title": "PoE: A Panel of Experts for Generalized Automatic Dialogue Assessment"}, {"paperId": "f6a6b32b61512b62eddac6097769a3d84e9a69cc", "title": "Don\u2019t Forget Your ABC\u2019s: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems"}, {"paperId": "a6f171598db5a21ece1ac38010c48df19b2b23ca", "title": "FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation"}, {"paperId": "1d8f36b95f51af99fd5c7c4527b9b7ad598b15f4", "title": "Keep Me Updated! Memory Management in Long-term Conversations"}, {"paperId": "cd6652fe413d57d05b44e0f3aa036c54f0eef464", "title": "Towards Boosting the Open-Domain Chatbot with Human Feedback"}, {"paperId": "21806eb8d48550508756faa7621e36d68e164693", "title": "Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems"}, {"paperId": "45c55fe92abc0ed4c2190fe039c9a23d70a0e33a", "title": "Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges"}, {"paperId": "de4635e95259118a545fdc0682407f416c16086a", "title": "AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation"}, {"paperId": "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb", "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text"}, {"paperId": "e0af8f2dd390fabcdf2c373640833efc62faa530", "title": "FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows"}, {"paperId": "d29036946152bddf950fec7a08c2828a8a8f902e", "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems"}, {"paperId": "5bf9e92343dae42bef58d3af491b41babc892f00", "title": "Are Experts Needed? On Human Evaluation of Counselling Reflection Generation"}, {"paperId": "66490616f9992c473347cb05cd05a46e6d897da2", "title": "Human-Centered Metrics for Dialog System Evaluation"}, {"paperId": "cf5cf89c748a500c2bf2ec4c9022c8818a4b569a", "title": "Uncertainty-aware Automatic Evaluation Method for Open-domain Dialogue Systems"}, {"paperId": "baba91591fb7e69ef5251a180b51001e0be16d5a", "title": "FFAEval: Evaluating Dialogue System via Free-For-All Ranking"}, {"paperId": "e1693c1430d3e0c60ec3af5c9b423770c5337fa7", "title": "Evaluation of Response Generation Models: Shouldn\u2019t It Be Shareable and Replicable?"}]}
