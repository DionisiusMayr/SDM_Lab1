{"paperId": "b9ea1cce667cc373ee57e2ecd4ef368a268cb3c5", "publicationVenue": null, "title": "FlexEnt: Entropy Coding to Curb Stragglers in Large-Scale Distributed Machine Learning", "abstract": "Distributed Machine Learning training is emerging in today\u2019s datacenters. The training time of a distributed ML job is closely tied to the parallelism technique used to distribute the job across worker nodes, as well as the available compute, storage, and network capabilities in the datacenter. Current parallelism techniques often implicitly make two simplifying assumptions: (i) datacenter nodes are homogeneous and (ii) there is little to no congestion in the network. However, at scale, these assumptions do not always hold and stragglers are a pervasive challenge in today\u2019s datacenter job scheduling. A straw-man approach to reduce the impact of stragglers is to replicate (sometimes speculatively) each sub-task. However, this approach commonly incorporates a timeout component to recover from straggler sub-tasks. In this work, we address the problem of how to curb the impact of stragglers in distributed ML jobs without having to speculate. Our approach leverages the unique nature of image recognition workloads and the theory of graph entropy. Our proposal, FlexEnt, identifies the entropy present in a training batch using a set of \u201cfeature filters\u201d and replicates only a fraction of the training data on a predetermined set of \u201cshadow workers\u201d. In doing so, FlexEnt will decode the training parameters as soon as a sub-set of sub-tasks are completed. This approach will enable datacenter operators to adjust a knob between the number of additional shadow workers and the degree of stragglers protection.", "venue": "", "year": 2019, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2086697093", "name": "James Salamy Ayush Sharma Manya Ghobadi Muriel M\u00e9dard"}], "citations": [{"paperId": "72b6e0a2e834d0f6b93fc8ea152b234b16d23165", "title": "Ultima: Robust and Tail-Optimal AllReduce for Distributed Deep Learning in the Cloud"}]}
