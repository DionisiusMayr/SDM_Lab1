{"paperId": "582c2f270a6c0ce89679eebaa78797711fa20293", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Making Large Language Models Perform Better in Knowledge Graph Completion", "abstract": "Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs and enrich the KGs to become better web infrastructure, which can benefit a lot of web-based automatic services. However, research about LLM-based KGC is limited and lacks effective utilization of LLM's inference capabilities, which ignores the important structural information in KGs and prevents LLMs from acquiring accurate factual knowledge. In this paper, we discuss how to incorporate the helpful KG structural information into the LLMs, aiming to achieve structrual-aware reasoning in the LLMs. We first transfer the existing LLM paradigms to structural-aware settings and further propose a knowledge prefix adapter (KoPA) to fulfill this stated goal. KoPA employs structural embedding pre-training to capture the structural information of entities and relations in the KG. Then KoPA informs the LLMs of the knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens as a prefix of the input prompt. We conduct comprehensive experiments on these structural-aware LLM-based KGC methods and provide an in-depth analysis comparing how the introduction of structural information would be better for LLM's knowledge reasoning ability. Our code is released at https://github.com/zjukg/KoPA.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-10", "journal": {"name": "ArXiv", "volume": "abs/2310.06671"}, "authors": [{"authorId": "2118158068", "name": "Yichi Zhang"}, {"authorId": "2283992213", "name": "Zhuo Chen"}, {"authorId": "2256109363", "name": "Wen Zhang"}, {"authorId": "14499025", "name": "Hua-zeng Chen"}], "citations": [{"paperId": "866234cef500959cc85f948e00b770255b482246", "title": "Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs"}, {"paperId": "1ffa2b384fd003c325e03b61b33728039fa0fb55", "title": "Enhancing Error Detection on Medical Knowledge Graphs via Intrinsic Label"}, {"paperId": "de5d87fe1c35906c3f84d3da5d9f854922d66302", "title": "ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases"}, {"paperId": "eff9d7ed06f30f121d30ee13802a11f172ef66f4", "title": "Demystifying Chains, Trees, and Graphs of Thoughts"}]}
