{"paperId": "7a5b44ea10a51708e18786595c8d70b18950da11", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios", "abstract": "The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-07-25", "journal": {"name": "ArXiv", "volume": "abs/2307.13528"}, "authors": [{"authorId": "2047713083", "name": "Ethan Chern"}, {"authorId": "2224851117", "name": "Steffi Chern"}, {"authorId": "2108956946", "name": "Shiqi Chen"}, {"authorId": "30300197", "name": "Weizhe Yuan"}, {"authorId": "2224772135", "name": "Kehua Feng"}, {"authorId": "2110714400", "name": "Chunting Zhou"}, {"authorId": "6215698", "name": "Junxian He"}, {"authorId": "1700325", "name": "Graham Neubig"}, {"authorId": "144118452", "name": "Pengfei Liu"}], "citations": [{"paperId": "79f9dac419693bb065e9ef0077389f24ae2207e6", "title": "AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis"}, {"paperId": "86f2f055218b9a8b1ab867bf3889f87b16da73f4", "title": "MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation"}, {"paperId": "ab8e6df5001dbb9b48445220099425aff536b3e8", "title": "Knowledge Conflicts for LLMs: A Survey"}, {"paperId": "e4cbfe9c868f405079613ade118703767b1da842", "title": "ALTO: An Efficient Network Orchestrator for Compound AI Systems"}, {"paperId": "8c5acaafe43e710d55b08c63d567550ad26ec437", "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification"}, {"paperId": "03cfdde24c6b9837ad8933cb535a2c4a7c0fd971", "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild"}, {"paperId": "cc72e18fa40327fa616fd348b87592bf9cc60e5b", "title": "Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore"}, {"paperId": "ce6aaee7872ec9e26024916df680a379f655a840", "title": "Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses"}, {"paperId": "44d0c9a483b0af2f3952ae9acdde3d091472bc69", "title": "Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions"}, {"paperId": "d38af39275524068e7aab12fa8d54d342eff7dfe", "title": "UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models"}, {"paperId": "a7d047dd9f41d5f3e7eaa39e5ba8c97cccc7276d", "title": "Reformatted Alignment"}, {"paperId": "bfc7762ab90d18bdb687d93723c51e9827be254a", "title": "Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models"}, {"paperId": "25243632a6159c19db280e2f0064aa59562a518a", "title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models"}, {"paperId": "4bb30cdd7216d0bde3f937489ec04b161b44977c", "title": "Comparing Hallucination Detection Metrics for Multilingual Generation"}, {"paperId": "9b204030b6baa4f8913595b4582dfc10dba8ba31", "title": "Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States"}, {"paperId": "19e909f88b8b9b0635bd6e441094e1738c3bba9a", "title": "Unified Hallucination Detection for Multimodal Large Language Models"}, {"paperId": "2fb593ca4b6d2631832d6424e238c32db3db5434", "title": "Factuality of Large Language Models in the Year 2024"}, {"paperId": "6e2704025046be6dc29b71339994422f9a9cacf1", "title": "Building Guardrails for Large Language Models"}, {"paperId": "0f51d47871d99cda3e9eaf4ae1c9c7025ae76325", "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains"}, {"paperId": "9c20d8d5cfc60f5b9aa058ff2968563f2af33398", "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models"}, {"paperId": "028d75496e51943f52c7b2177344a3c089c18058", "title": "Fine-grained Hallucination Detection and Editing for Language Models"}, {"paperId": "7ed27977b0ba1264ea5d6e68be391a841dc97f90", "title": "The Critique of Critique"}, {"paperId": "1b387e3fbec0447c8bf2dcee21f6db59cdddf698", "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models"}, {"paperId": "19174942f77857fc9308a877781ce9d4d1944ba9", "title": "Large Language Models for Social Networks: Applications, Challenges, and Solutions"}, {"paperId": "cfce709a65f90312d2bdc1a6cf0380c19becf694", "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models"}, {"paperId": "3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e", "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations"}, {"paperId": "6674e254f343cdb2511b84d4bf120182fb112b67", "title": "Evaluating Large Language Models for Health-related Queries with Presuppositions"}, {"paperId": "4073610a5fbe02b5f40bf6253509c03bf1bb2c62", "title": "E&V: Prompting Large Language Models to Perform Static Analysis by Pseudo-code Execution and Verification"}, {"paperId": "6cfbbf7604adda1df65932e3c4d157770a2df000", "title": "Alignment for Honesty"}, {"paperId": "936f7f0fa77efcd322805b93a8d74c48a4108290", "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?"}, {"paperId": "612cb53584b23e2066631d143e09b471852afaeb", "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting"}, {"paperId": "18b6cf8914e1216445b5b33a36f8f3ca9084b639", "title": "Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output"}, {"paperId": "b10482ab3dd1d340c3c926d92c3e617c24ee3949", "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification"}, {"paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257", "title": "Fine-tuning Language Models for Factuality"}, {"paperId": "6233b5863f9a0e8bacce47ce21bc3e81c09497bd", "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning"}, {"paperId": "3bddd0073728e6ebd8a139a93c85e1618187f58d", "title": "LLatrieval: LLM-Verified Retrieval for Verifiable Generation"}, {"paperId": "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8", "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"}, {"paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2", "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"}, {"paperId": "5d3bdae6cfb2239af70dfe8ae5cf1a4958330ac4", "title": "A Survey of Large Language Models Attribution"}, {"paperId": "45653ad43124f02dc2cf2db3357be1d1d78ddb18", "title": "Language Models Hallucinate, but May Excel at Fact Verification"}, {"paperId": "a09d3686285f609b60b18b5935da64b29a5cce0f", "title": "A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions"}, {"paperId": "b3474a0bfddd48d46abe2214f42e660b569cc4aa", "title": "FactCHD: Benchmarking Fact-Conflicting Hallucination Detection"}, {"paperId": "5689b9b7b465f6adb6dc0c63dce90804636f02a6", "title": "Quantifying Self-diagnostic Atomic Knowledge in Chinese Medical Foundation Model: A Computational Analysis"}, {"paperId": "3bddda884c5264d2d3ae7087c2d570243dbe1db4", "title": "MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities"}, {"paperId": "03fab98a9be74a253688840dba9144737a8ca92d", "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity"}, {"paperId": "29652bb2dc0396ab27c0be0c5f24c114c757df0f", "title": "Evaluating Hallucinations in Chinese Large Language Models"}, {"paperId": "bb3cc013c462ff2bf3dc5be90f731ebf34996f86", "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models"}, {"paperId": "e61a96cf602ebff6683929aaf916e25614a475bc", "title": "UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "6f75e8b61f13562237851d8119cb2f9d49e073fb", "title": "Can LLM-Generated Misinformation Be Detected?"}, {"paperId": "4b0b56be0ae9479d2bd5c2f0943db1906343c10f", "title": "Chain-of-Verification Reduces Hallucination in Large Language Models"}, {"paperId": "396305230ddcf915b19a19683a89e34d76321a33", "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models"}, {"paperId": "d00735241af700d21762d2f3ca00d920241a15a4", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"}, {"paperId": "d09d4c66cea1bc4b0405206092715ccd4ec6cb44", "title": "Answering Unseen Questions With Smaller Language Models Using Rationale Generation and Dense Retrieval"}, {"paperId": "ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04", "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"}, {"paperId": "d1fb9013ed41c67b18755f01f0b7c7c5ef0a047f", "title": "System-Level Natural Language Feedback"}, {"paperId": "85b5068d3e1364b44ec9f46b0930b521b4089df6", "title": "Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models"}, {"paperId": "4df7ab6d581e622e28b4e9929f5d69e3cd0287c8", "title": "Quantifying Self-diagnostic Atomic Knowledge in Chinese Medical Foundation Model: A Computational Analysis"}]}
