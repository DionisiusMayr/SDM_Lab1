{"paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models", "abstract": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-21", "journal": {"name": "ArXiv", "volume": "abs/2309.12284"}, "authors": [{"authorId": "2112584251", "name": "L. Yu"}, {"authorId": "2152123946", "name": "Weisen Jiang"}, {"authorId": "152751416", "name": "Han Shi"}, {"authorId": "2193887687", "name": "Jincheng Yu"}, {"authorId": "2239065052", "name": "Zhengying Liu"}, {"authorId": "2153638098", "name": "Yu Zhang"}, {"authorId": "2243335442", "name": "James T. Kwok"}, {"authorId": "121544682", "name": "Zheng Li"}, {"authorId": "145689461", "name": "Adrian Weller"}, {"authorId": "2243412679", "name": "Weiyang Liu"}], "citations": [{"paperId": "3bb853fe6bdbe889a8715c93f78dae01cf1bc65c", "title": "Evaluating Mathematical Reasoning Beyond Accuracy"}, {"paperId": "3fb1818b9ab3f8d36ed3f8329087058f561c05da", "title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline"}, {"paperId": "ee4014497ccf2f65d6e05d3956b0e6b0c7369bae", "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"}, {"paperId": "a6b624ada5db7bfc7e074eff35e246c08a940576", "title": "Stable Code Technical Report"}, {"paperId": "c00c3fd40e4ab89faa14c0d2d34e0ea5de8e3608", "title": "Exploring the Mystery of Influential Data for Mathematical Reasoning"}, {"paperId": "b30e8ef07654aa5f52ad26f6fbb1fa5ba7f9573f", "title": "Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization"}, {"paperId": "0659e2a187f43c5fa86632f3003f8b0d13dac329", "title": "Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization"}, {"paperId": "3f75c088b44be491fa1060a0eb3064f61dfe70c3", "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning"}, {"paperId": "02ac355296f001a010b1db115d909c052767ccb3", "title": "Stable LM 2 1.6B Technical Report"}, {"paperId": "f4ded695170b02238214a9078be47116d8bf35bd", "title": "LLM-Resistant Math Word Problem Generation via Adversarial Attacks"}, {"paperId": "30e33e9ec257a60d8f095a285f31c702511db04a", "title": "FuseChat: Knowledge Fusion of Chat Models"}, {"paperId": "0334987f094121c094d5043ab38f14ebf5852c05", "title": "DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents"}, {"paperId": "3d4fbb81345bac2a4ec3e6d89e36adf42b214fae", "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models"}, {"paperId": "ad2be51acf42f686a8d1de92d7435d84274ee62d", "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math"}, {"paperId": "2cb1933e7159a7cd2cd759c322e1973e28868cc7", "title": "Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs"}, {"paperId": "28d5e9fb1190e0cd0efcc8206bb73d7112904910", "title": "Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts"}, {"paperId": "574568e9403ca38d7b3e3c243e2a55ceeecc2ba8", "title": "Autonomous Data Selection with Language Models for Mathematical Texts"}, {"paperId": "af1d4a4141eaf52384ca85f1d468443f8f3d5c06", "title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning"}, {"paperId": "2ef23e8b3c07885a850164d86ba748303e335377", "title": "CultureLLM: Incorporating Cultural Differences into Large Language Models"}, {"paperId": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9", "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning"}, {"paperId": "35b142ea69598e6241f0011312128031df55895c", "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"}, {"paperId": "ba1ce9f45b5a84d8c8609c1db23aebc887c0ae4d", "title": "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision"}, {"paperId": "139f3e52d1b90992d6908497ed8d6535bad258a2", "title": "H2O-Danube-1.8B Technical Report"}, {"paperId": "675629ff78cef09665a1135fece66195ed80a640", "title": "MARIO: MAth Reasoning with code Interpreter Output - A Reproducible Pipeline"}, {"paperId": "135521c4432193178966a5ec24343f1a1599d541", "title": "VLLaVO: Mitigating Visual Gap through LLMs"}, {"paperId": "7260442ef9c0448f07ce3803efd49cebaffcebe9", "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"}, {"paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5", "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"}, {"paperId": "fee61c03fd5ad4a8d8bcdec5bcdfacfe25b361d9", "title": "Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges"}, {"paperId": "82730762d438b58a8af874d03e9903a73e18a39e", "title": "TinyGSM: achieving >80% on GSM8k with small language models"}, {"paperId": "4ba57555bef02f988f2ed3bab2f102733dc55221", "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations"}, {"paperId": "1e175ebd9f246ae096355afe6b12b4a3c514afff", "title": "Dual Branch Network Towards Accurate Printed Mathematical Expression Recognition"}, {"paperId": "5ee871537ae51e7e2e93d2a70fff5d100649a655", "title": "Mathematical Language Models: A Survey"}, {"paperId": "f502cf5c371f3768362816e2fefca1e8d2751341", "title": "Meta Prompting for AI Systems"}, {"paperId": "5781ddcd249cbee5032824d86e36dda506c62c3e", "title": "Nonparametric Teaching for Multiple Learners"}, {"paperId": "55f1cde49846c58b0bedebde15b8f7d939f39432", "title": "Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation of the Reversal Curse"}, {"paperId": "46a81d17a4f1b4c4fc3f3215ccfb2fc2921cecaa", "title": "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization"}, {"paperId": "98b607e7cb84e1a5c87c8a49562ae35435e6722d", "title": "Learning From Mistakes Makes LLM Better Reasoner"}, {"paperId": "40e2efc3d4900f7a1471d96e8fa668722b7cedfa", "title": "The Expressibility of Polynomial based Attention Scheme"}, {"paperId": "a3241277bde215e738b0edd179b705a751a0f947", "title": "Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models"}, {"paperId": "e3d9481a3d3419905f2f7b963f5e925e886ef58b", "title": "AI for Mathematics: A Cognitive Science Perspective"}, {"paperId": "b16c7d45183b9d595ab64301be019741b1528860", "title": "Llemma: An Open Language Model For Mathematics"}, {"paperId": "34ca51ce10e8d1d6c950ba519329714a0184d004", "title": "KwaiYiiMath: Technical Report"}, {"paperId": "e93562137240873bf1262e769dd9d73c2dcba858", "title": "Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization"}, {"paperId": "f8b5ee53c3410f20049e7def47bd52403fa388e3", "title": "LEGO-Prover: Neural Theorem Proving with Growing Libraries"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9", "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"}, {"paperId": "888728745dbb769e29ed475d4f7661eebe1a71cf", "title": "A Survey on Evaluation of Large Language Models"}, {"paperId": "df7e899a2070d5823b30a58b34ddd9bee6bf0cbb", "title": "Matrix Information Theory for Self-Supervised Learning"}, {"paperId": "e277bef6465198c2ea2786dbb832834cd9a0dfc3", "title": "M ATH -S HEPHERD : A L ABEL -F REE S TEP - BY -S TEP V ERIFIER FOR LLM S IN M ATHEMATICAL R EASONING"}]}
