{"paperId": "7fde8fdbf666fb2a42a9ce4c579dd4cd1236e183", "publicationVenue": {"id": "0c1420a4-aa9b-44f9-b264-ba3ac5c37050", "name": "International Conference for High Performance Computing, Networking, Storage and Analysis", "alternate_names": ["Int Conf High Perform Comput Netw Storage Anal"], "issn": "2167-4337", "alternate_issns": ["2167-4329"], "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000729"}, "title": "RIBBON: Cost-Effective and QoS-Aware Deep Learning Model Inference using a Diverse Pool of Cloud Computing Instances", "abstract": "Deep learning model inference is a key service in many businesses and scientific discovery processes. This paper introduces Ribbon, a novel deep learning inference serving system that meets two competing objectives: quality-of-service (QoS) target and cost-effectiveness. The key idea behind Ribbon is to intelligently em-ploy a diverse set of cloud computing instances (heterogeneous instances) to meet the QoS target and maximize cost savings. Rib-bon devises a Bayesian Optimization-driven strategy that helps users build the optimal set of heterogeneous instances for their model inference service needs on cloud computing platforms - and, Ribbon demonstrates its superiority over existing approaches of inference serving systems using homogeneous instance pools. Ribbon saves up to 16% of the inference service cost for different learning models including emerging deep learning recommender system models and drug-discovery enabling models.", "venue": "International Conference for High Performance Computing, Networking, Storage and Analysis", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2021-11-13", "journal": {"name": "SC21: International Conference for High Performance Computing, Networking, Storage and Analysis", "pages": "1-16"}, "authors": [{"authorId": "2118425325", "name": "Baolin Li"}, {"authorId": "15723239", "name": "Rohan Basu Roy"}, {"authorId": "28293642", "name": "Tirthak Patel"}, {"authorId": "74882299", "name": "V. Gadepally"}, {"authorId": "34998839", "name": "K. Gettings"}, {"authorId": "34966505", "name": "Devesh Tiwari"}], "citations": [{"paperId": "fd2514efb219f3cd49f090374ea32bc62a7b87c3", "title": "HEET: A Heterogeneity Measure to Quantify the Difference across Distributed Computing Systems"}, {"paperId": "acd0b07ba8848876188461c98d6ddc002d2384e5", "title": "Graph3PO: A Temporal Graph Data Processing Method for Latency QoS Guarantee in Object Cloud Storage System"}, {"paperId": "bd6dc5f6d915e320e4fb4d2e27b6f6f014933d97", "title": "Cost-Efficient Serverless Inference Serving with Joint Batching and Multi-Processing"}, {"paperId": "78375c1fe88a6e66b88af90b37476fa6301bd6b4", "title": "mSIRM: Cost-Efficient and SLO-aware ML Load Balancing on Fog and Multi-Cloud Network"}, {"paperId": "a023de0ae831e7b8bad5c60f8332c7bb35d39795", "title": "xCloudServing: Automated ML Serving Across Clouds"}, {"paperId": "e26b43bac071c9c5fd7b31df4cfed4add6ff8d76", "title": "Clover: Toward Sustainable AI with Carbon-Aware Machine Learning Inference Service"}, {"paperId": "4413fa78a0e655f8efdcebfdd4ee1d32ad064e77", "title": "SIRM: Cost efficient and SLO aware ML prediction on Fog-Cloud Network"}, {"paperId": "a2694d1f56f53bf92dccb4f678a445639373385e", "title": "Kairos: Building Cost-Efficient Machine Learning Inference Systems with Heterogeneous Cloud Resources"}, {"paperId": "9160175e3cc86b4097635f8e4c4825b1aa869ded", "title": "DASH: Scheduling Deep Learning Workloads on Multi-Generational GPU-Accelerated Clusters"}, {"paperId": "f806d16e3efd0f3f5b63491a6f6d4a7334df4cee", "title": "TCB: Accelerating Transformer Inference Services with Request Concatenation"}, {"paperId": "82eed40bf760a1fac01c99281ab86a6ce185a72e", "title": "PROFET: PROFiling-based CNN Training Latency ProphET for GPU Cloud Instances"}, {"paperId": "76ae7271caddfaf3114634bc8c6d82544c19a607", "title": "SMLT: A Serverless Framework for Scalable and Adaptive Machine Learning Design and Training"}, {"paperId": "f63291b893b5052f808d177067642f2f9b189404", "title": "Green Carbon Footprint for Model Inference Serving via Exploiting Mixed-Quality Models and GPU Partitioning"}, {"paperId": "77fc7be5a0cdc0141462a9cc00451e73329a13d7", "title": "Building Heterogeneous Cloud System for Machine Learning Inference"}]}
