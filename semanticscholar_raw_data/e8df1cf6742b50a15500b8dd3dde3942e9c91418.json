{"paperId": "e8df1cf6742b50a15500b8dd3dde3942e9c91418", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training", "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by using tree-search algorithms to guide multi-step reasoning. These methods rely on prompting a pre-trained model to serve as a value function and focus on problems with low search depth. As a result, these methods will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning. To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding. TS-LLM distinguishes itself in two key ways. (1) Leveraging a learned value function and AlphaZero-like algorithms, our approach can be generally adaptable to a wide range of tasks, language models of any size, and tasks of varying search depths. (2) Our approach can guide LLMs during both inference and training, iteratively improving the LLM. Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-29", "journal": {"name": "ArXiv", "volume": "abs/2309.17179"}, "authors": [{"authorId": "1443767933", "name": "Xidong Feng"}, {"authorId": "2249533760", "name": "Ziyu Wan"}, {"authorId": "2111875607", "name": "Muning Wen"}, {"authorId": "49021167", "name": "Ying Wen"}, {"authorId": "2244690305", "name": "Weinan Zhang"}, {"authorId": "2256981980", "name": "Jun Wang"}], "citations": [{"paperId": "ac8bc36c253c8593492afc2db620c01b152dc778", "title": "LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models"}, {"paperId": "df9ad7a8e603760a8e2ffe8b4571e8871fc09916", "title": "Reward Guided Latent Consistency Distillation"}, {"paperId": "fa0d056dd585eeffb4333cb55807d357808f8440", "title": "Can Large Language Models Play Games? A Case Study of A Self-Play Approach"}, {"paperId": "ea6c94a2992069abd683f592107c5f873f410c00", "title": "SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning"}, {"paperId": "66d491e0054f92e2959c8adb912b293a1e2af832", "title": "Natural Language Reinforcement Learning"}, {"paperId": "8f7f1431cc26f6332e6c7bc02c622008b78ee2c8", "title": "Entropy-Regularized Token-Level Policy Optimization for Large Language Models"}, {"paperId": "0a51afdcd7cf4f33987d766082a7d3f174936c8a", "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models"}, {"paperId": "ba1ce9f45b5a84d8c8609c1db23aebc887c0ae4d", "title": "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision"}, {"paperId": "24ec095f35bc76e2557a63baf5bcc6dd735aabce", "title": "On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)"}, {"paperId": "4a836363233398c0ac27daee942cb5533f467458", "title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning"}, {"paperId": "288e64e8adb23d81e291a2cb51e3a56b315023b7", "title": "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning"}, {"paperId": "0639e2e209213ecb54eb4d6555e271d070344842", "title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation"}, {"paperId": "9cb7f7415fb0590186a3d903a8d5d7044b7a3fdc", "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback"}, {"paperId": "e6f74f2746a9e8bc90701f2afcf3c47e5e98b2dd", "title": "KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection"}]}
