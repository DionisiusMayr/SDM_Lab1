{"paperId": "4916b3854223e21d5642752457521e290ae18965", "publicationVenue": null, "title": "Eye-tracking for human-centered mixed reality: promises and challenges", "abstract": "Eye-tracking hardware and software are being rapidly integrated into mixed reality (MR) technology. Cognitive science and human-computer interaction (HCI) research demonstrate several ways eye-tracking can be used to gauge user characteristics, intent, and status as well as provide active and passive input control to MR interfaces. In this paper, we argue that eye-tracking can be used to ground MR technology in the cognitive capacities and intentions of users and that such human-centered MR is important for MR designers and engineers to consider. We detail relevant and timely research in eye-tracking and MR and offer suggestions and recommendations to accelerate the development of eye-tracking-enabled human-centered MR, with a focus on recent research findings. We identify several promises that eye-tracking holds for improving MR experiences. In the near term, these include user authentication, gross interface interactions, monitoring visual attention across real and virtual scene elements, and adaptive graphical rendering enabled by relatively coarse eyetracking metrics. In the far term, hardware and software advances will enable gaze-depth aware foveated MR displays and attentive MR user interfaces that track user intent and status using fine and dynamic aspects of gaze. Challenges, such as current technological limitations, difficulties in translating lab-based eye-tracking metrics to MR, and heterogeneous MR use cases are considered alongside cutting-edge research working to address them. With a focused research effort grounded in an understanding of the promises and challenges for eye-tracking, human-centered MR can be realized to improve the efficacy and user experience of MR.", "venue": "AR, VR, MR", "year": 2020, "fieldsOfStudy": ["Engineering", "Computer Science"], "publicationTypes": null, "publicationDate": "2020-02-19", "journal": {"pages": "113100T - 113100T-18", "volume": "11310"}, "authors": [{"authorId": "3172532", "name": "Aaron Gardony"}, {"authorId": "1719686", "name": "R. Lindeman"}, {"authorId": "1909324", "name": "Tad T. Bruny\u00e9"}], "citations": [{"paperId": "10c7998f612f08ea801d1dd05dfa749cbea4f0d7", "title": "Pistol: Pupil Invisible Supportive Tool in the Wild"}, {"paperId": "1bc635a011188e77638137eb97239da5df81a7d6", "title": "A novel adaptive visualization method based on user intention in AR manual assembly"}, {"paperId": "265eca2fe9757301a5431ec9724754f0f71f41df", "title": "Characterizing information access needs in gaze-adaptive augmented reality interfaces: implications for fast-paced and dynamic usage contexts"}, {"paperId": "9128084994f9312349326804a375fd8b860a3800", "title": "Towards learner performance evaluation in iVR learning environments using eye-tracking and Machine-learning"}, {"paperId": "57867e428586cfe3385f499fbe36700bad346cbb", "title": "A Mechanistic Transform Model for Synthesizing Eye Movement Data with Improved Realism"}, {"paperId": "b441b075d78b853b7f5ea50fc4e89b94b1ec613e", "title": "The Metaverse evolution: Toward Future Digital Twin Campuses"}, {"paperId": "02c39c43b18c3b90160ca6b1c5f0302cfb34438f", "title": "A Pilot Study Investigating Student Interaction Preferences in Immersive Virtual Reality Environments"}, {"paperId": "30360aba0c6613296676a4dade82059d8da46725", "title": "Aided target recognition visual design impacts on cognition in simulated augmented reality"}, {"paperId": "3d7767c1023ca29ca9a67e8d9e81fb92f8452b24", "title": "AI and 6G Into the Metaverse: Fundamentals, Challenges and Future Research Trends"}, {"paperId": "0af3c23859ea9e9f56e25904c2d3a3936f7dac50", "title": "HPCGen: Hierarchical K-Means Clustering and Level Based Principal Components for Scan Path Genaration"}, {"paperId": "b4f57225db0820bceb208da6cca0871d4b43475e", "title": "Pistol: Pupil Invisible Supportive Tool to extract Pupil, Iris, Eye Opening, Eye Movements, Pupil and Iris Gaze Vector, and 2D as well as 3D Gaze"}, {"paperId": "68f248a6d43213dbb128f8ce619495c9c52bd4f3", "title": "Technologies for Multimodal Interaction in Extended Reality - A Scoping Review"}, {"paperId": "b00cc531f4872dcff8577fc238f70a088eef3b56", "title": "All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda"}, {"paperId": "991604a8d99700cef181e81dfc85554525e8becf", "title": "Telelife: The Future of Remote Living"}, {"paperId": "6ec41b668075f3053375de627be63b5661867880", "title": "StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR"}, {"paperId": "d0060b75556f153fcfbebb6c6716d1a201825981", "title": "Near-eye display optic deficiencies and ways to overcome them"}, {"paperId": "420f2486930f2efc00906329de88a11da99a43f8", "title": "Detection and Correspondence Matching of Corneal Reflections for Eye Tracking Using Deep Learning"}, {"paperId": "299db57848399826c2d7cfff8773f033e4a5d977", "title": "EToS-1: Eye Tracking on Shopfloors for User Engagement with Automation"}]}
