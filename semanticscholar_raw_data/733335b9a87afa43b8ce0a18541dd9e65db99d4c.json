{"paperId": "733335b9a87afa43b8ce0a18541dd9e65db99d4c", "publicationVenue": {"id": "7431ff67-91dc-41fa-b322-1b1ca657025f", "name": "International Conference on Information and Knowledge Management", "type": "conference", "alternate_names": ["Conference on Information and Knowledge Management", "Conf Inf Knowl Manag", "Int Conf Inf Knowl Manag", "CIKM"], "url": "http://www.cikm.org/"}, "title": "Offline Reinforcement Learning for Mobile Notifications", "abstract": "Mobile notification systems have taken a major role in driving and maintaining user engagement for online platforms. They are interesting recommender systems to machine learning practitioners with more sequential and long-term feedback considerations. Most machine learning applications in notification systems are built around response-prediction models, trying to attribute both short-term impact and long-term impact to a notification decision. However, a user's experience depends on a sequence of notifications and attributing impact to a single notification is not always accurate, if not impossible. In this paper, we argue that reinforcement learning is a better framework for notification systems in terms of performance and iteration speed. We propose an offline reinforcement learning framework to optimize sequential notification decisions for driving user engagement. We describe a state-marginalized importance sampling policy evaluation approach, which can be used to evaluate the policy offline and tune learning hyperparameters. Through simulations that approximate the notifications ecosystem, we demonstrate the performance and benefits of the offline evaluation approach as a part of the reinforcement learning modeling approach. Finally, we collect data through online exploration in the production system, train an offline Double Deep Q-Network and launch a successful policy online. We also discuss the practical considerations and results obtained by deploying these policies for a large-scale recommendation system use-case.", "venue": "International Conference on Information and Knowledge Management", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2022-02-04", "journal": {"name": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management"}, "authors": [{"authorId": "2112499492", "name": "Yiping Yuan"}, {"authorId": "34299063", "name": "A. Muralidharan"}, {"authorId": "38469153", "name": "Preetam Nandy"}, {"authorId": "39819150", "name": "Miao Cheng"}, {"authorId": "51151974", "name": "Prakruthi Prabhakar"}], "citations": [{"paperId": "62f559604c2869bd97d4bdc6c64645e8518c207c", "title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems"}, {"paperId": "30a2a170b95c4600f7452286d40618158bd820f6", "title": "Fair Notification Optimization: An Auction Approach"}, {"paperId": "e4f7c02a750a5701d89501faeba119b31e59ad8f", "title": "Multi-objective Optimization of Notifications Using Offline Reinforcement Learning"}, {"paperId": "95aba584548d75708d5e2c4166a665d73efe182c", "title": "Should I send this notification? Optimizing push notifications decision making by modeling the future"}, {"paperId": "fe0b2ebe2053dfaf61f78c4b7f9d715a0b3a746d", "title": "Personalization for Web-based Services using Offline Reinforcement Learning"}]}
