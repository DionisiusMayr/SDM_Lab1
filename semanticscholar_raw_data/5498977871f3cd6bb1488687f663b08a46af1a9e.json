{"paperId": "5498977871f3cd6bb1488687f663b08a46af1a9e", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "FaaSwap: SLO-Aware, GPU-Efficient Serverless Inference via Model Swapping", "abstract": "Serverless computing has become increasingly popular for machine learning inference. However, current serverless platforms lack efficient support for GPUs, limiting their ability to deliver low-latency inference. In this paper, we propose FaaSwap, a GPU-efficient serverless inference platform. FaaSwap employs a holistic approach to system and algorithm design. It maintains models in main memory and dynamically swaps them onto GPUs upon request arrivals (i.e., late binding), thereby enabling a large number of inference functions to efficiently share a node's GPUs. FaaSwap uses various techniques, including asynchronous API redirection, GPU runtime sharing, pipelined model execution, and efficient GPU memory management, to achieve the optimal performance. We also develop an interference-aware request scheduling algorithm that allows FaaSwap to meet the latency SLOs for individual inference functions. We have implemented FaaSwap as a prototype on a leading commercial serverless platform. Experimental evaluations demonstrate that, with model swapping, FaaSwap can concurrently serve hundreds of functions on a single worker node with 4 V100 GPUs, while achieving inference performance comparable to native execution (where each function runs on a dedicated GPU). When deployed on a 6-node production testbed, FaaSwap meets the latency SLOs for over 1k functions, the maximum that the testbed can handle concurrently.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-06-06", "journal": {"name": "ArXiv", "volume": "abs/2306.03622"}, "authors": [{"authorId": "81661827", "name": "Minchen Yu"}, {"authorId": "2115849670", "name": "Ao Wang"}, {"authorId": "2158192897", "name": "Dong-dong Chen"}, {"authorId": "2219012997", "name": "Haoxuan Yu"}, {"authorId": "2115827402", "name": "Xiaonan Luo"}, {"authorId": "2219545312", "name": "Zhuohao Li"}, {"authorId": "39470164", "name": "W. Wang"}, {"authorId": "2042672", "name": "Ruichuan Chen"}, {"authorId": "2219228041", "name": "Dapeng Nie"}, {"authorId": "2145058732", "name": "Haoran Yang"}], "citations": [{"paperId": "d9e43027a16d01ad9409703eb2237f1832c24014", "title": "ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models"}, {"paperId": "cf4dad59f723c28381310f2d56402872b6b6550e", "title": "Characterizing Network Requirements for GPU API Remoting in AI Applications"}, {"paperId": "e657bc68767368dd6c7131055aaf615d777305bf", "title": "A Survey of Serverless Machine Learning Model Inference"}]}
