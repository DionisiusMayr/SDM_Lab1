{"paperId": "0d86ad66746691d5b7e1537045d7c25e3ea15d4e", "publicationVenue": null, "title": "SURFB OARD : R EPRODUCIBLE P ERFORMANCE A NALYSIS FOR D ISTRIBUTED M ACHINE L EARNING W ORKFLOWS", "abstract": "Large-scale HPC infrastructures are enablers for scienti\ufb01c research in many domains. The recent advances in machine learning (ML) have led to an ever increasing demand for computation power, as well as the design of complex operational work\ufb02ows. Understanding the performance and ef\ufb01ciency of these work\ufb02ows is key to productivity, knowledge and model sharing, and energy ef\ufb01ciency. Even though there have been efforts in studying and designing portability protocols, performance analysis of large-scale ML is still an expert-driven task, tightly locked-in to speci\ufb01c physical and software infrastructure. Much like in other domains, this hinders reproducibility of both results and overall work\ufb02ow performance. To overcome this challenge, we propose the design of a container-based framework for reproducible performance analysis of ML work\ufb02ows at scale. We validate our framework using a case-study on two different large-scale production systems running ML work\ufb02ows. We show empirically that our containerized approach is portable and allows arbitrarily low-level performance evaluation when run on two different, production-based HPC clusters with hundreds of GPUs. We report our \ufb01ndings on widely-used open-source software stacks and datasets and offer practitioners insights into what types of analyses our framework enables. To bene\ufb01t the community, we open-source our software and results.", "venue": "", "year": 2021, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [], "citations": []}
