{"paperId": "f86ce7221ee9f5c43de832be1abeaa6faace663d", "publicationVenue": {"id": "b7aa40ac-729b-49d6-9064-4d1a9480e9a9", "name": "International Symposium on High-Performance Computer Architecture", "type": "conference", "alternate_names": ["HPCA", "High Perform Comput Appl", "Int Symp High-performance Comput Archit", "High Performance Computing and Applications"], "url": "https://web.archive.org/web/*/http://www.hpcaconf.org/"}, "title": "iCache: An Importance-Sampling-Informed Cache for Accelerating I/O-Bound DNN Model Training", "abstract": "Fetching a large amount of DNN training data from storage systems incurs long I/O latency and fetch stalls of GPUs. Importance sampling in DNN training can reduce the amount of data computing on GPUs while maintaining a similar model accuracy. However, existing DNN training frameworks do not have a cache layer that reduces the number of data fetches and manages cached items according to sample importance, resulting in unnecessary data fetches, poor cache hit ratios, and random I/Os when importance sampling is used.In this paper, we design a new importance-sampling-informed cache, namely, iCache, to accelerate I/O bound DNN training jobs. iCache only fetches parts of samples instead of all samples in the dataset. The cache is partitioned into two regions: H-cache and L-cache, which store samples of high importance and low importance respectively. Rather than using recency or frequency, we manage data items in H-cache according to their corresponding sample importance. When there is a cache miss in L-cache, we use sample substitutability and dynamic packaging to improve the cache hit ratio and reduce the number of random I/Os. When multiple concurrent jobs access the same datasets in H-cache, we design a model to assign the relative importance values to cached samples to avoid cache thrashing, which may happen when there is no coordination among the concurrent training jobs. Our experimental results show that iCache has a negligible impact on training accuracy and speeds up the DNN training time by up to 2.0\u00d7 compared to the state-of-the-art caching systems.", "venue": "International Symposium on High-Performance Computer Architecture", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-02-01", "journal": {"name": "2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "pages": "220-232"}, "authors": [{"authorId": "2109955995", "name": "Weijian Chen"}, {"authorId": "2099857", "name": "Shuibing He"}, {"authorId": "2181057100", "name": "Yaowen Xu"}, {"authorId": "2869098", "name": "Xuechen Zhang"}, {"authorId": "2108974695", "name": "Siling Yang"}, {"authorId": "2212272261", "name": "Shuang Hu"}, {"authorId": "1746991", "name": "Xian-He Sun"}, {"authorId": "2146663404", "name": "Gang Chen"}], "citations": [{"paperId": "398bb60aed5d51d2053349f4149c5f44257ff850", "title": "cedar: Composable and Optimized Machine Learning Input Data Pipelines"}, {"paperId": "99bcc0edbdeda56e79d23c83422256b522ed8fd2", "title": "Optimizing the Training of Co-Located Deep Learning Models Using Cache-Aware Staggering"}, {"paperId": "244ab847aee80cc3d0ab96f836f9e83630f5537d", "title": "NeSSA: Near-Storage Data Selection for Accelerated Machine Learning Training"}, {"paperId": "82540c3fdfdfb70e42de14af49b1f5d39981b3da", "title": "SHADE: Enable Fundamental Cacheability for Distributed Deep Learning Training"}, {"paperId": "6d3c385bf43a8592d5a3b1e78605ab9c464be8bc", "title": "This paper is included in the Proceedings of the 21st USENIX Conference on File and Storage Technologies."}]}
