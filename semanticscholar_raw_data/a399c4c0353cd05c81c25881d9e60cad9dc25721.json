{"paperId": "a399c4c0353cd05c81c25881d9e60cad9dc25721", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems", "abstract": "Training and deploying large machine learning (ML) models is time-consuming and requires significant distributed computing infrastructures. Based on real-world large model training on datacenter-scale infrastructures, we show 14~32% of all GPU hours are spent on communication with no overlapping computation. To minimize the outstanding communication latency, in this work, we develop an agile performance modeling framework to guide parallelization and hardware-software co-design strategies. Using the suite of real-world large ML models on state-of-the-art GPU training hardware, we demonstrate 2.24x and 5.27x throughput improvement potential for pre-training and inference scenarios, respectively.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-04", "journal": {"name": "ArXiv", "volume": "abs/2310.02784"}, "authors": [{"authorId": "1481699378", "name": "Samuel Hsia"}, {"authorId": "2253623020", "name": "Alicia Golden"}, {"authorId": "2064056189", "name": "Bilge Acun"}, {"authorId": "2774880", "name": "Newsha Ardalani"}, {"authorId": "2253681376", "name": "Zachary DeVito"}, {"authorId": "2113825170", "name": "Gu-Yeon Wei"}, {"authorId": "2146867407", "name": "David Brooks"}, {"authorId": "2253786890", "name": "Carole-Jean Wu"}], "citations": [{"paperId": "e657bc68767368dd6c7131055aaf615d777305bf", "title": "A Survey of Serverless Machine Learning Model Inference"}]}
