{"paperId": "f78fe02f681a0a9a6867b007bd39e3884de64a91", "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods in Natural Language Processing", "type": "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization", "abstract": "Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-12-20", "journal": {"pages": "12930-12949"}, "authors": [{"authorId": "32609381", "name": "Hyunwoo Kim"}, {"authorId": "2689239", "name": "Jack Hessel"}, {"authorId": "2112504145", "name": "Liwei Jiang"}, {"authorId": "119659229", "name": "Peter West"}, {"authorId": "50085131", "name": "Ximing Lu"}, {"authorId": "7877122", "name": "Youngjae Yu"}, {"authorId": "1557324013", "name": "Pei Zhou"}, {"authorId": "39227408", "name": "Ronan Le Bras"}, {"authorId": "2715920", "name": "Malihe Alikhani"}, {"authorId": "70308241", "name": "Gunhee Kim"}, {"authorId": "2729164", "name": "Maarten Sap"}, {"authorId": "1699545", "name": "Yejin Choi"}], "citations": [{"paperId": "6b65fa6e89a41fa2e7c9fd5e4a5219ca775ba76e", "title": "Dialogue with Robots: Proposals for Broadening Participation and Research in the SLIVAR Community"}, {"paperId": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6", "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference"}, {"paperId": "88d4b0084e030e0feeb5a7aa9ac32eb081c36a46", "title": "Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning"}, {"paperId": "bfc2aee63d20fb19c9a851da9e97fec40c454124", "title": "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs"}, {"paperId": "6b9699058152912efedd9a6d724ec08e0c4c9319", "title": "Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset"}, {"paperId": "387229badf09494d9397585da880c5c5de14812f", "title": "Book2Dial: Generating Teacher-Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots"}, {"paperId": "a04bfaafcd066fb71bb1438a76ca1efcb070fe05", "title": "Ever-Evolving Memory by Blending and Refining the Past"}, {"paperId": "0bf3a1867f7245b8a702093901c66b08b518eafc", "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents"}, {"paperId": "0a7dda04a744b2610a5e2cb3527b1cfccf1b9ba8", "title": "Multi-Cultural Commonsense Knowledge Distillation"}, {"paperId": "bddf62f93b2200d058c08983b5bb16207be73c4d", "title": "TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles"}, {"paperId": "ad4e02784491f9794f6abb76b8982c980f51a6ee", "title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models"}, {"paperId": "c21022a4dca7b1a32d2f79818cb7e51df712372f", "title": "UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset"}, {"paperId": "930b5331056d6e543191866df530ce3d5061c2ed", "title": "The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends"}, {"paperId": "6ecb6f2c4544e3c7f8dfd33a8e2f14735653b6ef", "title": "Improving Dialog Safety using Socially Aware Contrastive Learning"}, {"paperId": "a37daf2fb463fe2e9c895f2e94a055c3d3a655a6", "title": "SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization"}, {"paperId": "0627590d66398578c04975d96bc7fa713d0b18f5", "title": "KAUCUS - Knowledgeable User Simulators for Training Large Language Models"}, {"paperId": "8627a427cb538fa5827cd288cd7c928f209b7c3a", "title": "Contextualization Distillation from Large Language Model for Knowledge Graph Completion"}, {"paperId": "fd5331eb998ce21acb272a3d71ad88cca245e52f", "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation"}, {"paperId": "221a0e0c798d4bbc8a057d869b7251e03f1b0790", "title": "ChatQA: Building GPT-4 Level Conversational QA Models"}, {"paperId": "0bc0168f2c92861f6711cd5a2e54c027c9c024d5", "title": "Learning from Emotions, Demographic Information and Implicit User Feedback in Task-Oriented Document-Grounded Dialogues"}, {"paperId": "5aec6865043cb7c7f281699ae95652e0ff680f09", "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning"}, {"paperId": "facbc60bc55f5e548031ce1e382a1403c3c16901", "title": "PersonalityChat: Conversation Distillation for Personalized Dialog Modeling with Facts and Traits"}, {"paperId": "58ff2dd91baad00f98493901d1f560a5dd76ef5b", "title": "Understanding News Creation Intents: Frame, Dataset, and Method"}, {"paperId": "a86d3692af34ce70fd8caaa319009f85a88fdb4d", "title": "Faithful Persona-based Conversational Dataset Generation with Large Language Models"}, {"paperId": "3cbd663640482ea8ffb6d7f128642b8757dfb35e", "title": "NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation"}, {"paperId": "4e2a3ddeaf5defb9318b43c364b9a9efe3847dcf", "title": "Compressed Context Memory For Online Language Model Interaction"}, {"paperId": "e8b22bf8a78401b3807bcd46fa7c88d0c07f58ba", "title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation"}, {"paperId": "ef7097244ee0cc2d54649e7bec121abf7c628947", "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems"}, {"paperId": "5c51513d331128ed957ee57c3fa87ac4d4ab0852", "title": "Interpretation modeling: Social grounding of sentences by reasoning over their implicit moral judgments"}, {"paperId": "145b706e88a17c1b45588af77c4e0aa9a92bdee3", "title": "Can Language Model Moderators Improve the Health of Online Discourse?"}, {"paperId": "896a584885febb21fb4d287fe78984640cee1eb3", "title": "STEER: Unified Style Transfer with Expert Reinforcement"}, {"paperId": "a925b55e0bf07a68ba7554beabda9fa88cd025c5", "title": "Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks"}, {"paperId": "89512c767e0ca0fe64d12a436c64f15dffdad1e0", "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory"}, {"paperId": "d9bc5cbda85ca2872c01bdf951e88e14d982e10b", "title": "FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning"}, {"paperId": "55990bdf4c0e8dc5ae28a87d2a659ee4b7f56703", "title": "What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations"}, {"paperId": "65c6bf21f089a9226e2ade7990f8a1cda35ec944", "title": "Learning From Free-Text Human Feedback - Collect New Datasets Or Extend Existing Ones?"}, {"paperId": "de9c8eb8c2f42befcdb1e30f394d6f8525f5dfac", "title": "NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation"}, {"paperId": "b1d2a29860e69c6ce9987ddefbe112feb1efa16a", "title": "Large Language Models can Share Images, Too!"}, {"paperId": "8a7dcc4699b803b5eb10a6db797406b3106c424b", "title": "Social Commonsense-Guided Search Query Generation for Open-Domain Knowledge-Powered Conversations"}, {"paperId": "be99898941cfe6e7c98a6185bd6ce28d5ce7b5ce", "title": "Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation"}, {"paperId": "d2af7f63861ad683b061b508316624615bff162d", "title": "Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations"}, {"paperId": "6628f9ee35e36cdfdcac8a46cef4dba8d529a83b", "title": "Character-LLM: A Trainable Agent for Role-Playing"}, {"paperId": "00c19d9818bf093a2eed323d1bd5c763c4f512b9", "title": "Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms"}, {"paperId": "13244fdc42a091f87bc08eaaac2bcfd5883e8d0c", "title": "Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents"}, {"paperId": "675c87c9fed17b6dc1d9734606e12c9d0c46c573", "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining"}, {"paperId": "59472fe8bba99c998fb119fa684a80423d1f2f09", "title": "Adaptive Gating in Mixture-of-Experts based Language Models"}, {"paperId": "2b35b946a8ad64e018c24b283bc1c6c65d36fb67", "title": "Retrieval meets Long Context Large Language Models"}, {"paperId": "d6fac1765415ed0251363da01bfa1b3036320a25", "title": "IntentQA: Context-aware Video Intent Reasoning"}, {"paperId": "6bde8138560851914a9a9426c2a1ec3f11c6509f", "title": "Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data"}, {"paperId": "84a36e19f9394f22b34f79756fa9628a795e02ea", "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset"}, {"paperId": "6cdc91a8a6eaf655e0a228f3fb075d91de3a703e", "title": "Leveraging Large Language Models for Automated Dialogue Analysis"}, {"paperId": "d655f652d02251b45db43181c5e3c73dfc59cd51", "title": "Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties"}, {"paperId": "369bc24b935357d661dbefd66dc3c3b6403c4cb9", "title": "SalesBot 2.0: A Human-Like Intent-Guided Chit-Chat Dataset"}, {"paperId": "b5069352383579c6464d8e5ec34eab693c45f59a", "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets"}, {"paperId": "b34862afacf36e7011d40c67bb67c5ee9cf7da22", "title": "DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI"}, {"paperId": "67f23dec7687692660d8aa1315b9dbc8e1aacf22", "title": "Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems"}, {"paperId": "9699910e7fa8a5cc14ad3f2bb84273f017a99cb1", "title": "Does Collaborative Human-LM Dialogue Generation Help Information Extraction from Human Dialogues?"}, {"paperId": "185ace5661963e2e1eb998e739e4110272a6bb43", "title": "COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements"}, {"paperId": "9afc191c8b2a23467938feb992583f223556ab0c", "title": "Impossible Distillation: from Low-Quality Model to High-Quality Dataset&Model for Summarization and Paraphrasing"}, {"paperId": "d0f108b8f5fcfb81e4354b498f3f8491740ece7e", "title": "COMET-M: Reasoning about Multiple Events in Complex Sentences"}, {"paperId": "9573e2025440219a1d3393664b3c80bda51ac8f4", "title": "Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning"}, {"paperId": "6cd26d124ffeb6ce301ef351aada27fa0852f81b", "title": "MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems"}, {"paperId": "0744580e75a74357e466a57082c85cb42f548aa9", "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning"}, {"paperId": "a122863d239643453195424c04067e89406246e1", "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"}, {"paperId": "83fe73b5f35ab77444b80e2bf6fbd66b55531ad1", "title": "COKE: A Cognitive Knowledge Graph for Machine Theory of Mind"}, {"paperId": "700da3f3758e053c379f905bebee261ba69f1073", "title": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation"}, {"paperId": "ed2221b2260169acf5fe962cf757e46082f85bbf", "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting"}, {"paperId": "3270c3054e6a14c5ee483f690ccda7367dd9a556", "title": "CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population"}, {"paperId": "b63e97330154acece935ffa6901e3f36518e5703", "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study"}, {"paperId": "8c236be5cb8073cb3db317919ceb55130ab66dbe", "title": "Champagne: Learning Real-world Conversation from Large-Scale Web Videos"}, {"paperId": "681cee58cf7e54199191cf9e0baf6851d8356704", "title": "Complex QA and language models hybrid architectures, Survey"}, {"paperId": "5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691", "title": "PLACES: Prompting Language Models for Social Conversation Synthesis"}, {"paperId": "f0ea9e2d3889d37f34743ed1dc64f11e8e0484de", "title": "Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models"}, {"paperId": "2fb1ab005db6c88cb31b9fe0ae04132909a5f7ce", "title": "On-the-fly Denoising for Data Augmentation in Natural Language Understanding"}, {"paperId": "a85ae093657f62bf13bf18b0652402026dd4186d", "title": "DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset"}, {"paperId": "3c476acabd21dae0ae154cb77c6cdb96c6b6496b", "title": "CharmBana: Progressive Responses with Real-Time Internet Search for Knowledge-Powered Conversations"}, {"paperId": "c477438beee6dc81b01f74c3684fde5450cd5c1d", "title": "Controlled Data Augmentation for Training Task-Oriented Dialog Systems with Low Resource Data"}, {"paperId": "ac9d5e5cd77a4f36d9bfd4ee9d4c8089f89990a0", "title": "Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing"}, {"paperId": "8f959ab933f209a3430855c6380b2221ab471a1a", "title": "Dialogue Generation Conditional on Predefined Stories: Preliminary Results"}, {"paperId": "072882184e4f620bcef0b77a713859138bfc37e3", "title": "Advancing Open Domain Dialog: The Fifth Alexa Prize SocialBot Grand Challenge"}, {"paperId": "665d0ab8428e974fcc462727b4d415c08789eded", "title": "Mitigating Harms of LLMs via Knowledge Distillation for a Virtual Museum Tour Guide"}, {"paperId": "611609d35716f831f3cac0179fc17b2c5a50ee61", "title": "Large Language Models as SocioTechnical Systems"}, {"paperId": "a74ea4565bb3e934a32ba1537d7c9230eff322e3", "title": "ConPrompt: Pre-training a Language Model with Machine-Generated Data for Implicit Hate Speech Detection"}, {"paperId": "70cb08ee42867aa3a7983d75bbb4105252a9c1cf", "title": "ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection"}]}
