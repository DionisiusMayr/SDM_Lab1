{"paperId": "5001630bcc65e8e0e621b19625629a2689724743", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Generative Judge for Evaluating Alignment", "abstract": "The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, Auto-J, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, Auto-J outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-09", "journal": {"name": "ArXiv", "volume": "abs/2310.05470"}, "authors": [{"authorId": "46276940", "name": "Junlong Li"}, {"authorId": "2256995981", "name": "Shichao Sun"}, {"authorId": "30300197", "name": "Weizhe Yuan"}, {"authorId": "1657674644", "name": "Run-Ze Fan"}, {"authorId": "2257373887", "name": "Hai Zhao"}, {"authorId": "2256991660", "name": "Pengfei Liu"}], "citations": [{"paperId": "ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0", "title": "Advancing LLM Reasoning Generalists with Preference Trees"}, {"paperId": "e56f14ced9f7ce344ed14bdcb46860ccac72ac83", "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge"}, {"paperId": "67b3569e133e7667c4a3a545294d705294873aca", "title": "RewardBench: Evaluating Reward Models for Language Modeling"}, {"paperId": "30c1f450040b65c23f758dbdee7f1daffe278eef", "title": "Prediction-Powered Ranking of Large Language Models"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "66d491e0054f92e2959c8adb912b293a1e2af832", "title": "Natural Language Reinforcement Learning"}, {"paperId": "05ab4b262a2a343aa505ff3640fdf30b2530ac99", "title": "LLM-based NLG Evaluation: Current Status and Challenges"}, {"paperId": "485f8a429cf5f70c558181187f2d62e31784deaa", "title": "Reasons to Reject? Aligning Language Models with Judgments"}, {"paperId": "ed795e4913449e29ed57fdf92ccd8c7d109e1cfb", "title": "RIGHT: Retrieval-augmented Generation for Mainstream Hashtag Recommendation"}, {"paperId": "36b88e6cf9cd5fa4809602f365287cb2201f8350", "title": "EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling"}, {"paperId": "31ead991a433f233d490a7efd6c61d5fae98c327", "title": "Learning by Self-Explaining"}, {"paperId": "12ecb85d1a201ec5cda64024f22e91d6853ad212", "title": "E VALUATION OF M EDIUM -S IZED LANGUAGE MODELS IN G ERMAN A ND E NGLISH L ANGUAGE"}]}
