{"paperId": "d66a705285fd21fab2197b594e0e2f87db071308", "publicationVenue": {"id": "b02cf974-33be-46bd-9cbc-022db6cb7151", "name": "IEEE International Conference on Software Maintenance and Evolution", "type": "conference", "alternate_names": ["Int Conf Sci Manag Eng", "IEEE Int Conf Softw Maint Evol", "ICSME", "International Conference Science, Management, and Engineering"], "url": "http://conferences.computer.org/icsm/"}, "title": "You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search", "abstract": "Code search plays a crucial role in software development, enabling developers to retrieve and reuse code using natural language queries. While the performance of code search models improves with an increase in high-quality data, obtaining such data can be challenging and expensive. Recently, large language models (LLMs) such as ChatGPT have made remarkable progress in both natural and programming language understanding and generation, offering user-friendly interaction via simple prompts. Inspired by these advancements, we propose a novel approach ChatDANCE, which utilizes high-quality and diverse augmented data generated by a large language model and leverages a filtering mechanism to eliminate low-quality augmentations. Specifically, we first propose a set of ChatGPT prompting rules that are specifically designed for source code and queries. Then, we leverage ChatGPT to rewrite code and queries based on the according prompts and then propose a filtering mechanism which trains a cross-encoder from the backbone model UniXcoder to filter out code and query pairs with low matching scores. Finally, we re-train the backbone model using the obtained high-quality augmented data. Experimental results show that ChatDANCE achieves state-of-the-art performance, improving the best baseline by 13.2% (R@1) and 7% (MRR). Surprisingly, we find that this augment-filter-retrain strategy enables the backbone model (UniXcoder) to self-grow. Moreover, extensive experiments show the effectiveness of each component and ChatDANCE has stable performance under different hyperparameter settings. In addition, we conduct qualitative and quantitative analyses to investigate why ChatDANCE works well and find that it learns a more uniform distribution of representations and effectively aligns the code and query spaces. We have made the code and data anonymously available at https://anonymous.4open.science/r/ChatDANCE.", "venue": "IEEE International Conference on Software Maintenance and Evolution", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-10-01", "journal": {"name": "2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)", "pages": "14-25"}, "authors": [{"authorId": "2239164852", "name": "Yanlin Wang"}, {"authorId": "2217902484", "name": "Lianghong Guo"}, {"authorId": "2006371687", "name": "Ensheng Shi"}, {"authorId": "2274095496", "name": "Wenqing Chen"}, {"authorId": "2254800142", "name": "Jiachi Chen"}, {"authorId": "2249763710", "name": "Wanjun Zhong"}, {"authorId": "2274168740", "name": "Menghan Wang"}, {"authorId": "2274015732", "name": "Hui Li"}, {"authorId": "2243157248", "name": "Hongyu Zhang"}, {"authorId": "1920802076", "name": "Ziyu Lyu"}, {"authorId": "2267902535", "name": "Zibin Zheng"}], "citations": [{"paperId": "b7c59cdbcc0a4fa19d7fdaa2dac4b9e192d0de54", "title": "How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations"}]}
