{"paperId": "dbf829c977c121c3704d070d7800d29fe5914756", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "LLM Inference Unveiled: Survey and Roofline Model Insights", "abstract": "The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework identifies the bottlenecks when deploying LLMs on hardware devices and provides a clear understanding of practical problems, such as why LLMs are memory-bound, how much memory and computation they need, and how to choose the right hardware. We systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as model compression (e.g., Knowledge Distillation and Quantization), algorithm improvements (e.g., Early Exit and Mixture-of-Expert), and both hardware and system-level enhancements. Our survey stands out by analyzing these methods with roofline model, helping us understand their impact on memory access and computation. This distinctive approach not only showcases the current research landscape but also delivers valuable insights for practical implementation, positioning our work as an indispensable resource for researchers new to the field as well as for those seeking to deepen their understanding of efficient LLM deployment. The analyze tool, LLM-Viewer, is open-sourced.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2024-02-26", "journal": {"name": "ArXiv", "volume": "abs/2402.16363"}, "authors": [{"authorId": "2256166978", "name": "Zhihang Yuan"}, {"authorId": "2125633957", "name": "Yuzhang Shang"}, {"authorId": "2288027007", "name": "Yang Zhou"}, {"authorId": "2257809664", "name": "Zhen Dong"}, {"authorId": "2049473825", "name": "Chenhao Xue"}, {"authorId": "27055880", "name": "Bingzhe Wu"}, {"authorId": "6966340", "name": "Zhikai Li"}, {"authorId": "2257035774", "name": "Qingyi Gu"}, {"authorId": "2287936021", "name": "Yong Jae Lee"}, {"authorId": "2273583245", "name": "Yan Yan"}, {"authorId": "2267425628", "name": "Beidi Chen"}, {"authorId": "2273404942", "name": "Guangyu Sun"}, {"authorId": "2242659602", "name": "Kurt Keutzer"}], "citations": [{"paperId": "1f43c1f42dd5505f27c0b36c99e6e6fa01f87ad5", "title": "Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs"}, {"paperId": "c0ef72d02b93065e77c506e23ce9acbbcd945893", "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models"}]}
