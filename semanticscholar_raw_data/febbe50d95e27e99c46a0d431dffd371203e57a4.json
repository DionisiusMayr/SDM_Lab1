{"paperId": "febbe50d95e27e99c46a0d431dffd371203e57a4", "publicationVenue": null, "title": "This paper is included in the Proceedings of the 2023 USENIX Annual Technical Conference.", "abstract": "Graph neural network(GNN) has been widely applied in real-world applications, such as product recommendation in e-commerce platforms and risk control in financial management systems. Several cache-based GNN systems have been built to accelerate GNN training in a single machine with multiple GPUs. However, these systems fail to train billion-scale graphs efficiently, which is a common challenge in the industry. In this work, we propose Legion, a system that automatically pushes the envelope of multi-GPU systems for accelerating billion-scale GNN training. First, we design a hierarchical graph partitioning mechanism that significantly improves the multi-GPU cache performance. Second, we build a unified multi-GPU cache that helps to minimize the PCIe traffic incurred by caching both graph topology and features with the highest hotness. Third, we develop an automatic cache management mechanism that adapts the multi-GPU cache plan according to the hardware specifications and various graphs to maximize the overall training throughput. Evaluations on various GNN models and multiple datasets show that Legion supports training billion-scale GNNs in a single machine and significantly outperforms the state-of-the-art cache-based systems on small graphs.", "venue": "", "year": null, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2157178741", "name": "Jiecheng Sun"}, {"authorId": "145235479", "name": "L. Su"}, {"authorId": "2148360717", "name": "Zuocheng Shi"}, {"authorId": "2088422443", "name": "Wen Shen"}, {"authorId": "2108724347", "name": "Zeke Wang"}, {"authorId": "2152508555", "name": "Lei Wang"}, {"authorId": "40539618", "name": "J. Zhang"}, {"authorId": "2154405865", "name": "Yong Li"}, {"authorId": "37681774", "name": "Wenyuan Yu"}, {"authorId": "1709595", "name": "Jingren Zhou"}, {"authorId": "2187558910", "name": "Fei Wu"}], "citations": [{"paperId": "180d5df74b7ea3799ae0018a2dbf6189fe5d0fcd", "title": "GraphScope Flex: LEGO-like Graph Computing Stack"}, {"paperId": "d10cabff491718f2b5a94b8e2d9c2a6d44aebe24", "title": "GNNFlow: A Distributed Framework for Continuous Temporal GNN Learning on Dynamic Graphs"}, {"paperId": "96a88170729021e553648a2b17c73602a3171d2f", "title": "gSampler: General and Efficient GPU-based Graph Sampling for Graph Learning"}, {"paperId": "c07cf05a0fa90fa6207aa07912131646198066e0", "title": "Helios: An Efficient Out-of-core GNN Training System on Terabyte-scale Graphs with In-memory Performance"}, {"paperId": "7bc76d815ae3bf267a970d2244c3b3730336977c", "title": "GNNPipe: Scaling Deep GNN Training with Pipelined Model Parallelism"}, {"paperId": "cc9f2390f52965c80fe56f10a1497bc558295bff", "title": "Aion: Efficient Temporal Graph Data Management"}]}
