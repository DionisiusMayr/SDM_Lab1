{"paperId": "e373a309f6a9be59605317b8cee92fa710c31889", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "InstructIE: A Bilingual Instruction-based Information Extraction Dataset", "abstract": "Traditional information extraction (IE) methodologies, constrained by pre-defined classes and static training paradigms, often falter in adaptability, especially in the dynamic world. To bridge this gap, we explore an instruction-based IE paradigm in this paper, leveraging the substantial cross-task generalization capabilities of Large Language Models (LLMs). We observe that most existing IE datasets tend to be overly redundant in their label sets, which leads to the inclusion of numerous labels not directly relevant to the extraction content when constructing instructions. To tackle this issue, we introduce a bilingual theme-centric IE instruction dataset (Chinese and English), InstructIE, and for the first time, incorporate a theme scheme design that effectively simplifies the label structure. Furthermore, we develop an innovative framework named KG2Instruction, which is specifically designed for the automatic generation of such datasets. Experimental evaluations based on InstructIE reveal that while current models show promise in Instruction-based IE tasks, opportunities for their potential optimization also emerge. The dataset is available at https://huggingface.co/datasets/zjunlp/InstructIE.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-05-19", "journal": {"name": "ArXiv", "volume": "abs/2305.11527"}, "authors": [{"authorId": "2217228727", "name": "Honghao Gui"}, {"authorId": "2253784578", "name": "Jintian Zhang"}, {"authorId": "40463748", "name": "Hongbin Ye"}, {"authorId": "2608639", "name": "Ningyu Zhang"}], "citations": [{"paperId": "d299a6b26e9ee23d0337a1d1a896fc1c847f5a46", "title": "InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment"}, {"paperId": "351a2d50b4aff8e9754dc7074dd589b10a7465d4", "title": "Large Language Models for Generative Information Extraction: A Survey"}, {"paperId": "d4346af837aa6c2bb4a341cfe9bd91862ea5910a", "title": "Large Knowledge Model: Perspectives and Challenges"}, {"paperId": "6f77613b4ecfa5c96942ede6a9a38f8580df831a", "title": "A Knowledge-Enhanced Medical Named Entity Recognition Method that Integrates Pre-Trained Language Models"}, {"paperId": "396305230ddcf915b19a19683a89e34d76321a33", "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models"}]}
