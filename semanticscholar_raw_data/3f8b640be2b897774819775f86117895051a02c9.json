{"paperId": "3f8b640be2b897774819775f86117895051a02c9", "publicationVenue": {"id": "7bb54772-a70c-44df-9b9d-9f3b5354c0e2", "name": "IEEE International Parallel and Distributed Processing Symposium", "type": "conference", "alternate_names": ["IEEE Int Parallel Distrib Process Symp", "International Parallel and Distributed Processing Symposium", "IPDPS", "Int Parallel Distrib Process Symp"], "url": "http://www.ipdps.org/"}, "title": "HACCS: Heterogeneity-Aware Clustered Client Selection for Accelerated Federated Learning", "abstract": "Federated Learning is a machine learning paradigm where a global model is trained in-situ across a large number of distributed edge devices. While this technique avoids the cost of transferring data to a central location and achieves a strong degree of privacy, it presents additional challenges due to the heterogeneous hardware resources available for training. Furthermore, data is not independent and identically distributed (IID) across all edge devices, resulting in statistical heterogeneity across devices. Due to these constraints, client selection strategies play an important role for timely convergence during model training. Existing strategies ensure that each individual device is included, at least periodically, in the training process. In this work, we propose HACCS, a Heterogeneity-Aware Clustered Client Selection system that identifies and exploits the statistical heterogeneity by representing all distinguishable data distributions instead of individual devices in the training process. HACCS is robust to individual device dropout, provided other devices in the system have similar data distributions. We propose privacy-preserving methods for estimating these client distributions and clustering them. We also propose strategies for leveraging these clusters to make scheduling decisions in a federated learning system. Our evaluation on real-world datasets suggests that our framework can provide 18% \u221238% reduction in time to convergence compared to the state of the art without any compromise in accuracy.", "venue": "IEEE International Parallel and Distributed Processing Symposium", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-01", "journal": {"name": "2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "pages": "985-995"}, "authors": [{"authorId": "2051656808", "name": "Joel Wolfrath"}, {"authorId": "2051656286", "name": "N. Sreekumar"}, {"authorId": "2109402853", "name": "Dhruv Kumar"}, {"authorId": "5846035", "name": "Yuanli Wang"}, {"authorId": "144114650", "name": "A. Chandra"}], "citations": [{"paperId": "af2904881d40fe23576a14bd9149fa216bc3e80c", "title": "FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning"}, {"paperId": "f040e3dab354f3bc3c74c80b44c712b544a46efc", "title": "RCSR: Robust Client Selection and Replacement in Federated Learning"}, {"paperId": "e4246c25005314c5f4796a370ec4d1a683f38836", "title": "Self-Organizing Clustering for Federated Learning"}, {"paperId": "e47e3a821d2f4655c20908da4f12a637336cd4d4", "title": "Multi-Criteria Client Selection and Scheduling with Fairness Guarantee for Federated Learning Service"}, {"paperId": "a7712aae416914a31046e748027edf307be3ee45", "title": "Simultaneous and Heterogenous Multithreading"}, {"paperId": "9d35ae255660672a41fd6b3a05d0394fcbb78730", "title": "Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client Sampling"}, {"paperId": "a8ee9b353c7eca3f27990dbf9ac229964ec5a087", "title": "Joint Quality Evaluation, Model Splitting and Resource Provisioning for Split Edge Learning"}, {"paperId": "2316d88ded3fbd2c795f984097e9bf0980fc9b31", "title": "PFL-LDG: Privacy-preserving Federated Learning via Lightweight Device Grouping"}, {"paperId": "891c07269223350e8478aadef7b4c75684d8704d", "title": "ASFL: Adaptive Semi-asynchronous Federated Learning for Balancing Model Accuracy and Total Latency in Mobile Edge Networks"}, {"paperId": "c5da1e4e54a3dece9c6d9eb6e891dd9120c654ee", "title": "A Reinforcement Learning Approach for Minimizing Job Completion Time in Clustered Federated Learning"}, {"paperId": "fddcc2303d56f70c9eca0d0ab47bd68d2cd128b7", "title": "FedBIAD: Communication-Efficient and Accuracy-Guaranteed Federated Learning with Bayesian Inference-Based Adaptive Dropout"}, {"paperId": "7a662552dbd6d3c0fbae809381815c09e0332b3c", "title": "FedTrip: A Resource-Efficient Federated Learning Method with Triplet Regularization"}, {"paperId": "22e7ad7513c2d84462ed6f52d830eae43f49727d", "title": "Client Selection in Federated Learning: Principles, Challenges, and Opportunities"}, {"paperId": "0ccb8228ed901021987de1f52d5965b4d519f4e5", "title": "To Store or Not? Online Data Selection for Federated Learning with Limited Storage"}, {"paperId": "09f963df4b1a09cd090e0362bfced81c7db07099", "title": "DELTA: Diverse Client Sampling for Fasting Federated Learning"}, {"paperId": "c6002247dee39ff91db2b646d8f2dd375cbb0a43", "title": "Towards WAN-aware join sampling over geo-distributed data"}, {"paperId": "a7013d35c6c88b9dbc22956566895bf8c3cc5cdc", "title": "ODE: A Data Sampling Method for Practical Federated Learning with Streaming Data and Limited Buffer"}]}
