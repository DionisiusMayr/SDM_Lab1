{"paperId": "4f531f0805843ca87700dfe97faca7f9412d8d8e", "publicationVenue": {"id": "d9f3caaf-274f-498b-8eec-96e347fbee7d", "name": "IEEE Transactions on Communications", "type": "journal", "alternate_names": ["IEEE Trans Commun"], "issn": "0090-6778", "url": "http://www.comsoc.org/TC", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=26"]}, "title": "Distributed Learning based on 1-Bit Gradient Coding in the Presence of Stragglers", "abstract": "This paper considers the problem of distributed learning (DL) in the presence of stragglers. For this problem, DL methods based on gradient coding have been widely investigated, which redundantly distribute the training data to the workers to guarantee convergence when some workers are stragglers. However, these methods require the workers to transmit real-valued vectors during the process of learning, which induces very high communication burden. To overcome this drawback, we propose a novel DL method based on 1-bit gradient coding (1-bit GCDL), where 1-bit data encoded from the locally computed gradients are transmitted by the workers to reduce the communication overhead. We theoretically provide the convergence guarantees of the proposed method for both the convex loss functions and nonconvex loss functions. It is shown empirically that 1-bit GC-DL outperforms the baseline methods, which attains better learning performance under the same communication overhead.", "venue": "IEEE Transactions on Communications", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-03-19", "journal": {"name": "IEEE Transactions on Communications"}, "authors": [{"authorId": "2283508196", "name": "Chengxi Li"}, {"authorId": "2283602539", "name": "Mikael Skoglund"}], "citations": [{"paperId": "63ff21c8f9efa42770a597e31a694e6ff5a71df5", "title": "Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation"}, {"paperId": "4ec416c481e3ceafefd17ff54aa867ee0354464f", "title": "Gradient Coding in Decentralized Learning for Evading Stragglers"}]}
