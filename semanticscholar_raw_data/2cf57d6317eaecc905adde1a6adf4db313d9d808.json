{"paperId": "2cf57d6317eaecc905adde1a6adf4db313d9d808", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks", "abstract": "In this work, we assess the security of AI code generators via data poisoning, i.e., an attack that injects malicious samples into the training data to generate vulnerable code. We poison the training data by injecting increasing amounts of code containing security vulnerabilities and assess the attack's success on different state-of-the-art models for code generation. Our analysis shows that AI code generators are vulnerable to even a small amount of data poisoning. Moreover, the attack does not impact the correctness of code generated by pre-trained models, making it hard to detect.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-08-04", "journal": {"name": "ArXiv", "volume": "abs/2308.04451"}, "authors": [{"authorId": "1694894", "name": "Domenico Cotroneo"}, {"authorId": "93268228", "name": "Cristina Improta"}, {"authorId": "52013287", "name": "Pietro Liguori"}, {"authorId": "1830720", "name": "R. Natella"}], "citations": [{"paperId": "29d8d9ef1f8a4c6a00efad51babdfff556738d1c", "title": "Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers"}, {"paperId": "a800ff94e6cdb0747c1772f3eed4f98ae74faa76", "title": "Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations"}, {"paperId": "5759d7c7038bfb7b5f1d47539c8348b04f5e0ee7", "title": "DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions"}, {"paperId": "d30b943f59c9830718ad545497a03ab9a6132f73", "title": "Clover: Closed-Loop Verifiable Code Generation"}]}
