{"paperId": "e8881258c042da4b2f9d30dd8af8a2509974fd74", "publicationVenue": {"id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e", "name": "Proceedings of the VLDB Endowment", "type": "journal", "alternate_names": ["Proceedings of The Vldb Endowment", "Proc VLDB Endow", "Proc Vldb Endow"], "issn": "2150-8097", "url": "http://dl.acm.org/toc.cfm?id=J1174", "alternate_urls": ["http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"]}, "title": "Experimental Analysis of Large-scale Learnable Vector Storage Compression", "abstract": "Learnable embedding vector is one of the most important applications in machine learning, and is widely used in various database-related domains. However, the high dimensionality of sparse data in recommendation tasks and the huge volume of corpus in retrieval-related tasks lead to a large memory consumption of the embedding table, which poses a great challenge to the training and deployment of models. Recent research has proposed various methods to compress the embeddings at the cost of a slight decrease in model quality or the introduction of other overheads. Nevertheless, the relative performance of these methods remains unclear. Existing experimental comparisons only cover a subset of these methods and focus on limited metrics. In this paper, we perform a comprehensive comparative analysis and experimental evaluation of embedding compression. We introduce a new taxonomy that categorizes these techniques based on their characteristics and methodologies, and further develop a modular benchmarking framework that integrates 14 representative methods. Under a uniform test environment, our benchmark fairly evaluates each approach, presents their strengths and weaknesses under different memory budgets, and recommends the best method based on the use case. In addition to providing useful guidelines, our study also uncovers the limitations of current methods and suggests potential directions for future research.", "venue": "Proceedings of the VLDB Endowment", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-27", "journal": {"name": "ArXiv", "volume": "abs/2311.15578"}, "authors": [{"authorId": "2155916654", "name": "Hailin Zhang"}, {"authorId": "2268718776", "name": "Penghao Zhao"}, {"authorId": "1720763480", "name": "Xupeng Miao"}, {"authorId": "2237813", "name": "Yingxia Shao"}, {"authorId": "2236292233", "name": "Zirui Liu"}, {"authorId": "2268429210", "name": "Tong Yang"}, {"authorId": "2268401753", "name": "Bin Cui"}], "citations": [{"paperId": "2968f2054b378d47e61bf1f3a91b608919b2a898", "title": "CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models"}]}
