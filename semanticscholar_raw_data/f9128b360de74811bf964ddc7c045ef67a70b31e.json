{"paperId": "f9128b360de74811bf964ddc7c045ef67a70b31e", "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "title": "Boosting Distress Support Dialogue Responses with Motivational Interviewing Strategy", "abstract": "AI-driven chatbots have become an emerging solution to address psychological distress. Due to the lack of psychotherapeutic data, researchers use dialogues scraped from online peer support forums to train them. But since the responses in such platforms are not given by professionals, they contain both conforming and non-conforming responses. In this work, we attempt to recognize these conforming and non-conforming response types present in online distress-support dialogues using labels adapted from a well-established behavioral coding scheme named Motivational Interviewing Treatment Integrity (MITI) code and show how some response types could be rephrased into a more MI adherent form that can, in turn, enable chatbot responses to be more compliant with the MI strategy. As a proof of concept, we build several rephrasers by fine-tuning Blender and GPT3 to rephrase MI non-adherent\"Advise without permission\"responses into\"Advise with permission\". We show how this can be achieved with the construction of pseudo-parallel corpora avoiding costs for human labor. Through automatic and human evaluation we show that in the presence of less training data, techniques such as prompting and data augmentation can be used to produce substantially good rephrasings that reflect the intended style and preserve the content of the original text.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-05-17", "journal": {"name": "ArXiv", "volume": "abs/2305.10195"}, "authors": [{"authorId": "16939988", "name": "A. Welivita"}, {"authorId": "2188780149", "name": "Pearl Pu"}], "citations": [{"paperId": "07a52d25dd5d091957fbdfa6abd28a394719ef15", "title": "CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering"}, {"paperId": "78f43c0000d55e4ed45afe2d3d14e02ce623123b", "title": "Few-shot Dialogue Strategy Learning for Motivational Interviewing via Inductive Reasoning"}, {"paperId": "73d1bff2768eecde45bd0ba7ca71976fd825948c", "title": "Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation"}, {"paperId": "eb7a04c2f4ca8236cd67c26c8181c65c1f65e48c", "title": "Is ChatGPT More Empathetic than Humans?"}]}
