{"paperId": "2a05cb8677128664a80e1b225125e31d8cc91cc9", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Towards Efficient Large-Scale Graph Neural Network Computing", "abstract": "Recent deep learning models have moved beyond low-dimensional regular grids such as image, video, and speech, to high-dimensional graph-structured data, such as social networks, brain connections, and knowledge graphs. This evolution has led to large graph-based irregular and sparse models that go beyond what existing deep learning frameworks are designed for. Further, these models are not easily amenable to efficient, at scale, acceleration on parallel hardwares (e.g. GPUs). We introduce NGra, the first parallel processing framework for graph-based deep neural networks (GNNs). NGra presents a new SAGA-NN model for expressing deep neural networks as vertex programs with each layer in well-defined (Scatter, ApplyEdge, Gather, ApplyVertex) graph operation stages. This model not only allows GNNs to be expressed intuitively, but also facilitates the mapping to an efficient dataflow representation. NGra addresses the scalability challenge transparently through automatic graph partitioning and chunk-based stream processing out of GPU core or over multiple GPUs, which carefully considers data locality, data movement, and overlapping of parallel processing and data movement. NGra further achieves efficiency through highly optimized Scatter/Gather operators on GPUs despite its sparsity. Our evaluation shows that NGra scales to large real graphs that none of the existing frameworks can handle directly, while achieving up to about 4 times speedup even at small scales over the multiple-baseline design on TensorFlow.", "venue": "arXiv.org", "year": 2018, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-10-19", "journal": {"name": "ArXiv", "volume": "abs/1810.08403"}, "authors": [{"authorId": "2492241", "name": "Lingxiao Ma"}, {"authorId": "98256743", "name": "Zhi Yang"}, {"authorId": "11009920", "name": "Youshan Miao"}, {"authorId": "2870618", "name": "Jilong Xue"}, {"authorId": "145217421", "name": "Ming Wu"}, {"authorId": "93317102", "name": "Lidong Zhou"}, {"authorId": "34889832", "name": "Yafei Dai"}], "citations": [{"paperId": "de2057a4615c6f2d04a5b179bffd0287670aebd3", "title": "Accelerating GNN Training by Adapting Large Graphs to Distributed Heterogeneous Architectures"}, {"paperId": "17de61dab1353134de9c4ea53663c2d489c652f8", "title": "Agglomeration of Polygonal Grids using Graph Neural Networks with applications to Multigrid solvers"}, {"paperId": "3e51df99a41ad0769d0d93f1ce8ff9475db4c0d9", "title": "A Communication-Efficient Federated Learning Scheme for IoT-Based Traffic Forecasting"}, {"paperId": "ee2f02fbce8898298567a6f334172727ec2da07d", "title": "Parallelizing Graph Neural Networks via Matrix Compaction for Edge-Conditioned Networks"}, {"paperId": "b40352e71bae7ecb1dfe93cdad044749202eb6cb", "title": "PCGraph: Accelerating GNN Inference on Large Graphs via Partition Caching"}, {"paperId": "ae69a0a44be7e846fddc3bace995445e1f87d4e5", "title": "AdaGNN: A multi-modal latent representation meta-learner for GNNs based on AdaBoosting"}, {"paperId": "5b53a03186a418d2da94136aa499915ef50f88f7", "title": "Graph Neural Networks: Taxonomy, Advances, and Trends"}, {"paperId": "2e92728e393538304555e48a8d7532daf5ebabd4", "title": "Computing Graph Neural Networks: A Survey from Algorithms to Accelerators"}, {"paperId": "795c0caf0f5a3da4551f1b39e40e58bd8aa34888", "title": "Redundancy-Free Computation for Graph Neural Networks"}, {"paperId": "3eaba3c8bfbbfcfe703d0e2f37566e1672cf657b", "title": "Distributed Graph Computation Meets Machine Learning"}, {"paperId": "067aba96234df07d95501782df21b766def9086a", "title": "Spatial Structured Prediction Models: Applications, Challenges, and Techniques"}, {"paperId": "fd075bcdf2d7e13d23f7c249a8eded343d5bbe3b", "title": "Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs"}, {"paperId": "381411d740562de1e766dc8cc833844eb99dde01", "title": "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks."}, {"paperId": "8c324f9261261c505aae1aa4a24b4a4bdb4a988d", "title": "Fast Training of Sparse Graph Neural Networks on Dense Hardware"}]}
