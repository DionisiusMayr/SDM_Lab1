{"paperId": "829c9f335078afbb4a8217515fe24d146b939e40", "publicationVenue": null, "title": "Scaling Big Data Cleansing", "abstract": "Scaling Big Data Cleansing Zuhair Yarub Khayyat Data cleansing approaches have usually focused on detecting and fixing errors with little attention to big data scaling. This presents a serious impediment since identifying and repairing dirty data often involves processing huge input datasets, handling sophisticated error discovery approaches and managing huge arbitrary errors. With large datasets, error detection becomes overly expensive and complicated especially when considering user-defined functions. Furthermore, a distinctive algorithm is desired to optimize inequality joins in sophisticated error discovery rather than n\u00e4\u0131vely parallelizing them. Also, when repairing large errors, their skewed distribution may obstruct effective error repairs. In this dissertation, I present solutions to overcome the above three problems in scaling data cleansing. First, I present BigDansing as a general system to tackle efficiency, scalability, and ease-of-use issues in data cleansing for Big Data. It automatically parallelizes the user\u2019s code on top of general-purpose distributed platforms. Its programming interface allows users to express data quality rules independently from the requirements of parallel and distributed environments. Without sacrificing their quality, BigDansing also enables parallel execution of serial repair algorithms by exploiting the graph representation of discovered errors. The experimental results show that BigDansing outperforms existing baselines up to more than two orders of magnitude. Although BigDansing scales cleansing jobs, it still lacks the ability to handle sophisticated error discovery requiring inequality joins. Therefore, I developed IEJoin as an algorithm for fast inequality joins. It is based on sorted arrays and space efficient 5 bit-arrays to reduce the problem\u2019s search space. By comparing IEJoin against wellknown optimizations, I show that it is more scalable, and several orders of magnitude faster. BigDansing depends on vertex-centric graph systems, i.e., Pregel, to efficiently store and process discovered errors. Although Pregel scales general-purpose graph computations, it is not able to handle skewed workloads efficiently. Therefore, I introduce Mizan, a Pregel system that balances the workload transparently during runtime to adapt for changes in computing needs. Mizan is general; it does not assume any a priori knowledge of the graph structure or the algorithm behavior. Through extensive evaluations, I show that Mizan provides up to 84% improvement over techniques leveraging static graph pre-partitioning.", "venue": "", "year": 2017, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2017-07-31", "journal": {"name": "", "volume": ""}, "authors": [{"authorId": "2407411", "name": "Zuhair Khayyat"}], "citations": []}
