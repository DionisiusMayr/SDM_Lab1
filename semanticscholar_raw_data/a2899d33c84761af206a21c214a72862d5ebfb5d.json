{"paperId": "a2899d33c84761af206a21c214a72862d5ebfb5d", "publicationVenue": null, "title": "A RGUS E YES : Screening Native Machine Learning Pipelines", "abstract": "Software systems that learn from data are being deployed in increasing numbers in real-world application scenarios. It is a difficult and tedious task to ensure at development time that the end-to-end ML pipelines for such applications adhere to sound experimentation practices, such as the strict isolation of train and test data. Furthermore, there is a dire need to enforce legal and ethical compliance in automated decision-making with ML. For example, in order to determine whether a model works equally well for different groups. For enforcing privacy rights (such as the \u2018right to be forgotten\u2019 [1]), we must identify which models actually consumed the user\u2019s data for model training, in order to retrain them without this data. Moreover, model predictions can be corrupted due to undetected data distribution shift, e.g., when the train/test data was incorrectly sampled [2] or changed over time (covariate shift) or when the distribution of the target label changed (label shift) [3]. Data scientists also require support for uncovering erroneous data, e.g., to identify samples that are not helpful for the classifier and potentially dirty or mislabeled [4] or to identify subsets of data for which a model does not work well. Towards automated low-effort screening of ML pipelines. Most of the listed issues are typically addressed manually in an ad-hoc way and require a lot of expertise and extra code. In many cases, there is no system support for detecting particular issues, and typically, data has to be integrated first, as common libraries assume the input to be in a single table. Furthermore, specialised solutions are often incompatible with popular libraries in the ecosystem. This situation is in stark contrast to the software engineering world, with established best practices and infrastructure for testing and continuous integration. Provenance is all you need. We find that we can automate the detection of many common correctness issues in ML pipelines with access to (i) the materialised artifacts of a pipeline (its input relations, and its outputs, e.g., the feature matrices, labels, and predictions of a classifier) as well as (ii) their why-provenance [5] (e.g., the information which input records were used to compute a particular output). This allows us to design lightweight screening techniques with low invasiveness for natively written ML pipelines, which combine code from different libraries from the rapidly evolving data science ecosystem. Pipeline screening with ARGUSEYES. Based on these insights, we present our ARGUSEYES prototype, which operates on a natively written ML pipeline in Python, extracts intermediate results and provenance (in the form of provenance polynomials [6]) with MLINSPECT [7], and infers the semantics of their artifacts based on predefined \u201ctemplates\u201d (e.g., for a classification task). Our prototype enables the automatic detection of common issues w.r.t. best practices in ML, and the computation of metadata such as group fairness metrics, record usage by the model, or data valuation with Shapley values. Our prototype handles classification pipelines natively written in pandas/sklearn and keras, stores their artifacts and run data via mlflow [8], and can be easily hooked into continuous integration workflows. Current State & Future Work. An abstract about this work has been accepted at CIDR 2022. We provide a prototypical implementation of our proposed approach at https://github. com/schelterlabs/arguseyes. In the future, we plan to add support for additional pipeline types (e.g., clustering, recommendation, learning-to-rank), and more detection techniques for correctness violations, especially over multiple pipeline executions. A current limitation of our approach is that we rely on the pipeline being written based on many declarative constructs from pandas and scikit-learn, which might often not be the case for data science code. We intend to increase the robustness of MLINSPECT and ARGUSEYES against such scripts. This work was supported by Ahold Delhaize. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their employers.", "venue": "", "year": 2021, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "1393463989", "name": "Stefan Grafberger"}, {"authorId": "3375291", "name": "Shubha Guha"}, {"authorId": "2333066", "name": "O. Sprangers"}, {"authorId": "2180399", "name": "Sebastian Schelter"}], "citations": []}
