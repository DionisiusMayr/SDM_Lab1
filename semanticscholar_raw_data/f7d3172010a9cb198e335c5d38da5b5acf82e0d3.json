{"paperId": "f7d3172010a9cb198e335c5d38da5b5acf82e0d3", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources", "abstract": "Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update scheme for the quantized weights. As a result, QFT reduces the model state memory to 21% of the standard solution while achieving comparable performance, e.g., tuning a LLaMA-7B model requires only<30GB of memory, satisfied by a single A6000 GPU.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-11", "journal": {"name": "ArXiv", "volume": "abs/2310.07147"}, "authors": [{"authorId": "6966340", "name": "Zhikai Li"}, {"authorId": "2257094053", "name": "Xiaoxuan Liu"}, {"authorId": "2257971171", "name": "Banghua Zhu"}, {"authorId": "143879884", "name": "Zhen Dong"}, {"authorId": "2257035774", "name": "Qingyi Gu"}, {"authorId": "2242659602", "name": "Kurt Keutzer"}], "citations": [{"paperId": "dbf829c977c121c3704d070d7800d29fe5914756", "title": "LLM Inference Unveiled: Survey and Roofline Model Insights"}, {"paperId": "4819c866fc634a45ad5e8b8847b3497d23ccdf49", "title": "RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization"}]}
