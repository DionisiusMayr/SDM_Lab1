{"paperId": "110804428354df709b3693f9efc81946a9036ebf", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Neurons in Large Language Models: Dead, N-gram, Positional", "abstract": "We analyze a family of large language models in such a lightweight manner that can be done on a single GPU. Specifically, we focus on the OPT family of models ranging from 125m to 66b parameters and rely only on whether an FFN neuron is activated or not. First, we find that the early part of the network is sparse and represents many discrete features. Here, many neurons (more than 70% in some layers of the 66b model) are\"dead\", i.e. they never activate on a large collection of diverse data. At the same time, many of the alive neurons are reserved for discrete features and act as token and n-gram detectors. Interestingly, their corresponding FFN updates not only promote next token candidates as could be expected, but also explicitly focus on removing the information about triggering them tokens, i.e., current input. To the best of our knowledge, this is the first example of mechanisms specialized at removing (rather than adding) information from the residual stream. With scale, models become more sparse in a sense that they have more dead neurons and token detectors. Finally, some neurons are positional: them being activated or not depends largely (or solely) on position and less so (or not at all) on textual data. We find that smaller models have sets of neurons acting as position range indicators while larger models operate in a less explicit manner.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-09", "journal": {"name": "ArXiv", "volume": "abs/2309.04827"}, "authors": [{"authorId": "46235299", "name": "Elena Voita"}, {"authorId": "1751450782", "name": "Javier Ferrando"}, {"authorId": "31434304", "name": "Christoforos Nalmpantis"}], "citations": [{"paperId": "2ccbafd636184c6edd83e585359514923b6cf849", "title": "The Unreasonable Ineffectiveness of the Deeper Layers"}, {"paperId": "28948bb153ea3b50c749a576abbe298820ad1146", "title": "Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model"}, {"paperId": "9ee4ab6ac5f4e239f0617db49e4b5fb8fa2b5a97", "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models"}, {"paperId": "96edfa3441bb3624ea2b8bdfc5eec2c87efa9637", "title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models"}, {"paperId": "fd958ce58494f53cb2c8b05ba57a1c16f1566dc6", "title": "MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models"}, {"paperId": "8e54e747808f6c6fe3ecca29eeaca2724334acd0", "title": "Spectral Filters, Dark Signals, and Attention Sinks"}, {"paperId": "a2539713a31ee08cc8116c1355da9f96896df145", "title": "AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers"}, {"paperId": "436cd0fe00807bc9d14434f3313dc836530f2dae", "title": "Universal Neurons in GPT2 Language Models"}, {"paperId": "098da337aa9521aae80cbaab3572b75857cd911c", "title": "See the Unseen: Better Context-Consistent Knowledge-Editing by Noises"}, {"paperId": "e2bc390cf21dc319ea5aa9a7c3a223911dbf2012", "title": "Interpretability Illusions in the Generalization of Simplified Models"}, {"paperId": "9a149300e4291c3503651b8f031533240d4e9ec4", "title": "Sparsify-then-Classify: From Internal Neurons of Large Language Models To Efficient Text Classifiers"}, {"paperId": "04b880e1e32f37b3796d41a47d013fa07095ae32", "title": "From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning"}, {"paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6", "title": "A Simple and Effective Pruning Approach for Large Language Models"}]}
