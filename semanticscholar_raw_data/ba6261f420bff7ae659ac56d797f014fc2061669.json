{"paperId": "ba6261f420bff7ae659ac56d797f014fc2061669", "publicationVenue": {"id": "e84bb5a1-8f79-42cc-8eb1-3a52f7c73d63", "name": "IEEE International Conference on Systems, Man and Cybernetics", "type": "conference", "alternate_names": ["Smoky Mountains Computational Sciences and Engineering Conference", "Smoky Mt Comput Sci Eng Conf", "IEEE Int Conf Syst Man Cybern", "SMC", "Syst Man Cybern", "Systems, Man and Cybernetics"]}, "title": "FedGSync: Jointly Optimized Weak Synchronization and Gradient Transmission for Fast Distributed Machine Learning in Heterogeneous WAN", "abstract": "Due to privacy and cost reasons, distributed machine learning in Wide-Area Networks(DML-WAN) is becoming an emerging and popular collaborative learning paradigm. However, heterogeneity in computing power and data distribution among workers in different locations has a dramatic impact on training performance, including convergence speed and learning accuracy. Most of the existing works on distributed training mechanisms either focus on computing heterogeneity or data heterogeneity, and none of them can handle both well. In this paper, we propose FedGSync, a novel distributed training mechanism to improve the training performance for DML-WAN, where computing heterogeneity and data heterogeneity usually coexist. To speed up training and improve model accuracy, FedGSync clusters workers into groups according to the similarity of their data distribution and introduce group-based weak synchronization to minimize the synchronization delays waiting for slow workers and the accuracy loss by balancing the contributions of all data distributions. To preserve data privacy and improve efficiency, FedGSync only groups workers based on principal components of gradients and design an approximate grouping mechanism based on Kmeans. To further reduce synchronization time, FedGSync prioritizes packets and uses differential transmission for gradient packets between groups. Evaluation results demonstrate that FedGSync improves convergence speed and learning accuracy under the coexistence of computing heterogeneity and data heterogeneity compared with state-of-the-art distributed training mechanisms.", "venue": "IEEE International Conference on Systems, Man and Cybernetics", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-10-01", "journal": {"name": "2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)", "pages": "2075-2080"}, "authors": [{"authorId": "1739176388", "name": "Huaman Zhou"}, {"authorId": "2118917464", "name": "Yihong He"}, {"authorId": "2282091030", "name": "Zhihao Zhang"}, {"authorId": "2064538", "name": "Long Luo"}, {"authorId": "2261821411", "name": "Hong-Fang Yu"}, {"authorId": "2250029306", "name": "Gang Sun"}], "citations": []}
