{"paperId": "7a922abcda328f9333c5a3819ade8917b98f08c9", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs", "abstract": "Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing computational costs and allowing much more flexibility in combining with different LLM models. Experimental evaluations demonstrate the effectiveness of the proposed DGTL model on achieving superior or comparable performance over state-of-the-art baselines. Additionally, we also demonstrate that our DGTL model can offer natural language explanations for predictions, thereby significantly enhancing model interpretability.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-27", "journal": {"name": "ArXiv", "volume": "abs/2310.18152"}, "authors": [{"authorId": "2112481185", "name": "Yi Qin"}, {"authorId": "2153687490", "name": "Xin Wang"}, {"authorId": "2116460208", "name": "Ziwei Zhang"}, {"authorId": "2156154955", "name": "Wenwu Zhu"}], "citations": [{"paperId": "ca0c836a147be649af779492fcfcd4b5a25f34c2", "title": "A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion"}, {"paperId": "d299a6b26e9ee23d0337a1d1a896fc1c847f5a46", "title": "InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment"}, {"paperId": "a41d4a3b005c8ec4f821e6ee96672d930ca9596c", "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering"}, {"paperId": "7c9fb2be8b091e89e7edefe6e3f999fc896e1bae", "title": "Rendering Graphs for Graph Reasoning in Multimodal Large Language Models"}, {"paperId": "a15364e83f8986d8b8b1b6212596071e4f2d986d", "title": "Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion"}, {"paperId": "67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51", "title": "Large Language Models on Graphs: A Comprehensive Survey"}, {"paperId": "7ec393a898521e4e9a3f510be424861f5a518109", "title": "Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs"}, {"paperId": "54630cd92c0c6696a422c3b2aa986c1f75df70b3", "title": "A Survey of Graph Meets Large Language Model: Progress and Future Directions"}, {"paperId": "105669ec59a58fb2d4dd3021a984af33c227c5ab", "title": "Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs"}, {"paperId": "a3beafc334314e80e3f5f9f131ec9e6a6cdd7ce1", "title": "Graph Learning and Its Advancements on Large Language Models: A Holistic Survey"}]}
