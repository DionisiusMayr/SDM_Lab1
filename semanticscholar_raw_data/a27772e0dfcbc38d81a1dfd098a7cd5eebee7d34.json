{"paperId": "a27772e0dfcbc38d81a1dfd098a7cd5eebee7d34", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Optimizing the Communication-Accuracy Trade-off in Federated Learning with Rate-Distortion Theory", "abstract": "A significant bottleneck in federated learning (FL) is the network communication cost of sending model updates from client devices to the central server. We present a comprehensive empirical study of the statistics of model updates in FL, as well as the role and benefits of various compression techniques. Motivated by these observations, we propose a novel method to reduce the average communication cost, which is near-optimal in many use cases, and outperforms Top-K, DRIVE, 3LC and QSGD on Stack Overflow next-word prediction, a realistic and challenging FL benchmark. This is achieved by examining the problem using rate-distortion theory, and proposing distortion as a reliable proxy for model accuracy. Distortion can be more effectively used for optimizing the trade-off between model performance and communication cost across clients. We demonstrate empirically that in spite of the non-i.i.d. nature of federated learning, the rate-distortion frontier is consistent across datasets, optimizers, clients and training rounds.", "venue": "arXiv.org", "year": 2022, "fieldsOfStudy": ["Computer Science", "Mathematics"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-01-07", "journal": {"name": "ArXiv", "volume": "abs/2201.02664"}, "authors": [{"authorId": "1664853300", "name": "Nicole Mitchell"}, {"authorId": "2107648001", "name": "Johannes Ball'e"}, {"authorId": "143676545", "name": "Zachary B. Charles"}, {"authorId": "32139366", "name": "Jakub Konecn\u00fd"}], "citations": [{"paperId": "929279fb0536bcf883457c69d0ce0eb28e7b1915", "title": "ResFed: Communication0Efficient Federated Learning With Deep Compressed Residuals"}, {"paperId": "d2246170b3a5dab35a8a0619949965d55f031ec5", "title": "Federated Learning: Attacks, Defenses, Opportunities, and Challenges"}, {"paperId": "461af2af5582e3b27e56b0ef13f3e89720ac6b20", "title": "Detecting Malicious Blockchain Attacks through Flower using Horizontal Federated Learning: An Investigation of Federated Approaches"}, {"paperId": "da0139a0c4be287deb285004b25b13c56a7049fd", "title": "Communication-Efficient Federated Learning through Importance Sampling"}, {"paperId": "57743cf834386d576af8cda96c46e0452a0b6b1a", "title": "Joint Compression and Deadline Optimization for Wireless Federated Learning"}, {"paperId": "ade01d570d4bda2cdc3618081064243a9d40fed9", "title": "Federated Automatic Differentiation"}, {"paperId": "a4031f194b2b003220800e6457fe1a30af5669ca", "title": "ResFed: Communication Efficient Federated Learning by Transmitting Deep Compressed Residuals"}, {"paperId": "ffe13079f6e1d475e5e358ac5a1dbdd93c409015", "title": "Sparse Random Networks for Communication-Efficient Federated Learning"}, {"paperId": "acba7ea5905901944027b1bab14d5c6059edf062", "title": "Federated Select: A Primitive for Communication- and Memory-Efficient Federated Learning"}, {"paperId": "5d7a864de646e3f6150c707cf3250256c0b4a138", "title": "QUIC-FL: Quick Unbiased Compression for Federated Learning"}, {"paperId": "a716eb0e47d7271cdd1ad3e0a3f3a639796b4793", "title": "Mixed Federated Learning: Joint Decentralized and Centralized Learning"}, {"paperId": "4923e0bbe79f26cdea99190d6c40113cea5f4a8d", "title": "Correlated quantization for distributed mean estimation and optimization"}, {"paperId": "15fd98412176db65dae3928377afc453165f1dd7", "title": "Linear Stochastic Bandits over a Bit-Constrained Channel"}, {"paperId": "93252dd29ed549a0041c43408bdb09f0f251e46a", "title": "Local Differential Privacy in Federated Optimization"}]}
