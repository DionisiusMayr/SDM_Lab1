{"paperId": "9540e0ae29bcbbf10aac4585030b423e11163fe4", "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE International Conference on Computer Vision", "type": "conference", "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"}, "title": "DecentLaM: Decentralized Momentum SGD for Large-batch Deep Training", "abstract": "The scale of deep learning nowadays calls for efficient distributed training algorithms. Decentralized momentum SGD (DmSGD), in which each node averages only with its neighbors, is more communication efficient than vanilla Parallel momentum SGD that incurs global average across all computing nodes. On the other hand, the large-batch training has been demonstrated critical to achieve runtime speedup. This motivates us to investigate how DmSGD performs in the large-batch scenario.In this work, we find the momentum term can amplify the inconsistency bias in DmSGD. Such bias becomes more evident as batch-size grows large and hence results in severe performance degradation. We next propose DecentLaM, a novel decentralized large-batch momentum SGD to remove the momentum-incurred bias. The convergence rate for both strongly convex and non-convex scenarios is established. Our theoretical results justify the superiority of DecentLaM to DmSGD especially in the large-batch scenario. Experimental results on a a variety of computer vision tasks and models show that DecentLaM promises both efficient and high-quality training.", "venue": "IEEE International Conference on Computer Vision", "year": 2021, "fieldsOfStudy": ["Computer Science", "Mathematics"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-04-24", "journal": {"name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "pages": "3009-3019"}, "authors": [{"authorId": "1841311", "name": "K. Yuan"}, {"authorId": "2109367314", "name": "Yiming Chen"}, {"authorId": "1441111040", "name": "Xinmeng Huang"}, {"authorId": "2363741", "name": "Yingya Zhang"}, {"authorId": "1642296684", "name": "Pan Pan"}, {"authorId": "50125871", "name": "Yinghui Xu"}, {"authorId": "6833606", "name": "W. Yin"}], "citations": [{"paperId": "a9e468638a0f32669ac3cc5f6bff707f143b5570", "title": "Distributed Adaptive Gradient Algorithm with Gradient Tracking for Stochastic Non-Convex Optimization"}, {"paperId": "0d37d5d774644f39d9b4fd27c50227762fbe7fdb", "title": "Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization"}, {"paperId": "82044418831f0a768f37b74c93ad7e55ed3ea80c", "title": "An Accelerated Distributed Stochastic Gradient Method with Momentum"}, {"paperId": "19bb64946d8169ad6fd228916f71e20894d40c5d", "title": "Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity"}, {"paperId": "eaf9778b79a3cc8e61a75a8a8df02db129b5e3f1", "title": "Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise"}, {"paperId": "6d325044dab228e565e896c6681ce670fdf58cb8", "title": "Achieving Linear Speedup with Network-Independent Learning Rates in Decentralized Stochastic Optimization"}, {"paperId": "751a06db6863f48d42c43a9b65b530a31217a1d9", "title": "Towards Better Understanding the Influence of Directed Networks on Decentralized Stochastic Optimization"}, {"paperId": "15eacd77bc259cf07d5a2615a01aaf9a3cbccc9e", "title": "Compressed Gradient Tracking Algorithms for Distributed Nonconvex Optimization"}, {"paperId": "dd9c530f7994d2f8d5ddc8684bef8ae8bd875774", "title": "Joint Model Pruning and Topology Construction for Accelerating Decentralized Machine Learning"}, {"paperId": "a8a4c4f8dd0efc7818a682df5eb68825dafa2855", "title": "Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)"}, {"paperId": "fb1a85443b4fcddba3ba38cc78c02073f18aa0c8", "title": "Stochastic Controlled Averaging for Federated Learning with Communication Compression"}, {"paperId": "29364b734d3efbc87d1fda828a19d3abac19da9d", "title": "Modeling with Homophily Driven Heterogeneous Data in Gossip Learning"}, {"paperId": "41ac34482be0d45a0323e67906cf2e92df00f444", "title": "Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization"}, {"paperId": "a1513d2ffbb58de8229dd94c0d3e71ea2969e799", "title": "Decentralized Local Updates with Dual-Slow Estimation and Momentum-based Variance-Reduction for Non-Convex Optimization"}, {"paperId": "e48c95489430ad7d0978aaa7f6995eaf8d8d92d1", "title": "Momentum Benefits Non-IID Federated Learning Simply and Provably"}, {"paperId": "57b3491dd7b2faacde921f5f7051c2c591682ea0", "title": "Decentralized SGD and Average-direction SAM are Asymptotically Equivalent"}, {"paperId": "a3a6c3f0bdd831c4bf3f8e8d202bfb9425273a39", "title": "DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm"}, {"paperId": "f7cc8d5412b602540e5db5bffe5a688629f8d54b", "title": "Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?"}, {"paperId": "0fff24d83092c5af1d603e4507f55cfa1d6420f0", "title": "Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence"}, {"paperId": "e3226f4ea0b4db910f67338929183c7a371e9a58", "title": "Lower Bounds and Accelerated Algorithms in Distributed Stochastic Optimization with Communication Compression"}, {"paperId": "b5c834d985aa3193e37b5a71b4d97b6bc8ae4f99", "title": "Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning"}, {"paperId": "e60f0265a73009872627e12d3c7d37e750b9c7b6", "title": "Push-LSVRG-UP: Distributed Stochastic Optimization Over Unbalanced Directed Networks With Uncoordinated Triggered Probabilities"}, {"paperId": "0b40fe0a80748daa6096db08648d2f20c7272383", "title": "Proximal Stochastic Recursive Momentum Methods for Nonconvex Composite Decentralized Optimization"}, {"paperId": "18f869b06fee6303ffebc56eb75b6dfbfb9a3200", "title": "Optimal Complexity in Non-Convex Decentralized Learning over Time-Varying Networks"}, {"paperId": "3ea459c34bc6c4e34bc2de9cb83b1f422fe3b036", "title": "Revisiting Optimal Convergence Rate for Smooth and Non-convex Stochastic Decentralized Optimization"}, {"paperId": "fabf58a766313f3f7088e95c53233a1e3fd0ef6f", "title": "Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data"}, {"paperId": "3b6393a0e2382d0241f4093dd9c6f5c78b27861e", "title": "Efficient Decentralized Stochastic Gradient Descent Method for Nonconvex Finite-Sum Optimization Problems"}, {"paperId": "4c8432c45c9f57c0613d4aa687b56768f128d486", "title": "Sublinear Algorithms for Hierarchical Clustering"}, {"paperId": "10044d70ffefdfc57ebc69ae6d52e1f5178bb775", "title": "Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with Communication Compression"}, {"paperId": "6ca0442ecd7341be0b3c9f5b8442c516b952f6d2", "title": "Collaborative Learning of Discrete Distributions under Heterogeneity and Communication Constraints"}, {"paperId": "2792c922aa5559c7afe16a3c3b088fe6b653bf0d", "title": "Theoretical Analysis of Primal-Dual Algorithm for Non-Convex Stochastic Decentralized Optimization"}, {"paperId": "8b9d4f52c01f710b7679c73e49e946057315202d", "title": "Data-heterogeneity-aware Mixing for Decentralized Learning"}, {"paperId": "01c1e3109ad8416df81ea2d77d172e1e757b88b2", "title": "Refined Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data"}, {"paperId": "964cc7f1fe6f240e2aa0b659c4f2e46cb6bebaf9", "title": "An Improved Analysis of Gradient Tracking for Decentralized Machine Learning"}, {"paperId": "4ef2e765a3d39a4d59c6f84f90981bab19aeded3", "title": "Byzantine-Robust Decentralized Learning via ClippedGossip"}, {"paperId": "87b6eff7ef8aa498e7e0a640ce50f876707aebb2", "title": "BlueFog: Make Decentralized Algorithms Practical for Optimization and Deep Learning"}, {"paperId": "f0974cca1c0989f8d3472d34681241ac7c1c4b46", "title": "Exponential Graph is Provably Efficient for Decentralized Deep Training"}, {"paperId": "b9b9f79b9644404c38c280b611812db39b2df9a4", "title": "RelaySum for Decentralized Deep Learning on Heterogeneous Data"}, {"paperId": "f0c6348d93a82e4ef0a78a3efe5bbbd1e5f21f37", "title": "A Stochastic Proximal Gradient Framework for Decentralized Non-Convex Composite Optimization: Topology-Independent Sample Complexity and Communication Efficiency"}, {"paperId": "efea624caa48c0f0c14b68f444f37d41f1ad32e8", "title": "Removing Data Heterogeneity Influence Enhances Network Topology Dependence of Decentralized SGD"}, {"paperId": "948c28201ac06e6f9e24c3dc7d8249b39e0290c7", "title": "Stochastic Normalized Gradient Descent with Momentum for Large Batch Training"}, {"paperId": "74cac85b0b2fab6b3209af689d28d52c23e0b173", "title": "HTDFormer: Hyperspectral Target Detection Based on Transformer With Distributed Learning"}, {"paperId": "51631911f9a997237df615ffc6fb7e7646fcf6df", "title": "A Novel Named Entity Recognition Model Applied to Specialized Sequence Labeling"}, {"paperId": "91d3eacd85ec13a93d9a1f858aeba550f82240bb", "title": "Decentralized Matrix Sensing: Statistical Guarantees and Fast Convergence"}, {"paperId": "823b701a6da886b36415656c9ea73192fc67bcca", "title": "Yes, Topology Matters in Decentralized Optimization: Refined Convergence and Topology Learning under Heterogeneous Data"}, {"paperId": "d48cca1cc6dc53c2b7ada42254cc9a83fb28a348", "title": "Re\ufb01ned Convergence and Topology Learning for Decentralized Optimization with Heterogeneous Data"}, {"paperId": "6ce52d3c14e2c45fa6f0566c98bb9c2d1ebd0e34", "title": "Making Byzantine Decentralized Learning Efficient"}, {"paperId": "5273a3706a1cb40426a110b09eef7ad3ad684321", "title": "Byzantine-Robust Decentralized Learning via Self-Centered Clipping"}, {"paperId": "13b70f20c75758dfecd9f761d9104062d4b5dfb8", "title": "Removing Data Heterogeneity In\ufb02uence Enhances Network Topology Dependence of Decentralized SGD"}]}
