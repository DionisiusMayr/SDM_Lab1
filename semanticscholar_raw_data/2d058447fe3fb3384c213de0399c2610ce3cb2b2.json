{"paperId": "2d058447fe3fb3384c213de0399c2610ce3cb2b2", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "abstract": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient fine-tuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research of FedLLM and enable a broad range of industrial applications.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-16", "journal": {"name": "ArXiv", "volume": "abs/2310.10049"}, "authors": [{"authorId": "2072650627", "name": "Tao Fan"}, {"authorId": "1505828520", "name": "Yan Kang"}, {"authorId": "2147432129", "name": "Guoqiang Ma"}, {"authorId": "2258784471", "name": "Weijing Chen"}, {"authorId": "2258938576", "name": "Wenbin Wei"}, {"authorId": "2087083369", "name": "Lixin Fan"}, {"authorId": "2166949653", "name": "Qiang Yang"}], "citations": [{"paperId": "1359b818543bfde35f0966b74612d18ae28ca41f", "title": "On Protecting the Data Privacy of Large Language Models (LLMs): A Survey"}, {"paperId": "1d10aa5e7122d1df6d559999987c76de3a088f62", "title": "Training Machine Learning models at the Edge: A Survey"}, {"paperId": "af2904881d40fe23576a14bd9149fa216bc3e80c", "title": "FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning"}, {"paperId": "7ae48b24cbf955bf9b9498fb287bf4c5cd3b73d4", "title": "OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning"}, {"paperId": "836c652834b0f6ffe10e53ede1c0b9433cfad9ea", "title": "Copyright Protection in Generative AI: A Technical Perspective"}, {"paperId": "7fe5e23fffb189ce45659abe414d70bd770ed878", "title": "Horizontal Federated Computer Vision"}, {"paperId": "cef42360fbb467953dbdc1a5de6ed18fb25a869b", "title": "LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing"}, {"paperId": "06eae596ea3e996453039f6a2cc68732cbba884b", "title": "Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes"}, {"paperId": "383c598625110e0a4c60da4db10a838ef822fbcf", "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"}, {"paperId": "2e75a0db9d826ea7a94f1cfc80b3e81ad36f1a52", "title": "FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients"}, {"paperId": "e0a7de219b4621495b91f98dd2bd588279e899ac", "title": "Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives"}, {"paperId": "879e526c8ee954a7ca93f25de85c7d3c2f0b6a9f", "title": "A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions"}]}
