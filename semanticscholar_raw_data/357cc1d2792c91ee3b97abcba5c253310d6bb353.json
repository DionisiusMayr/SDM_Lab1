{"paperId": "357cc1d2792c91ee3b97abcba5c253310d6bb353", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation", "abstract": "Text-to-Image (T2I) diffusion models have achieved remarkable success in image generation. Despite their progress, challenges remain in both prompt-following ability, image quality and lack of high-quality datasets, which are essential for refining these models. As acquiring labeled data is costly, we introduce AGFSync, a framework that enhances T2I diffusion models through Direct Preference Optimization (DPO) in a fully AI-driven approach. AGFSync utilizes Vision-Language Models (VLM) to assess image quality across style, coherence, and aesthetics, generating feedback data within an AI-driven loop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and SDXL, our extensive experiments on the TIFA dataset demonstrate notable improvements in VQA scores, aesthetic evaluations, and performance on the HPSv2 benchmark, consistently outperforming the base models. AGFSync's method of refining T2I diffusion models paves the way for scalable alignment techniques.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-03-20", "journal": {"name": "ArXiv", "volume": "abs/2403.13352"}, "authors": [{"authorId": "2219271222", "name": "Jingkun An"}, {"authorId": "2098545", "name": "Yinghao Zhu"}, {"authorId": "2292306908", "name": "Zongjian Li"}, {"authorId": "2219181324", "name": "Haoran Feng"}, {"authorId": "2292362456", "name": "Bohua Chen"}, {"authorId": "2292341278", "name": "Yemin Shi"}, {"authorId": "2238637284", "name": "Chengwei Pan"}], "citations": []}
