{"paperId": "2ca7e36901022fde6f0477088267cb1fa36a1668", "publicationVenue": null, "title": "A2FL: Availability-Aware Federated Learning in Heterogeneous Environments", "abstract": "Distributed privacy-preserving machine learning (ML) meth-ods are increasingly becoming the norm for training ML models on edge devices. In a popular approach known as Federated learning (FL), service providers leverage end-user data to train ML models to improve services such as text auto-completion, virtual keyboards, and item recommendations. FL is expected to grow in importance with the increasing focus on privacy and 5G/6G technologies. FL faces major challenges such as resource and user heterogeneity, communication overheads, and efficient privacy preservation. In practice, training global models via FL is very time-consuming because of the heterogeneity of clients\u2019 computation and communication speeds. Even worse, clients may not always be available to participate in training which limits their representation in the global model. Empirical analysis shows that client availability impacts the model quality which motivates the design of A2FL . A2FL is a client selection method which mitigates the quality degradation caused by the misrepre-sentation of the client population. Our results show that, compared to existing methods, A2FL can enhance clients\u2019 representation and improve the trained model quality.", "venue": "", "year": 2023, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2575987", "name": "A. Abdelmoniem"}], "citations": []}
