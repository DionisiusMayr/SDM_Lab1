{"paperId": "efd0aa60187d3c9ab80d3343e4ae4561edc90a48", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "The Information of Large Language Model Geometry", "abstract": "This paper investigates the information encoded in the embeddings of large language models (LLMs). We conduct simulations to analyze the representation entropy and discover a power law relationship with model sizes. Building upon this observation, we propose a theory based on (conditional) entropy to elucidate the scaling law phenomenon. Furthermore, we delve into the auto-regressive structure of LLMs and examine the relationship between the last token and previous context tokens using information theory and regression techniques. Specifically, we establish a theoretical connection between the information gain of new tokens and ridge regression. Additionally, we explore the effectiveness of Lasso regression in selecting meaningful tokens, which sometimes outperforms the closely related attention weights. Finally, we conduct controlled experiments, and find that information is distributed across tokens, rather than being concentrated in specific\"meaningful\"tokens alone.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science", "Mathematics"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-01", "journal": {"name": "ArXiv", "volume": "abs/2402.03471"}, "authors": [{"authorId": "2261884659", "name": "Zhiquan Tan"}, {"authorId": "2281902679", "name": "Chenghai Li"}, {"authorId": "8007867", "name": "Weiran Huang"}], "citations": []}
