{"paperId": "5b01caa6e43fe733fbdd189b894226c64c3d6382", "publicationVenue": {"id": "0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e", "name": "ACM Transactions on Intelligent Systems and Technology", "type": "journal", "alternate_names": ["ACM Trans Intell Syst Technol"], "issn": "2157-6904", "url": "http://portal.acm.org/tist", "alternate_urls": ["https://tist.acm.org/"]}, "title": "Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation", "abstract": "Federated learning (FL) is a privacy-preserving machine learning paradigm in which the server periodically aggregates local model parameters from clients without assembling their private data. Constrained communication and personalization requirements pose severe challenges to FL. Federated distillation (FD) is proposed to simultaneously address the above two problems, which exchanges knowledge between the server and clients, supporting heterogeneous local models while significantly reducing communication overhead. However, most existing FD methods require a proxy dataset, which is often unavailable in reality. A few recent proxy-data-free FD approaches can eliminate the need for additional public data, but suffer from remarkable discrepancy among local knowledge due to client-side model heterogeneity, leading to ambiguous representation on the server and inevitable accuracy degradation. To tackle this issue, we propose a proxy-data-free FD algorithm based on distributed knowledge congruence (FedDKC). FedDKC leverages well-designed refinement strategies to narrow local knowledge differences into an acceptable upper bound, so as to mitigate the negative effects of knowledge incongruence. Specifically, from perspectives of peak probability and Shannon entropy of local knowledge, we design kernel-based knowledge refinement (KKR) and searching-based knowledge refinement (SKR) respectively, and theoretically guarantee that the refined-local knowledge can satisfy an approximately-similar distribution and be regarded as congruent. Extensive experiments conducted on three common datasets demonstrate that our proposed FedDKC significantly outperforms the state-of-the-art on various heterogeneous settings while evidently improving the convergence speed.", "venue": "ACM Transactions on Intelligent Systems and Technology", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-04-14", "journal": {"name": "ACM Transactions on Intelligent Systems and Technology"}, "authors": [{"authorId": "2146254894", "name": "Zhiyuan Wu"}, {"authorId": "2109354976", "name": "Sheng Sun"}, {"authorId": "119917160", "name": "Yuwei Wang"}, {"authorId": "2118169652", "name": "Min Liu"}, {"authorId": "145425483", "name": "Qing Liu"}, {"authorId": "2278650457", "name": "Zeju Li"}], "citations": [{"paperId": "9f0095d9476ccd46fb43967ebe81362b80320186", "title": "Logits Poisoning Attack in Federated Distillation"}, {"paperId": "1d871ce98e8d572cc16aab38ba1c980ace5e3551", "title": "Federated Class-Incremental Learning with New-Class Augmented Self-Distillation"}, {"paperId": "576597c684675832051001ac2f5923bdd1436cfb", "title": "Improving Communication Efficiency of Federated Distillation via Accumulating Local Updates"}, {"paperId": "9403e6586433048f4dd76db8243a957b2764f561", "title": "Agglomerative Federated Learning: Empowering Larger Model Training via End-Edge-Cloud Collaboration"}, {"paperId": "60e346f717533ad5c794d3516eb14af8e9200531", "title": "Federated Skewed Label Learning with Logits Fusion"}, {"paperId": "3256f44bcb3fa778d95a457ab15b10e9d959a914", "title": "FedCache: A Knowledge Cache-driven Federated Learning Architecture for Personalized Edge Intelligence"}, {"paperId": "6f9c0ff832e6e8b5c740d0ea0c49592d16138608", "title": "Knowledge Distillation in Federated Edge Learning: A Survey"}, {"paperId": "e9751729d11e582ff059452aeaa07d86cd8f8afa", "title": "FedICT: Federated Multi-task Distillation for Multi-access Edge Computing"}, {"paperId": "4558d4eb033dffcf7a260ea63f5ef6809603dffd", "title": "FedDyn: A dynamic and efficient federated distillation approach on Recommender System"}, {"paperId": "1094cb276084f7b17ce3f2cb53c71dcf23e0d5e6", "title": "PRACTICAL GUIDE"}]}
