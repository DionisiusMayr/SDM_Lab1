{"paperId": "3256f44bcb3fa778d95a457ab15b10e9d959a914", "publicationVenue": {"id": "4e46790b-e240-4236-9b8d-a70ed74f900a", "name": "IEEE Transactions on Mobile Computing", "type": "journal", "alternate_names": ["IEEE Trans Mob Comput"], "issn": "1536-1233", "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7755", "alternate_urls": ["http://www.computer.org/portal/web/tmc"]}, "title": "FedCache: A Knowledge Cache-driven Federated Learning Architecture for Personalized Edge Intelligence", "abstract": "Edge Intelligence (EI) allows Artificial Intelligence (AI) applications to run at the edge, where data analysis and decision-making can be performed in real-time and close to data sources. To protect data privacy and unify data silos among end devices in EI, Federated Learning (FL) is proposed for collaborative training of shared AI models across devices without compromising data privacy. However, the prevailing FL approaches cannot guarantee model generalization and adaptation on heterogeneous clients. Recently, Personalized Federated Learning (PFL) has drawn growing awareness in EI, as it enables a productive balance between local-specific training requirements inherent in devices and global-generalized optimization objectives for satisfactory performance. However, most existing PFL methods are based on the Parameters Interaction-based Architecture (PIA) represented by FedAvg, which causes unaffordable communication burdens due to large-scale parameters transmission between devices and the edge server. In contrast, Logits Interaction-based Architecture (LIA) allows to update model parameters with logits transfer and gains the advantages of communication lightweight and heterogeneous on-device model allowance compared to PIA. Nevertheless, previous LIA methods attempt to achieve satisfactory performance either relying on unrealistic public datasets or increasing communication overhead for additional information transmission other than logits. To tackle this dilemma, we propose a knowledge cache-driven PFL architecture, named FedCache, which reserves a knowledge cache on the server for fetching personalized knowledge from the samples with similar hashes to each given on-device sample. During the training phase, ensemble distillation is applied to on-device models for constructive optimization with personalized knowledge transferred from the server-side knowledge cache.", "venue": "IEEE Transactions on Mobile Computing", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-08-15", "journal": {"name": "ArXiv", "volume": "abs/2308.07816"}, "authors": [{"authorId": "2146254894", "name": "Zhiyuan Wu"}, {"authorId": "2109354976", "name": "Sheng Sun"}, {"authorId": "119917160", "name": "Yuwei Wang"}, {"authorId": "2118169652", "name": "Min Liu"}, {"authorId": "2237170553", "name": "Ke Xu"}, {"authorId": "2117828525", "name": "Wen Wang"}, {"authorId": "2144282461", "name": "Xue Jiang"}, {"authorId": "2199129359", "name": "Bo Gao"}, {"authorId": "2215280525", "name": "Jin Lu"}], "citations": [{"paperId": "9f0095d9476ccd46fb43967ebe81362b80320186", "title": "Logits Poisoning Attack in Federated Distillation"}, {"paperId": "1d871ce98e8d572cc16aab38ba1c980ace5e3551", "title": "Federated Class-Incremental Learning with New-Class Augmented Self-Distillation"}, {"paperId": "576597c684675832051001ac2f5923bdd1436cfb", "title": "Improving Communication Efficiency of Federated Distillation via Accumulating Local Updates"}, {"paperId": "60e346f717533ad5c794d3516eb14af8e9200531", "title": "Federated Skewed Label Learning with Logits Fusion"}, {"paperId": "6f9c0ff832e6e8b5c740d0ea0c49592d16138608", "title": "Knowledge Distillation in Federated Edge Learning: A Survey"}, {"paperId": "5b01caa6e43fe733fbdd189b894226c64c3d6382", "title": "Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation"}]}
