{"paperId": "f08c60aa410fd3ef5fd21c09332a428626859844", "publicationVenue": {"id": "c3e5f1c8-9ba7-47e5-acde-53063a69d483", "name": "Future Internet", "type": "journal", "issn": "1999-5903", "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-156830", "alternate_urls": ["http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-156830", "https://www.mdpi.com/journal/futureinternet"]}, "title": "Towards Efficient Resource Allocation for Federated Learning in Virtualized Managed Environments", "abstract": "Federated learning (FL) is a transformative approach to Machine Learning that enables the training of a shared model without transferring private data to a central location. This decentralized training paradigm has found particular applicability in edge computing, where IoT devices and edge nodes often possess limited computational power, network bandwidth, and energy resources. While various techniques have been developed to optimize the FL training process, an important question remains unanswered: how should resources be allocated in the training workflow? To address this question, it is crucial to understand the nature of these resources. In physical environments, the allocation is typically performed at the node level, with the entire node dedicated to executing a single workload. In contrast, virtualized environments allow for the dynamic partitioning of a node into containerized units that can adapt to changing workloads. Consequently, the new question that arises is: how can a physical node be partitioned into virtual resources to maximize the efficiency of the FL process? To answer this, we investigate various resource allocation methods that consider factors such as computational and network capabilities, the complexity of datasets, as well as the specific characteristics of the FL workflow and ML backend. We explore two scenarios: (i) running FL over a finite number of testbed nodes and (ii) hosting multiple parallel FL workflows on the same set of testbed nodes. Our findings reveal that the default configurations of state-of-the-art cloud orchestrators are sub-optimal when orchestrating FL workflows. Additionally, we demonstrate that different libraries and ML models exhibit diverse computational footprints. Building upon these insights, we discuss methods to mitigate computational interferences and enhance the overall performance of the FL pipeline execution.", "venue": "Future Internet", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-07-31", "journal": {"name": "Future Internet", "pages": "261", "volume": "15"}, "authors": [{"authorId": "98228359", "name": "F. Nikolaidis"}, {"authorId": "66948076", "name": "Moysis Symeonides"}, {"authorId": "1755017", "name": "Demetris Trihinas"}], "citations": [{"paperId": "6a0a9710abaf2ae66d0fc997c885a7858c2fd867", "title": "Personalized Federated Learning with Adaptive Feature Extraction and Category Prediction in Non-IID Datasets"}, {"paperId": "95e239db43b53a3d0f40f66acdacdce915fe50be", "title": "Federated machine learning in healthcare: A systematic review on clinical applications and technical architecture"}, {"paperId": "d6c6a80799efb7ce9dd7b681002bc2eedfcc5e4a", "title": "Model-Driven Engineering for Embedded Systems with Non-Functional Properties Analysis Integration: A Review"}]}
