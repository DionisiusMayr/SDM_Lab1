{"paperId": "de2057a4615c6f2d04a5b179bffd0287670aebd3", "publicationVenue": {"id": "42cd70f7-45f1-4f5a-9723-42d222d6c56e", "name": "IEEE transactions on computers", "type": "journal", "alternate_names": ["IEEE Transactions on Computers", "IEEE Trans Comput", "IEEE trans comput"], "issn": "0018-9340", "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=12", "alternate_urls": ["http://www.computer.org/portal/web/tc"]}, "title": "Accelerating GNN Training by Adapting Large Graphs to Distributed Heterogeneous Architectures", "abstract": "Graph neural networks (GNNs) have been successfully applied to many important application domains on graph data. As graphs become increasingly large, existing GNN training frameworks typically use mini-batch sampling during feature aggregation to lower resource burdens, which unfortunately suffer from long memory accessing latency and inefficient data transfer of vertex features from CPU to GPU. This paper proposes 2PGraph, a system that addresses these limitations of mini-batch sampling and feature aggregation and supports fast and efficient single-GPU and distributed GNN training. First, 2PGraph presents a locality awareness GNN-training scheduling method that schedules the vertices based on the locality of the graph topology, significantly accelerating the sampling and aggregation, improving the data locality of vertex access, and limiting the range of neighborhood expansion. Second, 2PGraph proposes a GNN-layer-aware feature caching method on available GPU resources with a hit rate up to 100<inline-formula><tex-math notation=\"LaTeX\">${\\bf\\%}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"bold\">%</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"lu-ieq1-3305077.gif\"/></alternatives></inline-formula>, which avoids redundant data transfer between CPU and GPU. Third, 2PGraph presents a self-dependence cluster-based graph partition method, achieving high sampling and cache efficiency for distributed environments. Experimental results on real-world graph datasets show that 2PGraph reduces memory access latency by up to 90<inline-formula><tex-math notation=\"LaTeX\">${\\boldsymbol{\\%}}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"bold\">%</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"lu-ieq2-3305077.gif\"/></alternatives></inline-formula> mini-batch sampling, and data transfer time by up to 99<inline-formula><tex-math notation=\"LaTeX\">${\\boldsymbol{\\%}}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"bold\">%</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"lu-ieq3-3305077.gif\"/></alternatives></inline-formula>. For distributed GNN training over an 8-GPU cluster, 2PGraph achieves up to 8.7<inline-formula><tex-math notation=\"LaTeX\">$\\times$</tex-math><alternatives><mml:math><mml:mo>\u00d7</mml:mo></mml:math><inline-graphic xlink:href=\"lu-ieq4-3305077.gif\"/></alternatives></inline-formula> performance speedup over state-of-the-art approaches.", "venue": "IEEE transactions on computers", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-12-01", "journal": {"name": "IEEE Transactions on Computers", "pages": "3473-3488", "volume": "72"}, "authors": [{"authorId": "2107957217", "name": "Lizhi Zhang"}, {"authorId": "50756759", "name": "KaiCheng Lu"}, {"authorId": "3050140", "name": "Zhiquan Lai"}, {"authorId": "7727056", "name": "Yongquan Fu"}, {"authorId": "2119310756", "name": "Yu Tang"}, {"authorId": "2135905059", "name": "Dongsheng Li"}], "citations": []}
