{"paperId": "dd06b331be3e72f5e7b74275aa4065b76b4df5dc", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Supersonic: Learning to Generate Source Code Optimizations in C/C++", "abstract": "Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present Supersonic, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, Supersonic is trained on C/C++ program pairs ($x_{t}$, $x_{t+1}$), where $x_{t+1}$ is an optimized version of $x_{t}$, and outputs a diff. Supersonic's performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that Supersonic not only outperforms both models on the code optimization task but also minimizes the extent of the change with a model more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-26", "journal": {"name": "ArXiv", "volume": "abs/2309.14846"}, "authors": [{"authorId": "46842516", "name": "Zimin Chen"}, {"authorId": "2110316453", "name": "Sen Fang"}, {"authorId": "150220964", "name": "Monperrus Martin"}], "citations": [{"paperId": "4fbd174e80502f8f3c4b9f48054872b028e2445a", "title": "Large Language Models for Software Engineering: Survey and Open Problems"}, {"paperId": "b5ccc0884e02ff8e81cdda6fb839fecaf6bb7c16", "title": "Enhancing Bug Report Summaries Through Knowledge-Specific and Contrastive Learning Pre-Training"}]}
