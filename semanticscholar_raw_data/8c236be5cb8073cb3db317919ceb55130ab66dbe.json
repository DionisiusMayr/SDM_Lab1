{"paperId": "8c236be5cb8073cb3db317919ceb55130ab66dbe", "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE International Conference on Computer Vision", "type": "conference", "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"}, "title": "Champagne: Learning Real-world Conversation from Large-Scale Web Videos", "abstract": "Visual information is central to conversation: body gestures and physical behaviour, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text. We introduce Champagne, a generative model of conversations that can account for visual contexts. To train Champagne, we collect and release YTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that converts error-prone automatic transcripts to a cleaner dialogue format while maintaining meaning.Human evaluation reveals that YTD-18M is more sensible and specific than prior resources (MMDialog [17], 1M dialogues), while maintaining visual-groundedness. Experiments demonstrate that 1) Champagne learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne.", "venue": "IEEE International Conference on Computer Vision", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-03-17", "journal": {"name": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)", "pages": "15452-15463"}, "authors": [{"authorId": "2423429", "name": "Seungju Han"}, {"authorId": "2689239", "name": "Jack Hessel"}, {"authorId": "46217681", "name": "Nouha Dziri"}, {"authorId": "1699545", "name": "Yejin Choi"}, {"authorId": "2111510510", "name": "Youngjae Yu"}], "citations": [{"paperId": "c44393114047e0f9c32e45b381970d50ed503260", "title": "LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs"}, {"paperId": "5f58863dd6474d6f127be995b5871e7c60f2792f", "title": "Video Understanding with Large Language Models: A Survey"}, {"paperId": "37410333c789da14d0635f6563b063c9b9427d23", "title": "SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models"}, {"paperId": "b1d2a29860e69c6ce9987ddefbe112feb1efa16a", "title": "Large Language Models can Share Images, Too!"}, {"paperId": "00c19d9818bf093a2eed323d1bd5c763c4f512b9", "title": "Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms"}, {"paperId": "7fabf538d7d61038dafc6aeb844ba1aa06cf7b6b", "title": "TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World"}, {"paperId": "a85ae093657f62bf13bf18b0652402026dd4186d", "title": "DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset"}]}
