{"paperId": "245b501c6594915141485eba55ae00c3cf010c5b", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Searching for Better Test Case Prioritization Schemes: a Case Study of AI-assisted Systematic Literature Review", "abstract": "Given the large numbers of publications in the SE field, it is difficult to keep current with the latest developments. In theory, AI tools could assist in finding relevant work but those AI tools have primarily been tested/validated in simulations rather than actual literature reviews. Accordingly, using a realistic case study, this paper assesses how well machine learning algorithms can help with literature reviews. The target of this case study is to identify test case prioritization techniques for automated UI testing; specifically from 8,349 papers on IEEE Xplore. This corpus was studied with an incrementally updated human-in-the-loop active learning text miner. Using that AI tool, in three hours, we found 242 relevant papers from which we identified 12 techniques representing the state-of-the-art in test case prioritization when source code information is not available. The foregoing results were validated by having six graduate students manually explore the same corpus. Using data from that validation study, we determined that without AI tools, this task would take 53 hours and would have found 27 extra papers. That is, with 6% of the effort of manual methods, our AI tools achieved a 90% recall. Significantly, the same 12 state-of-the-art test case prioritization techniques were found by both the AI study and the manual study. That is, the 27/242 papers missed by the AI study would not have changed our conclusions. Hence, this study endorses the use of machine learning algorithms to assist future literature reviews.", "venue": "arXiv.org", "year": 2019, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2019-09-16", "journal": {"name": "ArXiv", "volume": "abs/1909.07249"}, "authors": [{"authorId": "1491247841", "name": "Zhe Yu"}, {"authorId": "1680416", "name": "Jeffrey C. Carver"}, {"authorId": "1722304", "name": "G. Rothermel"}, {"authorId": "1703872", "name": "T. Menzies"}], "citations": [{"paperId": "21eac24a839100981a57d6a31e5c70b8907f01e8", "title": "Practical Accuracy Estimation for Efficient Deep Neural Network Testing"}, {"paperId": "462dfde80c8bcef7e971d4db06a236c761f47dc3", "title": "Understanding static code warnings: An incremental AI approach"}, {"paperId": "7326a55ac05bf37861078de7e7e6444e78164e1e", "title": "An Expert System for Learning Software Engineering Knowledge (with Case Studies in Understanding Static Code Warning)"}, {"paperId": "c0bb44e3292a0fb1a1ebdffb38a2828808e7c65c", "title": "Better Data Labelling With EMBLEM (and how that Impacts Defect Prediction)"}]}
