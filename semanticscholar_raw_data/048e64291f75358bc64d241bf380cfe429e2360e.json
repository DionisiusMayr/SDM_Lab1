{"paperId": "048e64291f75358bc64d241bf380cfe429e2360e", "publicationVenue": {"id": "528ced1f-e949-4e1a-8fee-2ffbf0be551d", "name": "Conference on Innovative Data Systems Research", "type": "conference", "alternate_names": ["CIDR", "Conf Innov Data Syst Res"], "url": "http://cidrdb.org/"}, "title": "Towards Observability for Machine Learning Pipelines", "abstract": "Software organizations are increasingly incorporating machine learning (ML) into their product offerings, driving a need for new data management tools. Many of these tools facilitate the initial development and deployment of ML applications, contributing to a crowded landscape of disconnected solutions targeted at different stages, or components, of the ML lifecycle. A lack of end-to-end ML pipeline visibility makes it hard to address any issues that may arise after a production deployment, such as unexpected output values or lower-quality predictions. In this paper, we propose a sys-tem that wraps around existing tools in the ML development stack and offers end-to-end observability. We introduce our prototype and our vision for MLTRACE , a platform-agnostic system that provides observability to ML practitioners by (1) executing prede\ufb01ned tests and monitoring ML-speci\ufb01c metrics at component runtime, (2) tracking end-to-end data \ufb02ow, and (3) allowing users to ask arbitrary post-hoc questions about pipeline health.", "venue": "Conference on Innovative Data Systems Research", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"name": "ArXiv", "volume": "abs/2108.13557"}, "authors": [{"authorId": "35403221", "name": "Shreya Shankar"}, {"authorId": "145592539", "name": "Aditya G. Parameswaran"}], "citations": [{"paperId": "1bd6eb74269d6c706cb33970b65f9152f8685636", "title": "Automatic and Precise Data Validation for Machine Learning"}, {"paperId": "de4991f7e34003a5b3188678e01d7c8aacdbc4e3", "title": "Monitoring and Adapting ML Models on Mobile Devices"}, {"paperId": "970d0914d7f512c9bab22a9b4e1894664f046b45", "title": "Moving Fast With Broken Data"}, {"paperId": "2e97a19016008e0b2851b43f768e2f85680e1ea5", "title": "Evolvability of Machine Learning-based Systems: An Architectural Design Decision Framework"}, {"paperId": "e7bd63df07502602b27a1c8d312c145663b65dba", "title": "Enabling Awareness of Quality of Training and Costs in Federated Machine Learning Marketplaces"}, {"paperId": "a12e64c8bd53136147862b570981581758e4937e", "title": "Management of Machine Learning Lifecycle Artifacts"}, {"paperId": "0705ddb754ad0a7cf43c54617a18c9ff342f639d", "title": "A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms"}, {"paperId": "a38af38baeb1b5e25c089b699ab5072823ae6b4c", "title": "The Fallacy of AI Functionality"}, {"paperId": "e58f77f3c89259d0ecfacf28137e82d6818c50b0", "title": "Rethinking Streaming Machine Learning Evaluation"}, {"paperId": "96b3cb4b2d2fe277e4296c35affba858f010c63a", "title": "TinyMLOps: Operational Challenges for Widespread Edge AI Adoption"}, {"paperId": "a171beea0e66977300b4542c987da904bcc6626f", "title": "Towards Observability for Production Machine Learning Pipelines [Vision]"}]}
