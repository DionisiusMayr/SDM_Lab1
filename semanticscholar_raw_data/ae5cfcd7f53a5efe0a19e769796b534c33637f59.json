{"paperId": "ae5cfcd7f53a5efe0a19e769796b534c33637f59", "publicationVenue": null, "title": "Membership Inference Attacks Against NLP Classi\ufb01cation Models", "abstract": "The success of natural language processing (NLP) is making NLP applications commonplace. Unfortunately, recent research has shown that privacy might be at stake given that these models are often trained on private user data. While privacy risks are demonstrated in text generation settings, privacy risks of the text classi\ufb01cation settings, which subsume myriad downstream applications, are largely unexplored. In this work, we study the susceptibility of NLP classi\ufb01cation models, used for text classi\ufb01cation tasks, to membership inference (MI), which is a fundamental type of privacy leakage. We design a comprehensive suite of attacks to assess the risk of sample-level MI , as well as that of relatively unexplored user-level MI . We introduce novel user-level MI attacks that outperform the existing attacks and conduct experiments on Transformer-based and RNN-based NLP models. Our evaluations show that user-level MI is signi\ufb01cantly stronger than sample-level MI. We further perform in-depth analyses showing the effect of various NLP-speci\ufb01c parameters on MI against NLP classi\ufb01cation models.", "venue": "", "year": 2021, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "148318826", "name": "Virat Shejwalkar"}, {"authorId": "3058104", "name": "Huseyin A. Inan"}, {"authorId": "1972973", "name": "Amir Houmansadr"}, {"authorId": "1562202621", "name": "Robert Sim"}], "citations": [{"paperId": "05de8d869abe43d166e591f83bc8dd050a6b05ce", "title": "SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks"}, {"paperId": "e9e746956d7f22bd844d2975a48d0ff8100e4bf2", "title": "Privacy-Preserving Language Model Inference with Instance Obfuscation"}, {"paperId": "74a2ef37466667c843b6322691c49b0475030cb0", "title": "Security and Privacy Challenges of Large Language Models: A Survey"}, {"paperId": "7d75b26b835292750aa199230c4a88ffee339a28", "title": "Code Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models"}, {"paperId": "3422d5e0cdfdc935d6a84a1e3d3f96659265fe3a", "title": "Detecting Pretraining Data from Large Language Models"}, {"paperId": "cf08fa8e17e4cfe44ef728dccc296467eb5b88b4", "title": "Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks"}, {"paperId": "aa853d6ddf598266a4c48647dae7b0a8e8a8ff49", "title": "User Inference Attacks on Large Language Models"}, {"paperId": "4b9e81a141ebd27a2227d2d1b63ef5fead7153cd", "title": "Improved Membership Inference Attacks Against Language Classification Models"}, {"paperId": "fec455cd5798dfcb70e741d76c88326d20d2536a", "title": "Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey"}, {"paperId": "4442b44a6861aaedee81e9146954ebfefa313479", "title": "Do Not Give Away My Secrets: Uncovering the Privacy Issue of Neural Code Completion Tools"}, {"paperId": "1f9079a7408d08ae6cd0dd74ebfaa728216b3626", "title": "Towards Sentence Level Inference Attack Against Pre-trained Language Models"}, {"paperId": "9c0f3179d31904ea1048d96c06bc1ab391891d16", "title": "Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models"}, {"paperId": "2f2a430ba6c93bcfaf4818316ff8a27b1e034b1a", "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models"}, {"paperId": "f0cc1cd2b9b41aa4a2fccfc52c79e25d10e2fa54", "title": "Watermarking Classification Dataset for Copyright Protection"}, {"paperId": "fdaeb41ebb60dbe60a3193f02320e3f00f8233fd", "title": "Stealing the Decoding Algorithms of Language Models"}, {"paperId": "baf3b64df9dd05385abcfd11d964f15c9684cc4d", "title": "Analyzing and Defending against Membership Inference Attacks in Natural Language Processing Classification"}, {"paperId": "f32b556ee9a0756196956a24aa0527704bf029ba", "title": "Recycling Scraps: Improving Private Learning by Leveraging Intermediate Checkpoints"}, {"paperId": "c4fc84c6c00708ddaf4754a34daf3fc3ee4ccd5a", "title": "Data Provenance via Differential Auditing"}, {"paperId": "ab3da5ee7c7a920f084ee2d3b7255141cace4dc7", "title": "Combing for Credentials: Active Pattern Extraction from Smart Reply"}, {"paperId": "92d0df018da15c1179568f1459da25c4ab0dfe1a", "title": "Privacy Leakage in Text Classification A Data Extraction Approach"}, {"paperId": "14f47cadbc5df34e40131c8bf04d977526712ce4", "title": "Subject Membership Inference Attacks in Federated Learning"}, {"paperId": "cc3b41c4d2ededb94b481089633ecb43d6b2162f", "title": "Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks"}, {"paperId": "62d17b6f6ad77fd71ef9954c7784700d5e316f1f", "title": "What Does it Mean for a Language Model to Preserve Privacy?"}, {"paperId": "7b2cd44fe906029710691687b521265dff2a0157", "title": "Privacy-Preserving Knowledge Transfer through Partial Parameter Sharing"}, {"paperId": "56800ea46f23427ef5b57448d00b15dc9833af4c", "title": "Subject Membership Inference Attacks in Federated Learning"}, {"paperId": "5d142f8b1dc67ec6307050d34dbcd6dfd4c0218c", "title": "Active Data Pattern Extraction Attacks on Generative Language Models"}, {"paperId": "041878c76f3cf39958448d2af454e1d38248a08f", "title": "Generating Realistic Synthetic Curricula Vitae for Machine Learning Applications under Differential Privacy"}, {"paperId": null, "title": "PrivateNLP 2022 The 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}, {"paperId": "db5b9b624a4595ba9dfdb1d0e144c5167371a608", "title": "On the Privacy Risk of In-context Learning"}]}
