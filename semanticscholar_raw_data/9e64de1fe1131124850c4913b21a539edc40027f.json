{"paperId": "9e64de1fe1131124850c4913b21a539edc40027f", "publicationVenue": {"id": "a0edb93b-1e95-4128-a295-6b1659149cef", "name": "Knowledge Discovery and Data Mining", "type": "conference", "alternate_names": ["KDD", "Knowl Discov Data Min"], "url": "http://www.acm.org/sigkdd/"}, "title": "Efficient Approximate Algorithms for Empirical Variance with Hashed Block Sampling", "abstract": "Empirical variance is a fundamental concept widely used in data management and data analytics, e.g., query optimization, approximate query processing, and feature selection. A direct solution to derive the empirical variance is scanning the whole data table, which is expensive when the data size is huge. Hence, most current works focus on approximate answers by sampling. For results with approximation guarantees, the samples usually need to be uniformly independent random, incurring high cache miss rates especially in compact columnar style layouts. An alternative uses block sampling to avoid this issue, which directly samples a block of consecutive records fitting page sizes instead of sampling one record each time. However, this provides no theoretical guarantee. Existing studies show that the practical estimations can be inaccurate as the records within a block can be correlated. Motivated by this, we investigate how to provide approximation guarantees for empirical variances with block sampling from a theoretical perspective. Our results shows that if the records stored in a table are 4-wise independent to each other according to keys, a slightly modified block sampling can provide the same approximation guarantee with the same asymptotic sampling cost as that of independent random sampling. In practice, storing records via hash clusters or hash organized tables are typical scenarios in modern commercial database systems. Thus, for data analysis on tables in the data lake or OLAP stores that are exported from such hash-based storage, our strategy can be easily integrated to improve the sampling efficiency. Based on our sampling strategy, we present an approximate algorithm for empirical variance and an approximate top-k algorithm to return the k columns with the highest empirical variance scores. Extensive experiments show that our solutions outperform existing solutions by up to an order of magnitude.", "venue": "Knowledge Discovery and Data Mining", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2022-08-14", "journal": {"name": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"}, "authors": [{"authorId": "1836010", "name": "Xingguang Chen"}, {"authorId": "2181369153", "name": "Fangyuan Zhang"}, {"authorId": "39996718", "name": "Sibo Wang"}], "citations": [{"paperId": "33ee38675a7444c48017598ec571b0e239a7f19b", "title": "Scalable Approximate Butterfly and Bi-triangle Counting for Large Bipartite Networks"}, {"paperId": "dec013c762d2c56ee5064c272e948813fdd30ff7", "title": "Efficient Approximation Framework for Attribute Recommendation"}]}
