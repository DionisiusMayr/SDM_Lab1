{"paperId": "5327ddac2f48e7eb1ba0729f938f85089e83637a", "publicationVenue": {"id": "2633f5b2-c15c-49fe-80f5-07523e770c26", "name": "IEEE Access", "type": "journal", "issn": "2169-3536", "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"]}, "title": "CAMM: Cross-Attention Multimodal Classification of Disaster-Related Tweets", "abstract": "During the past decade, social media platforms have been extensively used for information dissemination by the affected community and humanitarian agencies during a disaster. Although many studies have been done recently to classify the informative and non-informative messages from social media posts, most are unimodal, i.e., have independently used textual or visual data to build deep learning models. In the present study, we integrate the complementary information provided by the text and image messages about the same event posted by the affected community on the social media platform Twitter and build a multimodal deep learning model based on the concept of the attention mechanism. The attention mechanism is a recent breakthrough that has revolutionized the field of deep learning. Just as humans pay more attention to a specific part of the text or image, ignoring the rest, neural networks can also be trained to concentrate on more relevant features through the attention mechanism. We propose a novel Cross-Attention Multi-Modal (CAMM) deep neural network for classifying multimodal disaster data, which uses the attention mask of the textual modality to highlight the features of the visual modality. We compare CAMM with unimodal models and the most popular bilinear multimodal models, MUTAN and BLOCK, generally used for visual question answering. CAMM achieves an average F1-score of 84.08%, better than the MUTAN and BLOCK methods by 6.31% and 5.91%, respectively. The proposed cross-attention-based multimodal deep learning method outperforms the current state-of-the-art fusion methods on the benchmark multimodal disaster dataset by highlighting more relevant cross-domain features of text and image tweets.", "venue": "IEEE Access", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"name": "IEEE Access", "pages": "92889-92902", "volume": "10"}, "authors": [{"authorId": "1753742409", "name": "Anuradha Khattar"}, {"authorId": "2055875317", "name": "S. Quadri"}], "citations": [{"paperId": "b6d73965f1e28ddf61601936f8fffd8908862c32", "title": "Drowning in the Information Flood: Machine-Learning-Based Relevance Classification of Flood-Related Tweets for Disaster Management"}, {"paperId": "c985e2ffd89b4d845b7f25cb7572188bdcb451ea", "title": "A deep parallel hybrid fusion model for disaster tweet classification on Twitter data"}, {"paperId": "fe9dfe7437c2218f33bb1e3018bc753881d04ec9", "title": "Disaster Related Tweets Analysis with Machine Learning Approaches"}, {"paperId": "b7f893d4dfcabe7008f99b77d5018613db767f72", "title": "Enhancing Disaster Tweet Classification with Ensemble Models and Multiple Embeddings"}, {"paperId": "636c837de4c03f66afaaf9d08fd5ec17b831bee1", "title": "Visual and Linguistic Double Transformer Fusion Model for Multimodal Tweet Classification"}, {"paperId": "022cea21999662034c2e8c5d0e2c3791d52d30b8", "title": "Fusion of Multimodal Textual and Visual Descriptors for Analyzing Disaster Response"}]}
