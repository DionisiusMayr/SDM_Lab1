{"paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Instruction Tuning for Large Language Models: A Survey", "abstract": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-08-21", "journal": {"name": "ArXiv", "volume": "abs/2308.10792"}, "authors": [{"authorId": "1739188006", "name": "Shengyu Zhang"}, {"authorId": "2218934599", "name": "Linfeng Dong"}, {"authorId": "2845020", "name": "Xiaoya Li"}, {"authorId": "2107968400", "name": "Sen Zhang"}, {"authorId": "2109329406", "name": "Xiaofei Sun"}, {"authorId": "2109514219", "name": "Shuhe Wang"}, {"authorId": "2172372802", "name": "Jiwei Li"}, {"authorId": "2232782142", "name": "Runyi Hu"}, {"authorId": "2146331573", "name": "Tianwei Zhang"}, {"authorId": "2257430459", "name": "Fei Wu"}, {"authorId": "2220704972", "name": "Guoyin Wang"}], "citations": [{"paperId": "b34111fe8e230d371dd9446d52388a84b5592390", "title": "Facial Affective Behavior Analysis with Instruction Tuning"}, {"paperId": "9689b5fdb0d3a1bad802d03d348bd32aa5a4c2df", "title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data"}, {"paperId": "8719833751cf1bfc779c944fc7954a337b2c0833", "title": "Source-Aware Training Enables Knowledge Attribution in Language Models"}, {"paperId": "92fd80a619d6f9454b69882e81a710f857fd9cdf", "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning"}, {"paperId": "8e6124739e03c9d7ea4903de00c3370d2f1a8387", "title": "Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback"}, {"paperId": "a2c59567ceff5debe2cea463162d7beb5a4abfec", "title": "ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler"}, {"paperId": "6415a2521fe9bd27baaecd25abd8eaac8eae48b9", "title": "m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt"}, {"paperId": "c113d96c532c11eca026f50e80541eafb112e35f", "title": "COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"}, {"paperId": "83c41d2701bbd1a3d9b2dd89a65d288295611185", "title": "Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models"}, {"paperId": "2d1adda7a9b0596d8b470c065da5a6528b364a7f", "title": "Automated Data Curation for Robust Language Model Fine-Tuning"}, {"paperId": "e35e304fd28db3c74295fbc905e7f28f845da91a", "title": "ChatGPT y GPT-4: utilidades en el sector jur\u00eddico, funcionamiento, limitaciones y riesgos de los modelos fundacionales"}, {"paperId": "bb5393126610ab89983b29d8934b45f67a16241d", "title": "What Was Your Prompt? A Remote Keylogging Attack on AI Assistants"}, {"paperId": "5c5adfbeb53b2e7668e60b4dd9612d671e707f81", "title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts"}, {"paperId": "917aa8165c0ab778ea3aea7a2b286c74cd2889f0", "title": "SMART: Submodular Data Mixture Strategy for Instruction Tuning"}, {"paperId": "108b75ae8d6f4a0e6fe687eea6645aa3567dd410", "title": "KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models"}, {"paperId": "cf65db0934696e1f8896da52811b3d7f79836abd", "title": "From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification"}, {"paperId": "d48eb2400161a2ceb1d787be580dc20f61943d73", "title": "FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning"}, {"paperId": "e866687bc3053c0a38aa2847122f50c573df29dc", "title": "A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries"}, {"paperId": "eb6b054789ff8c9edf7c1d50667be5bdd95e019b", "title": "CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios"}, {"paperId": "9a257f5a2c3ab73ec123c3980c2a29e7d8258fbe", "title": "CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning"}, {"paperId": "f282f8a64476c1ab2a05c63d6a3a0d7e4b2996d9", "title": "Electrocardiogram Instruction Tuning for Report Generation"}, {"paperId": "3edc37be7ffe4647f8f271c0f0aa1861d98a11be", "title": "Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges"}, {"paperId": "009f64d890d8402cae8739b63bc0c3d06d62ed06", "title": "NoteLLM: A Retrievable Large Language Model for Note Recommendation"}, {"paperId": "c90318dd9ce7a1b0a45d71ceeb555cee3896a618", "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey"}, {"paperId": "45a8a7f4814797cfd8f7f256c45f2f85f40320e9", "title": "Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish"}, {"paperId": "2403ca4ff39727bb1c922891d0320c07004bc17e", "title": "Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation"}, {"paperId": "ee414ff78922ac70dfb31abfff37bd40c661ac92", "title": "Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models"}, {"paperId": "1296d51001f8a73c0a3356f78b136a691928985c", "title": "ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing"}, {"paperId": "0a88726548d4ab50c3b28f1ce839220997875cf6", "title": "InstructEdit: Instruction-based Knowledge Editing for Large Language Models"}, {"paperId": "16fbff9f07963fb394aef9fbc209e8554badc455", "title": "Rethinking Software Engineering in the Foundation Model Era: A Curated Catalogue of Challenges in the Development of Trustworthy FMware"}, {"paperId": "3b13389584c554fb5d7eb2f2ffc9b11b50385664", "title": "PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA"}, {"paperId": "c2cfa4d147d3f1e1fcf9584a7d4947321b4160c6", "title": "Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment"}, {"paperId": "bedfd672b897bd70201a5edb3f16c47b7661b4bf", "title": "INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models"}, {"paperId": "d30e33051496383a66b2a2ab1b961bc866afa409", "title": "VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models"}, {"paperId": "fbf18a32422798077ccc4a5fba8cf88a7203d7b2", "title": "AgentScope: A Flexible yet Robust Multi-Agent Platform"}, {"paperId": "59638d5237a53ec2f713af2aebcb38f925a8a308", "title": "STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning"}, {"paperId": "c6bf5b971ddd0db1293f70f2a88b652199a56612", "title": "Transformer-based Causal Language Models Perform Clustering"}, {"paperId": "2f8270afa120285b31df9779fbc8a6c5884bd059", "title": "Multi-dimensional Evaluation of Empathetic Dialog Responses"}, {"paperId": "ded732209b0ba8a6704cc62ab8197a898b57f833", "title": "EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models"}, {"paperId": "e710115f7251bc552e23311c69086253ddab2429", "title": "Contrastive Instruction Tuning"}, {"paperId": "472939d7cb3f81c9e158c89e8578a16f481ceeaf", "title": "LAPDoc: Layout-Aware Prompting for Documents"}, {"paperId": "e10691827473d0d4e940112adf02a6eb9040de3a", "title": "Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization"}, {"paperId": "77ff5c9276bcf30b524a7cc030e0365a33b686bf", "title": "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs"}, {"paperId": "f904551ef0622587a10434cfb6734710fd267dc0", "title": "Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?"}, {"paperId": "931a111a3f77ba61a6952173ecfde4c6a889bbf1", "title": "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking"}, {"paperId": "68254b5518c598d6939adca610339c8747fa0b24", "title": "Sentinels of the Stream: Unleashing Large Language Models for Dynamic Packet Classification in Software Defined Networks - Position Paper"}, {"paperId": "a1f76db91c0debcf93ae9889736bce8470902113", "title": "Large Language Models: A Survey"}, {"paperId": "f5e7e22036c3fe7d6660eee90642f716c3b303f5", "title": "StruQ: Defending Against Prompt Injection with Structured Queries"}, {"paperId": "b8dbdb9eb9141562f7d48bc3709fee2d23f5e5bd", "title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education"}, {"paperId": "9f3f7a5c252400504b380074d2c0f5527b2f6379", "title": "Large Language Model Meets Graph Neural Network in Knowledge Distillation"}, {"paperId": "d0e09bb06723ec6388f0dcb3f3588fa3e95f7216", "title": "RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation"}, {"paperId": "6b0fe48bf5b43772153bc26fd7090778ad6c5f2c", "title": "Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction"}, {"paperId": "10d78553cc14f2113e752185d81a37a1ddad5496", "title": "Preference-Conditioned Language-Guided Abstraction"}, {"paperId": "2e8d818aea94fd76fe42ee0267640ab87912e053", "title": "MobilityGPT: Enhanced Human Mobility Modeling with a GPT model"}, {"paperId": "d80a87f3624461b36acdeb809c50979ba89fb1be", "title": "A Survey of Large Language Models in Finance (FinLLMs)"}, {"paperId": "836c652834b0f6ffe10e53ede1c0b9433cfad9ea", "title": "Copyright Protection in Generative AI: A Technical Perspective"}, {"paperId": "c991dedacba67949a28640cd8755de4c8ae297b0", "title": "A Survey on Data Selection for LLM Instruction Tuning"}, {"paperId": "acbce5ebf3f254188d10f6ba7de1ba716db89774", "title": "A Closer Look at the Limitations of Instruction Tuning"}, {"paperId": "bd0cd89337cc40d39d3a4cbe9c8709e06e877f3e", "title": "Continual Learning for Large Language Models: A Survey"}, {"paperId": "aaad842a346edeeb28ba9e1d16b3dfadf55de2d8", "title": "Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?"}, {"paperId": "b31f198842ecc19299ddb666f6221398299d1e08", "title": "How Useful is Continued Pre-Training for Generative Unsupervised Domain Adaptation?"}, {"paperId": "cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3", "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"}, {"paperId": "573bb43c3c99b953f3b0cf5aa96e2aec1efc3bbb", "title": "Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation"}, {"paperId": "8f018d29d9fbd7ff2c37df676081cd47f321b59b", "title": "Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes"}, {"paperId": "ecc3415b74717b3f786760e12934a31b37d98312", "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data"}, {"paperId": "b2f4d22fddf3619a38a1754d9497935aa0848426", "title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey"}, {"paperId": "abac48e32e7bb4f3556f7d16029f1f141d60e67b", "title": "Large Language Model Lateral Spear Phishing: A Comparative Study in Large-Scale Organizational Settings"}, {"paperId": "221a0e0c798d4bbc8a057d869b7251e03f1b0790", "title": "ChatQA: Building GPT-4 Level Conversational QA Models"}, {"paperId": "449eb627a55bda10cf6c899f5fc941828f34e5a5", "title": "Instruct-Imagen: Image Generation with Multi-modal Instruction"}, {"paperId": "4c4544beb93df95b8ce49d15c8c78e766b1f5bae", "title": "Using Large Language Models to Enhance the Reusability of Sensor Data"}, {"paperId": "6a6f6070e765f77422c6d6c14c1e6b41a9ed562a", "title": "Jatmo: Prompt Injection Defense by Task-Specific Finetuning"}, {"paperId": "bbf159cfafbb37108bafdf9dcf4767643ae5d23d", "title": "RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation"}, {"paperId": "ab7d320cbae173aef86c31faa087780cba44551f", "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling"}, {"paperId": "e305b60c0743b0399c737c14749c3ac17b52bfa6", "title": "Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code"}, {"paperId": "8bbcc26b4b8edf730f1b19997b318e9e63e80159", "title": "LLM Instruction-Example Adaptive Prompting (LEAP) Framework for Clinical Relation Extraction"}, {"paperId": "7a31971b0af439dec6fc484cca20df57f440b644", "title": "One Shot Learning as Instruction Data Prospector for Large Language Models"}, {"paperId": "70a55b59314f79966e111d136a7b0f2bb272501a", "title": "Distilling Large Language Models for Matching Patients to Clinical Trials"}, {"paperId": "fc237b87681cb11a42890b829dcb413a4aecfd57", "title": "Towards Trustworthy AI Software Development Assistance"}, {"paperId": "5ee871537ae51e7e2e93d2a70fff5d100649a655", "title": "Mathematical Language Models: A Survey"}, {"paperId": "af89c2e64cfd12c0553500889d9bd04530b6e7cc", "title": "Sim-GPT: Text Similarity via GPT Annotated Data"}, {"paperId": "117a9f9227ebe4acc67ff8a52eb9db2ab16390c8", "title": "Data Management For Large Language Models: A Survey"}, {"paperId": "7e17ef56273063dfa838de30b7cc0546b2e5ee10", "title": "Jellyfish: A Large Language Model for Data Preprocessing"}, {"paperId": "dac7158541d30f9a1eb663bc8dc21f5475d47043", "title": "Instruction-tuning Aligns LLMs to the Human Brain"}, {"paperId": "51fb6598a3ebe36b371b096b4824d718e6e527fb", "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey"}, {"paperId": "ea12b4bff088bb3829e7277e516842e552a63be4", "title": "The Philosopher's Stone: Trojaning Plugins of Large Language Models"}, {"paperId": "6b3e292c7e3963c3292a803b816438ab6b9a6c14", "title": "Hyperparameter Optimization for Large Language Model Instruction-Tuning"}, {"paperId": "843b4a054236e83e9cfe8c2ba8ccb99bb442ba09", "title": "AviationGPT: A Large Language Model for the Aviation Domain"}, {"paperId": "0fd454e62f76b9edcfa8baa43a7467368748483f", "title": "CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM Instruction Tuning"}, {"paperId": "91e3906550821c4624146e6e87db36c3296e773a", "title": "Applications of Large Scale Foundation Models for Autonomous Driving"}, {"paperId": "5760d44f33375d696d354d0cea6c01a6c9177f14", "title": "From beasts to bytes: Revolutionizing zoological research with artificial intelligence"}, {"paperId": "62c8c4e610abf823e6c4f5d3cfc2aa006c59036c", "title": "Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies"}, {"paperId": "f7a0501a246882103bc84cbc4f7270d1e7e428a8", "title": "Enabling Large Language Models to Learn from Rules"}, {"paperId": "bf11f01929afed0ad3ccfe1b5e0fd34d90ef2b4f", "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"}, {"paperId": "3cc6c4e88b6135d53938315a262285a501803c48", "title": "Vision-Language Instruction Tuning: A Review and Analysis"}, {"paperId": "bb638b927d15b4d1e2774342a851f2736fc45767", "title": "Psychometric Predictive Power of Large Language Models"}, {"paperId": "a8568bac56c24b5d25e373a117f947171d5f97be", "title": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering"}, {"paperId": "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8", "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"}, {"paperId": "bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3", "title": "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"}, {"paperId": "eaa70e42a10364ffe87e84656df435cd58fb430e", "title": "Sentiment Analysis through LLM Negotiations"}, {"paperId": "8d2709ed1788a67e64425fb410bb49f3ee49e088", "title": "Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"}, {"paperId": "f8f9b9e5a1b9dd67fe61bd358f1d90f6cceeb4a1", "title": "Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace"}, {"paperId": "c2efafc7036a61c0038f068632f4f55f8292373e", "title": "Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective"}, {"paperId": "ab90b84b42d43c3077c374cd34b3a48a881faf43", "title": "Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation"}, {"paperId": "ffb2f048ae694e08a741f8aa339e0f80003497bc", "title": "Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases"}, {"paperId": "1fb8f2d080e965c833c777f06fccf09dc9856b91", "title": "MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications"}, {"paperId": "4941b37136cdf1836b78ddb6cee65a28c3ce45f0", "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models"}, {"paperId": "c59b23ee87ba01942a7bab79728f85e66ba34ec7", "title": "Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models"}, {"paperId": "67daf8c4fe1958d20ebdf95c2a36dd490c73836f", "title": "FireAct: Toward Language Agent Fine-tuning"}, {"paperId": "c31396f00c4e4ddba20d085d0da819b89c71bf4a", "title": "CITING: Large Language Models Create Curriculum for Instruction Tuning"}, {"paperId": "ab90da70bf4671bb95d8e7ff97b2cce19768c579", "title": "Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models"}, {"paperId": "d6354e91d8dcf73bff50097b76a81de874f7bd7a", "title": "OceanGPT: A Large Language Model for Ocean Science Tasks"}, {"paperId": "af089827ede3207283a96e07732ab2e6b7fdc661", "title": "Instances Need More Care: Rewriting Prompts for Instances with LLMs in the Loop Yields Better Zero-Shot Performance"}, {"paperId": "0ea1d396ce3804054c1919d7b78d3bcddaa761c0", "title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models"}, {"paperId": "0f45608ddc01b3e192f3490330f4c4b8de074f79", "title": "Are Human-generated Demonstrations Necessary for In-context Learning?"}, {"paperId": "77b046c5d568b329a927cfc895ea2e6c8f43ff43", "title": "The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute"}, {"paperId": "bc9f29881c1d93d225f0a74fa700531202c7043a", "title": "OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch"}, {"paperId": "1ecf2955a2f4f2039f36b0334e2c376a5c901d6c", "title": "LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models"}, {"paperId": "78b2336d5d0c9e5dd1eefebdc82faf90d9586d81", "title": "Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models"}, {"paperId": "d00735241af700d21762d2f3ca00d920241a15a4", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"}, {"paperId": "f5ec0b22930faf15c3a7912bda367e8a9f4c8bd4", "title": "On the Unexpected Abilities of Large Language Models"}, {"paperId": "9afa0c3227fd0ec3a76928784e59c4205cbace24", "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks"}, {"paperId": "586d317634acea3a327e0567c210c35e93835985", "title": "How Can Client Motivational Language Inform Psychotherapy Agents?"}, {"paperId": "014f9a903edba91a4aca33e424db518cb85e2f9f", "title": "CEAN GPT: A L ARGE"}, {"paperId": "22986a1ba2f3c409e860fd835fadb3d2541b4113", "title": "Speculative Behavior: An Approach to Large Language Model Evaluation and Optimization"}, {"paperId": "7a8b0544912a96593da9e12fc86790e4c7a4a23d", "title": "Natural Language Explanations for Suicide Risk Classification Using Large Language Models"}]}
