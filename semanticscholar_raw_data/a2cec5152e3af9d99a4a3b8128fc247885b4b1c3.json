{"paperId": "a2cec5152e3af9d99a4a3b8128fc247885b4b1c3", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning", "abstract": "In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-08-08", "journal": {"name": "ArXiv", "volume": "abs/2308.04275"}, "authors": [{"authorId": "40500540", "name": "Xiaochuang Han"}], "citations": [{"paperId": "66e7edf09589527ebb58418632418758cee668cd", "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"}, {"paperId": "f6d7482bb5baf33f22b862ad4997f5c8cd13db21", "title": "Tuning Language Models by Proxy"}, {"paperId": "5708f725e13362da80a1062f51df118fca3529ab", "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples"}, {"paperId": "600d9287efc4703bdb99ce39b5e8b37da0baa6f6", "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning"}]}
