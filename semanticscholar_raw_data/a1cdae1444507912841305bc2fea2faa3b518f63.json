{"paperId": "a1cdae1444507912841305bc2fea2faa3b518f63", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks", "abstract": "Recently, Large language models (LLMs) with powerful general capabilities have been increasingly integrated into various Web applications, while undergoing alignment training to ensure that the generated content aligns with user intent and ethics. Unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications. Current approaches primarily rely on detecting, collecting, and training against harmful prompts to prevent such risks. However, they typically focused on the\"superficial\"harmful prompts with a solitary intent, ignoring composite attack instructions with multiple intentions that can easily elicit harmful content in real-world scenarios. In this paper, we introduce an innovative technique for obfuscating harmful instructions: Compositional Instruction Attacks (CIA), which refers to attacking by combination and encapsulation of multiple instructions. CIA hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify underlying malicious intentions. Furthermore, we implement two transformation methods, known as T-CIA and W-CIA, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety assessment datasets and two harmful prompt datasets. It achieves an attack success rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets. Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development. Warning: this paper may contain offensive or upsetting content!", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-16", "journal": {"name": "ArXiv", "volume": "abs/2310.10077"}, "authors": [{"authorId": "7027584", "name": "Shuyu Jiang"}, {"authorId": "2257126773", "name": "Xingshu Chen"}, {"authorId": "2258720735", "name": "Rui Tang"}], "citations": [{"paperId": "8b1378a728ac223309e5e4c6d2006654b2d469bf", "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models"}, {"paperId": "4ab03200801816b27d1363373e9c55c115c4b09b", "title": "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History"}, {"paperId": "76132c4494ef9d4792b7aeb7a868ffe45ca850a8", "title": "A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents"}, {"paperId": "383c598625110e0a4c60da4db10a838ef822fbcf", "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"}, {"paperId": "1a5a79b393b3f00eb5a47243ee031ad799d2f641", "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"}, {"paperId": "263a58f4fd32caca1dad2351af4d711aec451fe6", "title": "Evil Geniuses: Delving into the Safety of LLM-based Agents"}]}
