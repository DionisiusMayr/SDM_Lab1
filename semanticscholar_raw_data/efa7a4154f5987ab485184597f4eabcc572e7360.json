{"paperId": "efa7a4154f5987ab485184597f4eabcc572e7360", "publicationVenue": {"id": "0942fb86-c16f-4084-9902-10ddcfe18180", "name": "Micro", "type": "conference", "alternate_names": ["Int Symp Microarchitecture", "MICRO", "International Symposium on Microarchitecture", "Annual IEEE/ACM International Symposium on Microarchitecture", "Annu IEEE/ACM Int Symp Microarchitecture"], "issn": "0271-9002", "alternate_issns": ["2151-4143", "2673-8023"], "url": "http://www.microarch.org/"}, "title": "Efficient SpMV Operation for Large and Highly Sparse Matrices using Scalable Multi-way Merge Parallelization", "abstract": "The importance of Sparse Matrix dense Vector multiplication (SpMV) operation in graph analytics and numerous scientific applications has led to development of custom accelerators that are intended to over-come the difficulties of sparse data operations on general purpose architectures. However, efficient SpMV operation on large problem (i.e. working set exceeds on-chip storage) is severely constrained due to strong dependence on limited amount of fast random access memory to scale. Additionally, unstructured matrix with high sparsity pose difficulties as most solutions rely on exploitation of data locality. This work presents an algorithm co-optimized scalable hardware architecture that can efficiently operate on very large (~billion nodes) and/or highly sparse (avg. degree <10) graphs with significantly less on-chip fast memory than existing solutions. A novel parallelization methodology for implementing large and high throughput multi-way merge network is the key enabler of this high performance SpMV accelerator. Additionally, a data compression scheme to reduce off-chip traffic and special computation for nodes with exceptionally large number of edges, commonly found in power-law graphs, are presented. This accelerator is demonstrated with 16-nm fabricated ASIC and Stratix\u00ae 10 FPGA platforms. Experimental results show more than an order of magnitude improvement over current custom hardware solutions and more than two orders of magnitude improvement over commercial off-the-shelf (COTS) architectures for both performance and energy efficiency.", "venue": "Micro", "year": 2019, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2019-10-12", "journal": {"name": "Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture"}, "authors": [{"authorId": "3102425", "name": "Fazle Sadi"}, {"authorId": "144940567", "name": "Joseph Sweeney"}, {"authorId": "3124035", "name": "Tze Meng Low"}, {"authorId": "1703329", "name": "J. Hoe"}, {"authorId": "1795550", "name": "L. Pileggi"}, {"authorId": "1803350", "name": "F. Franchetti"}], "citations": [{"paperId": "f5ee953fba3959b63db569ae03432f7c5e8aeb44", "title": "Rubick: A Unified Infrastructure for Analyzing, Exploring, and Implementing Spatial Architectures via Dataflow Decomposition"}, {"paperId": "52b8337fbc94e6c1b31e4126d6b6c4d3d29c40ea", "title": "Cerberus: Triple Mode Acceleration of Sparse Matrix and Vector Multiplication"}, {"paperId": "0c63540cec789e45950eefc38c8201b5c519f8f2", "title": "SparseACC: A Generalized Linear Model Accelerator for Sparse Datasets"}, {"paperId": "3262ba0e1753613c5002614909f5a2ff1e9c8cee", "title": "Dedicated Hardware Accelerators for Processing of Sparse Matrices and Vectors: A Survey"}, {"paperId": "149e6824aa241cd6faa1819b5aa85338dbeda0f6", "title": "Graph for Science: From API based Programming to Graph Engine based Programming for HPC"}, {"paperId": "ebbbb056b72c1fe96d181e890e951b5f2f6b36e1", "title": "Spatula: A Hardware Accelerator for Sparse Matrix Factorization"}, {"paperId": "a79f72248ef6fb59269a5f7c9e56db0f16b8ac08", "title": "Pipestitch: An energy-minimal dataflow architecture with lightweight threads"}, {"paperId": "ad8ee8ea6bd57d98c32c5d41943393c57256545e", "title": "Accelerating SpMV on FPGAs Through Block-Row Compress: A Task-Based Approach"}, {"paperId": "27cd904a301ebc5e83504d9a0477bc480dc6e3d4", "title": "Spica: Exploring FPGA Optimizations to Enable an Efficient SpMV Implementation for Computations at Edge"}, {"paperId": "06bc044d391ed4ecfa162c672f067d1cb2f6046f", "title": "A Survey of Accelerating Parallel Sparse Linear Algebra"}, {"paperId": "3f5e63168d0ae1af41c3434e9e3e7e84dda9a5d8", "title": "FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction"}, {"paperId": "3c930af43515f3c8fe33b11202f6ca8ed85b3111", "title": "Sparse Stream Semantic Registers: A Lightweight ISA Extension Accelerating General Sparse Linear Algebra"}, {"paperId": "77d642d1add1fa21bfdfc29f972d161c21c1a38c", "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"}, {"paperId": "4ce86e7bb5a598ff0b5f07f040e1910bc50c8c49", "title": "Accelerating Sparse Data Orchestration via Dynamic Reflexive Tiling"}, {"paperId": "2e24c192aa130183188b67f4ba4450605b6dbc6d", "title": "Software-hardware co-design for accelerating large-scale graph convolutional network inference on FPGA"}, {"paperId": "954bf70c187fe0611d0b5ccf5fa13883f1b76de9", "title": "SGCN: Exploiting Compressed-Sparse Features in Deep Graph Convolutional Network Accelerators"}, {"paperId": "57b84d0b1bb0cbf0725762d5b00ec22037645184", "title": "A sparse matrix vector multiplication accelerator based on high-bandwidth memory"}, {"paperId": "a04b6b6a98e318132b9c289303b290b2a9bd3416", "title": "Performance Enhancement Strategies for Sparse Matrix-Vector Multiplication (SpMV) and Iterative Linear Solvers"}, {"paperId": "1a5a9a1dac57e74745ccf056c1021df1eb7b2e9b", "title": "Memory-Side Acceleration and Sparse Compression for Quantized Packed Convolutions"}, {"paperId": "aee831f3e9525261f10a42997dc4582a8acf25bc", "title": "Sparse-T: Hardware accelerator thread for unstructured sparse data processing"}, {"paperId": "2a0ea17fc849b1ecb77db0f829e21ff61bec25ec", "title": "Slice-and-Forge: Making Better Use of Caches for Graph Convolutional Network Accelerators"}, {"paperId": "ba2df4ed3e06b1c7569b11708a6a945b16deb315", "title": "Squaring the circle: Executing Sparse Matrix Computations on FlexTPU---A TPU-Like Processor"}, {"paperId": "0ebc85727789414e51a4bd3a0fb3814c6bf9e4d5", "title": "Flexible Hardware Accelerator Design Generation with Spiral"}, {"paperId": "246e760c4d34db77c7c9a8923bae1e474cb186cc", "title": "ASA: Accelerating Sparse Accumulation in Column-wise SpGEMM"}, {"paperId": "afa7e152e5ba46b0f1ae803112eaaff9e0e7e1a9", "title": "MeNDA: a near-memory multi-way merge solution for sparse transposition and dataflows"}, {"paperId": "1de8e605b7f1ba72e1e23ea7acc436aad495abff", "title": "Gearbox: a case for supporting accumulation dispatching and hybrid partitioning in PIM-based accelerators"}, {"paperId": "75d00430e169519f871a8fecc2a1235643d6eb12", "title": "HETEROGENEOUS ARCHITECTURE FOR SPARSE DATA PROCESSING"}, {"paperId": "0294eda74cb5808eef1673c67819f03ec7708e89", "title": "A scalable adaptive-matrix SPMV for heterogeneous architectures"}, {"paperId": "cbf4d8928e9d55a03575fd752dcf9eeefcfbe0e3", "title": "SparseP"}, {"paperId": "b8be066b41179472c0bff1388b2fc92e338117d0", "title": "High-Performance Sparse Linear Algebra on HBM-Equipped FPGAs Using HLS: A Case Study on SpMV"}, {"paperId": "ef58a34c000f5f7cc4329d03641481342b830a84", "title": "SparseP: Towards Efficient Sparse Matrix Vector Multiplication on Real Processing-In-Memory Systems"}, {"paperId": "c76142d04ba5da3e4470e08ef9ddb54b8f56f89c", "title": "Serpens: a high bandwidth memory based accelerator for general-purpose sparse matrix-vector multiplication"}, {"paperId": "ae464dc54a594de682fddd59479736b1e65bbf52", "title": "TENET: A Framework for Modeling Tensor Dataflow Based on Relation-centric Notation"}, {"paperId": "ea76c776b3cd3ad448572999ad3b4323abe47453", "title": "Gamma: leveraging Gustavson\u2019s algorithm to accelerate sparse matrix multiplication"}, {"paperId": "24723c0487825448d10eb66c3bed22a2e6fd4574", "title": "Efficiently Solving Partial Differential Equations in a Partially Reconfigurable Specialized Hardware"}, {"paperId": "33001234d9fe7e230e3ce9d8371347e935dc46d7", "title": "Solving Large Top-K Graph Eigenproblems with a Memory and Compute-optimized FPGA Design"}, {"paperId": "483cf5bf9d8cea79751eb0befbd035c6c84177d2", "title": "Scaling up HBM Efficiency of Top-K SpMV for Approximate Embedding Similarity on FPGAs"}, {"paperId": "c674139862eb662b674969413f2a9af5b02c5f14", "title": "BoostGCN: A Framework for Optimizing GCN Inference on FPGA"}, {"paperId": "9f9f8113afd4c1e449293a46d99018df6c34e0e0", "title": "FAFNIR: Accelerating Sparse Gathering by Using Efficient Near-Memory Intelligent Reduction"}, {"paperId": "ed7a1bc29f273d296b1881a973141633226245ec", "title": "SPAGHETTI: Streaming Accelerators for Highly Sparse GEMM on FPGAs"}, {"paperId": "7e5589aa46d506d7fd5c9534b0844c4a0d0d566d", "title": "VIA: A Smart Scratchpad for Vector Units with Application to Sparse Matrix Computations"}, {"paperId": "6a70cf69e1749c9f19cf65cb0a477042932a9eae", "title": "SpaceA: Sparse Matrix Vector Multiplication on Processing-in-Memory Accelerator"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "7bb6015f45457fdbe3bb39e17dc9cb0786ecd688", "title": "A Modern Primer on Processing in Memory"}, {"paperId": "4176fa8d5a4f1c7d0abf85ce873d5bfe5cd41bd7", "title": "Accelerating PageRank in Shared-Memory for Efficient Social Network Graph Analytics"}, {"paperId": "099d6dfa111bc3df609a0466d73cbb5a0f3dfa4b", "title": "Copernicus: Characterizing the Performance Implications of Compression Formats Used in Sparse Workloads"}, {"paperId": "01ab2e17a4955a5d17f447a085a4aa412ef58f63", "title": "Newton: A DRAM-maker\u2019s Accelerator-in-Memory (AiM) Architecture for Machine Learning"}, {"paperId": "49165462db203dcae1f2089814774ce8fd749604", "title": "ExPress: Simultaneously Achieving Storage, Execution and Energy Efficiencies in Moderately Sparse Matrix Computations"}, {"paperId": "53439309acd147a51555dfcbe797beab652b25c5", "title": "Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise Sparsity"}, {"paperId": "f0b16a23a661fa28bb6d1d3ca9bfbe5b4a5db8f2", "title": "Efficient Block Algorithms for Parallel Sparse Triangular Solve"}, {"paperId": "07a79ff9168da5b54636f3887469daba9c60c3ad", "title": "MViD: Sparse Matrix-Vector Multiplication in Mobile DRAM for Accelerating Recurrent Neural Networks"}, {"paperId": "6ab7393739616fb613f60c36ee96933104528d5d", "title": "tpSpMV: A two-phase large-scale sparse matrix-vector multiplication kernel for manycore architectures"}, {"paperId": "c0d41f90eb35372e2346112995815556c8232600", "title": "DSAGEN: Synthesizing Programmable Spatial Accelerators"}, {"paperId": "e3f936f2dec36a6d2a278b9d4f128cbe33575c45", "title": "Multi-Mode SpMV Accelerator for Transprecision PageRank With Real-World Graphs"}, {"paperId": "61f33db10a1a442cf93178a965617188c9ce2c3e", "title": "Streaming Sparse Data on Architectures with Vector Extensions using Near Data Processing"}, {"paperId": "ae06ce1af0361e7c14d427ff591ab917586f6721", "title": "HHT: Hardware Support For Accelerating Sparse Matrix Algorithms"}]}
