{"paperId": "897940fb5dd4d739b88c4659c4565d05f48d06b8", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher", "abstract": "Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. Our code and data will be released at https://github.com/RobustNLP/CipherChat.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-08-12", "journal": {"name": "ArXiv", "volume": "abs/2308.06463"}, "authors": [{"authorId": "2143398111", "name": "Youliang Yuan"}, {"authorId": "12386833", "name": "Wenxiang Jiao"}, {"authorId": "2144328160", "name": "Wenxuan Wang"}, {"authorId": "2161306685", "name": "Jen-tse Huang"}, {"authorId": "40532404", "name": "Pinjia He"}, {"authorId": "2072684668", "name": "Shuming Shi"}, {"authorId": "2909321", "name": "Zhaopeng Tu"}], "citations": [{"paperId": "ed6d26a21184878a5cbb4320a01105b5329e2ada", "title": "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"}, {"paperId": "627a5edf93091a4a50c9501c5ae5541fde393fa3", "title": "JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks"}, {"paperId": "8bb527c332b133e52bb7b6c5da84375888f41c05", "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models"}, {"paperId": "2f4cc3f4a1c70cd5aca14c1304037491cd3aeb9b", "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content"}, {"paperId": "55eed6f9ede6c4187d849224d61e60fda73a54df", "title": "EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models"}, {"paperId": "425d3c381dde8f576b6237a6d443b97880dd6bc2", "title": "Tastle: Distract Large Language Models for Automatic Jailbreak Attack"}, {"paperId": "5f2f6f395b500010cec482776d2b885efc44599c", "title": "Exploring Safety Generalization Challenges of Large Language Models via Code"}, {"paperId": "06b9ad0b52d23231f650be0aeb0b17cc52c8a74b", "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning"}, {"paperId": "29083f7d40c7c9f4e490a8871d80e4f0fe18706f", "title": "Accelerating Greedy Coordinate Gradient via Probe Sampling"}, {"paperId": "f6fa682b62c7981402336ca57da1196ccbf3fc54", "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction"}, {"paperId": "72f51c3ef967f7905e3194296cf6fd8337b1a437", "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models"}, {"paperId": "d0746668f85fcdbe00306bd7486a24f8750207d5", "title": "How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries"}, {"paperId": "50ceabc6aa41e08480fa5976342bfe04bb47bce3", "title": "Defending Jailbreak Prompts via In-Context Adversarial Game"}, {"paperId": "1b95053af03b5a06809a4967c6cf5ca137bbcde4", "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks"}, {"paperId": "e79671a83e25288fedd897e1c9e6152f70f7f52e", "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey"}, {"paperId": "b0ada492ba48e85016cbbfd95ec7180fb7e79648", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast"}, {"paperId": "f75f401f046d508753d6b207f3f19414f489bd08", "title": "Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia"}, {"paperId": "aa6a03f3368cbb4a413f7e11650fb8a6a2b71de1", "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications"}, {"paperId": "88d59e31575f5b3dd88a2c2033b55f628c2adbc9", "title": "Weak-to-Strong Jailbreaking on Large Language Models"}, {"paperId": "4fda99880cdbf8f178f01eb4c8dbdae7f959ea94", "title": "Red-Teaming for Generative AI: Silver Bullet or Security Theater?"}, {"paperId": "1fec7c75f2183a20b99dd8c34f13f9886e368f06", "title": "An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios"}, {"paperId": "be6873f649f809592aea9c5f07060799a1609411", "title": "MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds"}, {"paperId": "0e0ea3593dda3039cb93d2ec795a87420006ec08", "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents"}, {"paperId": "b843fd79f0ddfd1a3e5ff3bd182715429e28aa35", "title": "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering"}, {"paperId": "732ce53c573475f2691a7cfc716cf4f568d17360", "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs"}, {"paperId": "8fd29e810540c40846cddce3cbdf5060cd59fb57", "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender"}, {"paperId": "de4dfb773ab455081e5fb1862e08f581c58d43bc", "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems"}, {"paperId": "5fce3ce847e1cf3ccdf45fbedd5e02f90526b7e7", "title": "Maatphor: Automated Variant Analysis for Prompt Injection Attacks"}, {"paperId": "89641466373aa9ce2976e3f384b0791a7bd0931c", "title": "Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs"}, {"paperId": "54c9a97637822c9e1956b1ec70b0c9a0f2338d2c", "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking"}, {"paperId": "e92e8ff1becb9a9e4a7dd09878eaacb2a62ffb6b", "title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"}, {"paperId": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed", "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"}, {"paperId": "712f6dfcee099ee38d6d09af23e8bc0a7e82bb72", "title": "Fake Alignment: Are LLMs Really Aligned Well?"}, {"paperId": "b78b5ce5f21f46d8149824463f8eebd6103d49aa", "title": "FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"}, {"paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2", "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"}, {"paperId": "b80a7663585123abc53c10f8aac9bd234eedf063", "title": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems"}, {"paperId": "c7ad19da81e24c387f0377fef6d19b0fce2cf470", "title": "Self-Guard: Empower the LLM to Safeguard Itself"}, {"paperId": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2", "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models"}, {"paperId": "ffb2f048ae694e08a741f8aa339e0f80003497bc", "title": "Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases"}, {"paperId": "1a9f394b5b7f5bcdecee487174a3f4fc65d30e33", "title": "Multilingual Jailbreak Challenges in Large Language Models"}, {"paperId": "764fc56883bf83392cac99a7b5a264ac9fe2cdc5", "title": "Low-Resource Languages Jailbreak GPT-4"}, {"paperId": "f3f23f7f9f5369aade19f20bc5d028cce7b9c9aa", "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"}, {"paperId": "c02cded20074fff4310d7bc943d0b8bfff305d58", "title": "Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench"}, {"paperId": "6f75e8b61f13562237851d8119cb2f9d49e073fb", "title": "Can LLM-Generated Misinformation Be Detected?"}, {"paperId": "d4177489596748e43aa571f59556097f2cc4c8be", "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts"}, {"paperId": "cd29c25c489562b409a60f83365f93f33ee1a0a1", "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"}, {"paperId": "b8b8d5655df1c6a71bbb713387863e34cc055332", "title": "Detecting Language Model Attacks with Perplexity"}, {"paperId": "7cfaec8004c6d9f4fb5cf10287d15513c35b0a63", "title": "Towards Safer Generative Language Models: A Survey on Safety Risks, Evaluations, and Improvements"}, {"paperId": "b819f721cec2331fb58794b86bceda215807485e", "title": "How Generative AI Was Mentioned in Social Media and Academic Field? A Text Mining Based on Internet Text Data"}, {"paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4", "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models"}]}
