{"paperId": "20eba77b3ecbb040b8cff2b87c0506f52916c310", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "From Indeterminacy to Determinacy: Augmenting Logical Reasoning Capabilities with Large Language Models", "abstract": "Recent advances in LLMs have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior works focus on modeling reasoning steps using specific thought structures like chains, trees, or graphs. However, LLM-based reasoning continues to encounter three challenges: 1) Selecting appropriate reasoning structures for various tasks; 2) Exploiting known conditions sufficiently and efficiently to deduce new insights; 3) Considering the impact of historical reasoning experience. To address these challenges, we propose DetermLR, a novel reasoning framework that formulates the reasoning process as a transformational journey from indeterminate premises to determinate ones. This process is marked by the incremental accumulation of determinate premises, making the conclusion progressively closer to clarity. DetermLR includes three essential components: 1) Premise identification: We categorize premises into two distinct types: determinate and indeterminate. This empowers LLMs to customize reasoning structures to match the specific task complexities. 2) Premise prioritization and exploration: We leverage quantitative measurements to assess the relevance of each premise to the target, prioritizing more relevant premises for exploring new insights. 3) Iterative process with reasoning memory: We introduce a reasoning memory module to automate storage and extraction of available premises and reasoning paths, preserving historical reasoning details for more accurate premise prioritization. Comprehensive experimental results show that DetermLR outperforms all baselines on four challenging logical reasoning tasks: LogiQA, ProofWriter, FOLIO, and LogicalDeduction. DetermLR can achieve better reasoning performance while requiring fewer visited states, highlighting its superior efficiency and effectiveness in tackling logical reasoning tasks.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-28", "journal": {"name": "ArXiv", "volume": "abs/2310.18659"}, "authors": [{"authorId": "2257127695", "name": "Hongda Sun"}, {"authorId": "2262867071", "name": "Weikai Xu"}, {"authorId": "2257333016", "name": "Wei Liu"}, {"authorId": "2257013742", "name": "Jian Luan"}, {"authorId": "2257388949", "name": "Bin Wang"}, {"authorId": "2232780079", "name": "Shuo Shang"}, {"authorId": "2263887786", "name": "Ji-Rong Wen"}, {"authorId": "2257014132", "name": "Rui Yan"}], "citations": [{"paperId": "507acddb0b7f36b83fd7c8bff2f121eb506ac8fb", "title": "Cumulative Reasoning with Large Language Models"}]}
