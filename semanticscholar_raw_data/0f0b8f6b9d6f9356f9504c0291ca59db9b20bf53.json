{"paperId": "0f0b8f6b9d6f9356f9504c0291ca59db9b20bf53", "publicationVenue": {"id": "3779a5a7-9119-4f69-84fe-f7eef193eb49", "name": "Conference on Computational Natural Language Learning", "type": "conference", "alternate_names": ["CoNLL", "Conf Comput Nat Lang Learn"]}, "title": "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization", "abstract": "Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing 300 document-instruction pairs with 3 answers each. All 900 answers are rated by 3 human annotators. Using riSum, we analyze the agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on par with costly reference-based metrics that require high-quality summaries.", "venue": "Conference on Computational Natural Language Learning", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-12", "journal": {"pages": "221-237"}, "authors": [{"authorId": "52018133", "name": "Ondrej Skopek"}, {"authorId": "2257346647", "name": "Rahul Aralikatte"}, {"authorId": "2265752448", "name": "Sian Gooding"}, {"authorId": "10740650", "name": "V. Carbune"}], "citations": [{"paperId": "acf3e66d4e046dc021e75f223a681d72203efa61", "title": "METAL: Towards Multilingual Meta-Evaluation"}, {"paperId": "e29743ff496d0b6e59d10b86fc9cd7f00a1adaf3", "title": "RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models"}, {"paperId": "37cd61cdc7fc9db4791cc15706704a4789e96f0c", "title": "Measuring and Controlling Instruction (In)Stability in Language Model Dialogs"}, {"paperId": "4b8df079495cbec21ae90d60ab84e8dd813ca7e6", "title": "Leveraging Large Language Models for NLG Evaluation: A Survey"}, {"paperId": "f495d2741f804cb37a6afc3c039f1d3964b964a8", "title": "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization"}]}
