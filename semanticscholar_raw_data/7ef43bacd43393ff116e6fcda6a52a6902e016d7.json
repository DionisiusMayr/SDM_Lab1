{"paperId": "7ef43bacd43393ff116e6fcda6a52a6902e016d7", "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods in Natural Language Processing", "type": "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "title": "\u201cI\u2019m sorry to hear that\u201d: Finding New Biases in Language Models with a Holistic Descriptor Dataset", "abstract": "As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes and are commonly used with preset bias tests that presuppose which types of biases models can exhibit. In this work, we present a new, more inclusive bias measurement dataset, HolisticBias, which includes nearly 600 descriptor terms across 13 different demographic axes. HolisticBias was assembled in a participatory process including experts and community members with lived experience of these terms. These descriptors combine with a set of bias measurement templates to produce over 450,000 unique sentence prompts, which we use to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that HolisticBias is effective at measuring previously undetectable biases in token likelihoods from language models, as well as in an offensiveness classifier. We will invite additions and amendments to the dataset, which we hope will serve as a basis for more easy-to-use and standardized methods for evaluating bias in NLP models.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-05-18", "journal": {"pages": "9180-9211"}, "authors": [{"authorId": "51324296", "name": "Eric Michael Smith"}, {"authorId": "120861776", "name": "Melissa Hall"}, {"authorId": "2272979", "name": "Melanie Kambadur"}, {"authorId": "6072807", "name": "Eleonora Presani"}, {"authorId": "2110032535", "name": "Adina Williams"}], "citations": [{"paperId": "d862aae89c364eb97442372979a0d185e95ae3bb", "title": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety"}, {"paperId": "d95bf07bf21d8c17810da9c1f87190257d757b05", "title": "Developing Safe and Responsible Large Language Models -- A Comprehensive Framework"}, {"paperId": "3b263ca9f0cebd2943ac5f181e68044e84238f7c", "title": "Uncovering Bias in Large Vision-Language Models with Counterfactuals"}, {"paperId": "8f77307a394f006633c27cca56a345c0879eb67c", "title": "Measuring Political Bias in Large Language Models: What Is Said and How It Is Said"}, {"paperId": "0fd73d32176189f7980847206a9d797c3b0f4e1d", "title": "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception"}, {"paperId": "554bc2c910bf33cbab6fc0d174719fd3513333f9", "title": "Protected group bias and stereotypes in Large Language Models"}, {"paperId": "d8f8a61a497c999471246c712acd98ce49b92123", "title": "Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies"}, {"paperId": "721ea8aad94376db2489428334c7be98b0c53878", "title": "Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations"}, {"paperId": "3283764bbbd7084c9e6995e20953dbf36b25a226", "title": "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!"}, {"paperId": "839629148d8c1a4d8b7fe0e37c89a543b6e985d7", "title": "Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric"}, {"paperId": "7547e30ba98a0217f07a6bb9fc393902bbc89269", "title": "SpiRit-LM: Interleaved Spoken and Written Language Model"}, {"paperId": "ce0219aab283c07369187348d72f8c5fe9898e9b", "title": "Large Language Models are Geographically Biased"}, {"paperId": "de4dfb773ab455081e5fb1862e08f581c58d43bc", "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems"}, {"paperId": "2f89071c59b71effc434f9eed8697e3152f7acd4", "title": "Fairness-Aware Structured Pruning in Transformers"}, {"paperId": "3409a29c7287a5e0010f48f8bca42679e3b10c12", "title": "Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies"}, {"paperId": "da89cdeb0014666f4024f797d0c67cd45d92a7c9", "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity"}, {"paperId": "2384f94c1555e5b8ef90c042441fb9a99dc9389e", "title": "Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples"}, {"paperId": "9c893f54d86a362b8e62e5883bb38c14240441f5", "title": "Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting"}, {"paperId": "33ba6ff1d2b599178e60d029da10b41f7a3c4729", "title": "\"One-Size-Fits-All\"? Examining Expectations around What Constitute\"Fair\"or\"Good\"NLG System Behaviors"}, {"paperId": "e76cf464a3a685acab9ad8cabeae05ecc3e5c641", "title": "Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems"}, {"paperId": "c811b7e98b755ab7d34baa466796d00a93f662e7", "title": "Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models"}, {"paperId": "749d59f887c8ac83fd4f5178465e8b03e463358c", "title": "Large Language Model Alignment: A Survey"}, {"paperId": "e65d911a698c406e2ebfa3262e11f223fe70053b", "title": "Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models"}, {"paperId": "e4282cab4a435d5249fc8db49fc1c9268438fedb", "title": "Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West"}, {"paperId": "fec06097c0db7e0b0347ae31585e246dba06ea49", "title": "Challenges in Annotating Datasets to Quantify Bias in Under-represented Society"}, {"paperId": "dfabb392f8bb04a0d87d47ce67dd9d98ae8e742d", "title": "TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models"}, {"paperId": "fb2719aa3245a1757144d273be0a9b3a96d43a3f", "title": "Gender-specific Machine Translation with Large Language Models"}, {"paperId": "6812ea42fd92397f643c3b3c98fdc53622f90c9b", "title": "How Crowd Worker Factors Influence Subjective Annotations: A Study of Tagging Misogynistic Hate Speech in Tweets"}, {"paperId": "bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7", "title": "Bias and Fairness in Large Language Models: A Survey"}, {"paperId": "98e96775e5f86bcbbab686213ca46f472694e003", "title": "The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages"}, {"paperId": "cf852c18387cfcdcb3cb3bb5ba6a549b35766c33", "title": "Gender bias and stereotypes in Large Language Models"}, {"paperId": "d2e57d6ba6989ed3e76884f3d0e0e77ba3118528", "title": "CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias"}, {"paperId": "03bf28df6e282a7e36e1686edeb9c624e6ffb13b", "title": "A Survey on Fairness in Large Language Models"}, {"paperId": "d0dcedafee8dc07dcb361f017af9b120385bc050", "title": "TorchQL: A Programming Framework for Integrity Constraints in Machine Learning"}, {"paperId": "a00537a398ce1cacf3b6835a9817ade535d2dae2", "title": "DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "110ee943ab797120c333d1446876ad47354d1df1", "title": "Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser with Prompts"}, {"paperId": "88549b4f48b9709acdfb8b9e41656b6d133c5390", "title": "Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models"}, {"paperId": "72c7cb545f7da68efd1014afe3a4f01b590e435b", "title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models"}, {"paperId": "9d81ec931b85d6c6cf3453126670cd7a30a689e7", "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models"}, {"paperId": "cfd2145fa17d2fcd7f1dba27bd713eaa4e798c1f", "title": "Sociodemographic Bias in Language Models: A Survey and Forward Path"}, {"paperId": "cfda0a6bafae0580b256c0e8883a2810a013a693", "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks"}, {"paperId": "0b6edce3dde7e502c6b7c6d83bac0230ec912482", "title": "Improving Open Language Models by Learning from Organic Interactions"}, {"paperId": "8d9ca1e2c703e2752a4904c967a65d45d0bef5f6", "title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models"}, {"paperId": "430a95e3cbebddf8d727c874e007d32ab844f148", "title": "Having Beer after Prayer? Measuring Cultural Bias in Large Language Models"}, {"paperId": "015d73903bf2460ba0642080f97c9917133f7056", "title": "Multilingual Holistic Bias: Extending Descriptors and Patterns to Unveil Demographic Biases in Languages at Scale"}, {"paperId": "27f9b6d984496b78998facd7c9ea53927d899c4a", "title": "ReSeTOX: Re-learning attention weights for toxicity mitigation in machine translation"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "784e03977f16b0b5a00c16523fdbcb9ffcefc545", "title": "Supporting Human-AI Collaboration in Auditing LLMs with LLMs"}, {"paperId": "7afac8aa4d64dd81023df2acb8615d23ceea3767", "title": "Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP"}, {"paperId": "12d16f426edc6ab248fb476007bd1646282d4d68", "title": "Language Model Behavior: A Comprehensive Survey"}, {"paperId": "2eb0d00e5675582980245b95a48e40bd8e5f46a0", "title": "Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities"}, {"paperId": "4ae163ad7dac91bae435eff844d0fd084f0399ec", "title": "Human-Guided Fair Classification for Natural Language Processing"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "0d5b0415b856318bf7702ba3c83695e7ce646605", "title": "Fairness Testing: A Comprehensive Survey and Analysis of Trends"}, {"paperId": "4c868a92c615df3859433e28b2441bbce9e65fb3", "title": "Reward Reports for Reinforcement Learning"}, {"paperId": "3ca6b9f70f1af02622c2c53537b009524e33c776", "title": "Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor"}, {"paperId": "92d2aca10f1aa68ca580e51ca732e517daeec102", "title": "An Empathy-Based Sandbox Approach to Bridge Attitudes, Goals, Knowledge, and Behaviors in the Privacy Paradox"}, {"paperId": "c373c792bcf5d0add8de812425d384ff101ef070", "title": "Unlearning Bias in Language Models by Partitioning Gradients"}, {"paperId": "84ea22760587182aafb42a469c7f66fc5686ad7c", "title": "T \ufffd\ufffd\ufffd\ufffd QL : A Programming Framework for Integrity Constraints in Machine Learning"}, {"paperId": "204e385124c188627caf4695ec4adbdde5c800cd", "title": "TorchQL: A Programming Framework for Integrity Constraints in Machine Learning"}]}
