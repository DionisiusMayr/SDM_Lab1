{"paperId": "8f68a1cbb3f81a806f955af764e7a424cf5105ff", "publicationVenue": {"id": "cd492924-29c4-496b-9535-082805f228d8", "name": "International Conference on Performance Engineering", "type": "conference", "alternate_names": ["Int Conf Perform Eng", "ICPE"], "url": "http://www.wikicfp.com/cfp/program?id=1445"}, "title": "Same, Same, but Dissimilar: Exploring Measurements for Workload Time-series Similarity", "abstract": "Benchmarking is a core element in the toolbox of most systems researchers and is used for analyzing, comparing, and validating complex systems. In the quest for reliable benchmark results, a consensus has formed that a significant experiment must be based on multiple runs. To interpret these runs, mean and standard deviation are often used. In case of experiments where each run produces a time series, applying and comparing the mean is not easily applicable and not necessarily statistically sound. Such an approach ignores the possibility of significant differences between runs with a similar average. In order to verify this hypothesis, we conducted a survey of 1,112 publications of selected performance engineering and systems conferences canvassing open data sets from performance experiments. The identified 3 data sets purely rely on average and standard deviation. Therefore, we propose a novel analysis approach based on similarity analysis to enhance the reliability of performance evaluations. Our approach evaluates 12 (dis-)similarity measures with respect to their applicability in analysing performance measurements and identifies four suitable similarity measures. We validate our approach by demonstrating the increase in reliability for the data sets found in the survey.", "venue": "International Conference on Performance Engineering", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle", "Review"], "publicationDate": "2022-04-09", "journal": {"name": "Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering"}, "authors": [{"authorId": "51185014", "name": "Mark Leznik"}, {"authorId": "34825425", "name": "Johannes Grohmann"}, {"authorId": "2160047536", "name": "Nina Kliche"}, {"authorId": "151489116", "name": "Andr\u00e9 Bauer"}, {"authorId": "40596015", "name": "Daniel Seybold"}, {"authorId": "40879589", "name": "Simon Eismann"}, {"authorId": "2137102879", "name": "Samuel Kounev"}, {"authorId": "2214992", "name": "J\u00f6rg Domaschka"}], "citations": [{"paperId": "b17cb655d4cdab72839641cb701b1e1767d9b357", "title": "Searching for the Ground Truth: Assessing the Similarity of Benchmarking Runs"}]}
