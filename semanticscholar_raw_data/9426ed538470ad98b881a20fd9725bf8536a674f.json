{"paperId": "9426ed538470ad98b881a20fd9725bf8536a674f", "publicationVenue": null, "title": "FLIP: Towards Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction", "abstract": "Click-through rate (CTR) prediction plays as a core function module in various personalized online services. The traditional ID-based models for CTR prediction take as inputs the one-hot encoded ID features of tabular modality, which capture the collaborative signals via feature interaction modeling. But the one-hot encoding discards the semantic information conceived in the original feature texts. Recently, the emergence of Pretrained Language Models (PLMs) has given rise to another paradigm, which takes as inputs the sentences of textual modality obtained by hard prompt templates and adopts PLMs to extract the semantic knowledge. However, PLMs generally tokenize the input text data into subword tokens and ignore field-wise collaborative signals. Therefore, these two lines of research focus on different characteristics of the same input data (i.e., textual and tabular modalities), forming a distinct complementary relationship with each other. In this paper, we propose to conduct Fine-grained feature-level ALignment between ID-based Models and Pretrained Language Models (FLIP) for CTR prediction. We design a novel joint reconstruction pretraining task for both masked language and tabular modeling. Specifically, the masked data of one modality (i.e., tokens or features) has to be recovered with the help of the other modality, which establishes the feature-level interaction and alignment via sufficient mutual information extraction between dual modalities. Moreover, we propose to jointly finetune the ID-based model and PLM for downstream CTR prediction tasks, thus achieving superior performance by combining the advantages of both models. Extensive experiments on three real-world datasets demonstrate that FLIP outperforms SOTA baselines, and is highly compatible for various ID-based models and PLMs.", "venue": "", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2023-10-30", "journal": null, "authors": [{"authorId": "2113290993", "name": "Hangyu Wang"}, {"authorId": "2144908858", "name": "Jianghao Lin"}, {"authorId": "2181637944", "name": "Xiangyang Li"}, {"authorId": "2258709565", "name": "Bo Chen"}, {"authorId": "2115802321", "name": "Chenxu Zhu"}, {"authorId": "2257180930", "name": "Ruiming Tang"}, {"authorId": "2240768092", "name": "Weinan Zhang"}, {"authorId": "2237958078", "name": "Yong Yu"}], "citations": [{"paperId": "ac457964969cdf99650acacb65cd84985eb84863", "title": "Tired of Plugins? Large Language Models Can Be End-To-End Recommenders"}, {"paperId": "d066f8b59565f9cb6713ece87afc3f3268f45474", "title": "Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models"}, {"paperId": "8a439444d888202a711b8b8a195934cdb138342e", "title": "Towards Efficient and Effective Unlearning of Large Language Models for Recommendation"}, {"paperId": "b6bc1590ae632fd8325fab23edf5c333a9a7723c", "title": "A Unified Framework for Multi-Domain CTR Prediction via Large Language Models"}]}
