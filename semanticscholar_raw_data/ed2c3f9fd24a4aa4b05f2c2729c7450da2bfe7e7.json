{"paperId": "ed2c3f9fd24a4aa4b05f2c2729c7450da2bfe7e7", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs", "abstract": "Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs. We will release the code to facilitate future research.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-06", "journal": {"name": "ArXiv", "volume": "abs/2402.03804"}, "authors": [{"authorId": "2621696", "name": "Zhengyan Zhang"}, {"authorId": "2275214886", "name": "Yixin Song"}, {"authorId": "2283258934", "name": "Guanghui Yu"}, {"authorId": "48506411", "name": "Xu Han"}, {"authorId": "2427350", "name": "Yankai Lin"}, {"authorId": "51131083", "name": "Chaojun Xiao"}, {"authorId": "2283100077", "name": "Chenyang Song"}, {"authorId": "2261154189", "name": "Zhiyuan Liu"}, {"authorId": "2282962002", "name": "Zeyu Mi"}, {"authorId": "2273551430", "name": "Maosong Sun"}], "citations": [{"paperId": "1c1b5bc728cb6c59574e77987441ec066bea9109", "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models"}]}
