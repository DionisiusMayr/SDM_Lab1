{"paperId": "4c86f91116483d946bde77684aa10844f932d5f5", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations", "abstract": "Large language models (LLMs) and their variants have shown extraordinary efficacy across numerous downstream natural language processing (NLP) tasks, which has presented a new vision for the development of NLP. Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typical trained without leveraging multi-modal information. To overcome these limitations, we propose DialogueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues. The visual information is considered as the supplementary knowledge to construct high-quality instructions. We offer a comprehensive evaluation of our proposed model on three benchmarking emotion recognition in conversations (ERC) datasets and compare the results against the SOTA baselines and other SOTA LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB A100 GPU in 5 hours, facilitating reproducibility for other researchers.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-17", "journal": {"name": "ArXiv", "volume": "abs/2310.11374"}, "authors": [{"authorId": "2108008897", "name": "Yazhou Zhang"}, {"authorId": "2258997604", "name": "Mengyao Wang"}, {"authorId": "2258956770", "name": "Prayag Tiwari"}, {"authorId": "2108273524", "name": "Qiuchi Li"}, {"authorId": "2259884847", "name": "Benyou Wang"}, {"authorId": "2259358765", "name": "Jing Qin"}], "citations": [{"paperId": "4f74968915d2a7268d10b6a7361e0a3504325171", "title": "Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation"}, {"paperId": "2b176beef141a8a2123d6a9738a02409e7d0e685", "title": "Training A Small Emotional Vision Language Model for Visual Art Comprehension"}, {"paperId": "4d4d9da4f2c39089ea6c8d84e5031c195548d7b6", "title": "Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence"}, {"paperId": "b078e62296e646fa08b39f3d980a652a37f5dc7b", "title": "EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis"}, {"paperId": "d13dc7f94991e4cf7fba70c340b1d9d36346f238", "title": "Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought"}, {"paperId": "c48debaf2a14017741e79f8236b72f89c933376c", "title": "eMotions: A Large-Scale Dataset for Emotion Recognition in Short Videos"}]}
