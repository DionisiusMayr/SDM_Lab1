{"paperId": "182a8db5b76cf69294ade7b8a3902850135218ba", "publicationVenue": {"id": "8ee71e17-421e-43db-ad2d-cc8af6217a0d", "name": "European Conference on Information Retrieval", "type": "conference", "alternate_names": ["ECIR", "Eur Conf Inf Retr"], "url": "https://en.wikipedia.org/wiki/European_Conference_on_Information_Retrieval"}, "title": "Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers", "abstract": "Large language models can now directly generate answers to many factual questions without referencing external sources. Unfortunately, relatively little attention has been paid to methods for evaluating the quality and correctness of these answers, for comparing the performance of one model to another, or for comparing one prompt to another. In addition, the quality of generated answers are rarely directly compared to the quality of retrieved answers. As models evolve and prompts are modified, we have no systematic way to measure improvements without resorting to expensive human judgments. To address this problem we adapt standard retrieval benchmarks to evaluate answers generated by large language models. Inspired by the BERTScore metric for summarization, we explore two approaches. In the first, we base our evaluation on the benchmark relevance judgments. We empirically run experiments on how information retrieval relevance judgments can be utilized as an anchor to evaluating the generated answers. In the second, we compare generated answers to the top results retrieved by a diverse set of retrieval models, ranging from traditional approaches to advanced methods, allowing us to measure improvements without human judgments. In both cases, we measure the similarity between an embedded representation of the generated answer and an embedded representation of a known, or assumed, relevant passage from the retrieval benchmark.", "venue": "European Conference on Information Retrieval", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-01-09", "journal": {"pages": "399-414"}, "authors": [{"authorId": "81447039", "name": "Negar Arabzadeh"}, {"authorId": "2073050239", "name": "A. Bigdeli"}, {"authorId": "2278827569", "name": "Charles L. A. Clarke"}], "citations": [{"paperId": "35a8d6d9188e60c515aa3184e7125307e57c61f4", "title": "A Comparison of Methods for Evaluating Generative IR"}, {"paperId": "0dfb0abcd2f0c60105ca8d1e7358ad0f1e8f14d6", "title": "An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments"}, {"paperId": "bfc0a7cf0c05c46373ed7bad379d8e11d1ad4dce", "title": "Fr\u00e9chet Distance for Offline Evaluation of Information Retrieval Systems with Sparse Labels"}]}
