{"paperId": "844bc3b26b5c63ec3b251ae634c194dcfb41a7d2", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning", "abstract": "We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned using a small set of carefully curated instructions on a well-trained foundation model, which is consistent with the Superficial Alignment Hypothesis (Zhou et al., 2023). From a practical perspective, this work develops a state-of-the-art financial domain LLM with superior capability in understanding financial texts and providing helpful investment advice, potentially enhancing the work efficiency of financial professionals. We release the model parameters to the research community.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science", "Economics"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-15", "journal": {"name": "ArXiv", "volume": "abs/2309.13064"}, "authors": [{"authorId": "2246043972", "name": "Yi Yang"}, {"authorId": "49578084", "name": "Yixuan Tang"}, {"authorId": "1805674", "name": "K. Tam"}], "citations": [{"paperId": "4c79dbd16ce257243ae53abf3cb905990015ef55", "title": "GreedLlama: Performance of Financial Value-Aligned Large Language Models in Moral Reasoning"}, {"paperId": "34268fbec2ba4cafc2b2dc4772a3102cea0bc6e6", "title": "Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives"}, {"paperId": "eb419b57023d7de3284b182a5b680195c9095040", "title": "No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "16c6af5ba8989a70c84567549effd2fd7932d2ec", "title": "D\u00f3lares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English"}, {"paperId": "d80a87f3624461b36acdeb809c50979ba89fb1be", "title": "A Survey of Large Language Models in Finance (FinLLMs)"}, {"paperId": "777bf0028ca91b8a82df6042eeb3662341a21205", "title": "Enabling Large Language Models to Think Twice When Its Answer Is Unreliable: A Case Study In Cancer Screening"}, {"paperId": "44d16a076c00ecada3d425203377e4ec951c4ed0", "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning"}, {"paperId": "c9b61a39f712da283395de8369f5f226f62b209c", "title": "FinEntity: Entity-level Sentiment Classification for Financial Texts"}, {"paperId": "99e470e72d74bebb31a08ca9d4cd6eca3db6ca7d", "title": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing"}, {"paperId": "14dcafae548d578f6b8c683d0972531bc46423ca", "title": "Co-audit: tools to help humans double-check AI-generated content"}, {"paperId": "6121fb3e393597e02481a516f0035f06ec9a5836", "title": "FinGPT: Democratizing Internet-scale Data for Financial Large Language Models"}]}
