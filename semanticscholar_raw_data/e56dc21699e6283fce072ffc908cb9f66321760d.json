{"paperId": "e56dc21699e6283fce072ffc908cb9f66321760d", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF", "abstract": "During the last stage of RLHF, a large language model is aligned to human intents via PPO training, a process that generally requires large-scale computational resources. In this technical report, we empirically investigate an efficient implementation of RLHF using low-rank adaptation (LoRA), which allows us to align the LLaMA 7B checkpoint on the Alpaca dataset using only two A100 GPUs instead of the eight required for full model fine-tuning. Despite tuning only 0.2% of LLaMA 7B's parameters, our implementation achieves better performance than the publicly-released AlpacaFarm checkpoint with full model fine-tuning. Next, we analyze several configurations of our LoRA-based PPO implementation, varying the form of the KL regularization term in the training objective. We find that (1) removing this penalty term does not harm performance on the AlpacaFarm evaluation set under our LoRA setup; (2) other regularizers, such as Jensen-Shannon divergence, lead to improved performance; and (3) while PPO training negatively impacts the factuality of model-generated responses, training with LoRA largely mitigates this effect. We release our code and pretrained checkpoints to facilitate future research on more efficient RLHF.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-16", "journal": {"name": "ArXiv", "volume": "abs/2309.09055"}, "authors": [{"authorId": "23134878", "name": "Simeng Sun"}, {"authorId": "2241349204", "name": "Dhawal Gupta"}, {"authorId": "2136562", "name": "Mohit Iyyer"}], "citations": [{"paperId": "6c776ab9d309ca2541ad087d8af784819d13357b", "title": "Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text Reranking with Large Language Models"}, {"paperId": "c78350e81298ca87bc1d59b466fa40081232caaa", "title": "Teaching Large Language Models to Reason with Reinforcement Learning"}, {"paperId": "8e624e215908934a38044500f8434a0f88c69059", "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models"}, {"paperId": "44162aa2763c88a384d9c51d60eafcc59277a1c9", "title": "Decoding-time Realignment of Language Models"}, {"paperId": "329a949f3d67fa345db5f7b807faef022e92d364", "title": "Preference-free Alignment Learning with Regularized Relevance Reward"}, {"paperId": "37680e5cb6030e01f1a44a5abe2257972196ae26", "title": "Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2"}, {"paperId": "2be910eb19f2f8f2e8038d2a835bc48f868ccbf1", "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models"}, {"paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579", "title": "A Long Way to Go: Investigating Length Correlations in RLHF"}, {"paperId": "ec0ca7eb1f365a8ee74247b9724e190651483706", "title": "Chatmap : Large Language Model Interaction with Cartographic Data"}, {"paperId": "bd5deadc58ee45b5e004378ba1d54a96bc947b4a", "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"}]}
