{"paperId": "7e36e6b1a29a7d0865f40f9266d84302929971f4", "publicationVenue": null, "title": "Enhancing Reasoning Capacity of SLM using Cognitive Enhancement", "abstract": "Large Language Models (LLMs) have been applied to automate cyber security activities and processes including cyber investigation and digital forensics. However, the use of such models for cyber investigation and digital forensics should address accountability and security considerations. Accountability ensures models have the means to provide explainable reasonings and outcomes. This information can be extracted through explicit prompt requests. For security considerations, it is crucial to address privacy and confidentiality of the involved data during data processing as well. One approach to deal with this consideration is to have the data processed locally using a local instance of the model. Due to limitations of locally available resources, namely memory and GPU capacities, a Smaller Large Language Model (SLM) will typically be used. These SLMs have significantly fewer parameters compared to the LLMs. However, such size reductions have notable performance reduction, especially when tasked to provide reasoning explanations. In this paper, we aim to mitigate performance reduction through the integration of cognitive strategies that humans use for problem-solving. We term this as cognitive enhancement through prompts. Our experiments showed significant improvement gains of the SLMs' performances when such enhancements were applied. We believe that our exploration study paves the way for further investigation into the use of cognitive enhancement to optimize SLM for cyber security applications.", "venue": "", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2024-04-01", "journal": null, "authors": [{"authorId": "2213687575", "name": "Jonathan Pan"}, {"authorId": "2266142024", "name": "Swee Liang Wong"}, {"authorId": "2294359097", "name": "Xin Wei Chia"}, {"authorId": "2265932326", "name": "Yidi Yuan"}], "citations": []}
