{"paperId": "fc4e95185b9a669dab434c42fdcc5aa13c59c0bb", "publicationVenue": null, "title": "A Unified and Efficient Coordinating Framework for Autonomous DBMS Tuning", "abstract": "Recently using machine learning (ML) based techniques to optimize the performance of modern database management systems (DBMSs) has attracted intensive interest from both industry and academia. With an objective to tune a specific component of a DBMS (e.g., index selection, knobs tuning), the ML-based tuning agents have shown to be able to find better configurations than experienced database administrators (DBAs). However, one critical yet challenging question remains unexplored -- how to make those ML-based tuning agents work collaboratively. Existing methods do not consider the dependencies among the multiple agents, and the model used by each agent only studies the effect of changing the configurations in a single component. To tune different components for DBMS, a coordinating mechanism is needed to make the multiple agents be cognizant of each other. Also, we need to decide how to allocate the limited tuning budget (e.g., time and resources) among the agents to maximize the performance. Such a decision is difficult to make since the distribution of the reward (i.e., performance improvement) corresponding to each agent is unknown and non-stationary. In this paper, we study the above question and present a unified coordinating framework to efficiently utilize existing ML-based agents. First, we propose a message propagation protocol that specifies the collaboration behaviors for agents and encapsulates the global tuning messages in each agent's model. Second, we combine Thompson Sampling, a well-studied reinforcement learning algorithm with a memory buffer so that our framework can allocate the tuning budget judiciously in a non-stationary environment. Our framework defines the interfaces adapted to a broad class of ML-based tuning agents, yet simple enough for integration with existing implementations and future extensions. Based on extensive evaluations, we show that this framework can effectively utilize different ML-based agents and find better configurations with 1.4~14.1x speedups on the workload execution time compared with baselines.", "venue": "Proc. ACM Manag. Data", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-03-10", "journal": {"name": "Proceedings of the ACM on Management of Data", "pages": "1 - 26", "volume": "1"}, "authors": [{"authorId": "2120364630", "name": "Xinyi Zhang"}, {"authorId": "2116977240", "name": "Zhuonan Chang"}, {"authorId": "2120431661", "name": "Hong Wu"}, {"authorId": "1864836046", "name": "Yang Li"}, {"authorId": "2108200071", "name": "Jia Chen"}, {"authorId": "2115409231", "name": "Jian Tan"}, {"authorId": "1442059955", "name": "Feifei Li"}, {"authorId": "2143385807", "name": "Bin Cui"}], "citations": [{"paperId": "0f119b265c39f70db0b6965ed9a68a85959e1e3f", "title": "Towards General and Efficient Online Tuning for Spark"}]}
