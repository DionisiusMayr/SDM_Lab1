{"paperId": "055c7147829bde1a56fff853d18926928772b171", "publicationVenue": {"id": "7f92b1d2-f2b3-454d-adbe-ff02c83fe404", "name": "IEEE Conference on Computer Communications", "type": "conference", "alternate_names": ["INFOCOM", "IEEE Conf Comput Commun"], "url": "http://www.ieee-infocom.org/"}, "title": "Near-Optimal Topology-adaptive Parameter Synchronization in Distributed DNN Training", "abstract": "Distributed machine learning with multiple concurrent workers has been widely adopted to train large deep neural networks (DNNs). Parameter synchronization is a key component in each iteration of distributed training, where workers exchange locally computed gradients through an AllReduce operation or parameter servers, for global parameter updates. Parameter synchronization often constitutes a significant portion of the training time; minimizing the communication time contributes substantially to DNN training speed-up. Standard ring-based AllReduce or PS architecture work efficiently mostly with homogeneous inter-worker connectivity. However, available bandwidth among workers in real-world clusters is often heterogeneous, due to different hardware configurations, switching topologies, and contention with concurrent jobs. This work investigates the best parameter synchronization topology and schedule among workers for most expedited communication in distributed DNN training. We show that the optimal parameter synchronization topology should be comprised of trees with different workers as roots, each for aggregating or broadcasting a partition of gradients/parameters. We identify near-optimal forest packing to maximally utilize available bandwidth and overlap aggregation and broadcast stages to minimize communication time. We provide theoretical analysis of the performance bound, and show that our scheme outperforms state-of-the-art parameter synchronization schemes by up to 18.3 times with extensive evaluation under various settings.", "venue": "IEEE Conference on Computer Communications", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-10", "journal": {"name": "IEEE INFOCOM 2021 - IEEE Conference on Computer Communications", "pages": "1-10"}, "authors": [{"authorId": "2117995596", "name": "Zhe Zhang"}, {"authorId": "1726963", "name": "Chuan Wu"}, {"authorId": "2118206988", "name": "Zongpeng Li"}], "citations": [{"paperId": "250dc8f7059e4a23985c06edd7b5b2ca9291a693", "title": "Topologies in distributed machine learning: Comprehensive survey, recommendations and future directions"}, {"paperId": "257d04dddf20dc4b98118d1fa61a9f92a2accd8a", "title": "Wrht: Efficient All-reduce for Distributed DNN Training in Optical Interconnect Systems"}, {"paperId": "e72eee3fe4163c7191b79079ce8e1f583f781892", "title": "Accelerating model synchronization for distributed machine learning in an optical wide area network"}, {"paperId": "0701cfcc37ea7654b06d0abd5522856b22a72743", "title": "Efficient All-Reduce for Distributed DNN Training in Optical Interconnect Systems"}, {"paperId": "d45f611ad37dddecdf8f7f4acfaaed07f3106566", "title": "Load Balancing Optimization for Transformer in Distributed Environment"}]}
