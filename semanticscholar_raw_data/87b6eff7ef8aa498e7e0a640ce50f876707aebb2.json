{"paperId": "87b6eff7ef8aa498e7e0a640ce50f876707aebb2", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "BlueFog: Make Decentralized Algorithms Practical for Optimization and Deep Learning", "abstract": "Decentralized algorithm is a form of computation that achieves a global goal through local dynamics that relies on low-cost communication between directly-connected agents. On large-scale optimization tasks involving distributed datasets, decentralized algorithms have shown strong, sometimes superior, performance over distributed algorithms with a central node. Recently, developing decentralized algorithms for deep learning has attracted great attention. They are considered as low-communication-overhead alternatives to those using a parameter server or the Ring-Allreduce protocol. However, the lack of an easy-to-use and efficient software package has kept most decentralized algorithms merely on paper. To fill the gap, we introduce BlueFog, a python library for straightforward, high-performance implementations of diverse decentralized algorithms. Based on a unified abstraction of various communication operations, BlueFog offers intuitive interfaces to implement a spectrum of decentralized algorithms, from those using a static, undirected graph for synchronous operations to those using dynamic and directed graphs for asynchronous operations. BlueFog also adopts several system-level acceleration techniques to further optimize the performance on the deep learning tasks. On mainstream DNN training tasks, BlueFog reaches a much higher throughput and achieves an overall $1.2\\times \\sim 1.8\\times$ speedup over Horovod, a state-of-the-art distributed deep learning package based on Ring-Allreduce. BlueFog is open source at https://github.com/Bluefog-Lib/bluefog.", "venue": "arXiv.org", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-11-08", "journal": {"name": "ArXiv", "volume": "abs/2111.04287"}, "authors": [{"authorId": "3156029", "name": "Bicheng Ying"}, {"authorId": "50492964", "name": "Kun Yuan"}, {"authorId": "33708840", "name": "Hanbin Hu"}, {"authorId": "2109367314", "name": "Yiming Chen"}, {"authorId": "6833606", "name": "W. Yin"}], "citations": [{"paperId": "9104a14a784cbbdc0e20eb34a31b9dfb1f17d1b3", "title": "Accelerating Gradient Tracking with Periodic Global Averaging"}, {"paperId": "19bb64946d8169ad6fd228916f71e20894d40c5d", "title": "Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity"}, {"paperId": "1e2f355efa001e54da8a884b9fe86bb4325879ec", "title": "Developing Elementary Federated Learning Algorithms Leveraging the ChatGPT"}, {"paperId": "036df2749d346178cb0e6e34b7f3355a8842dd29", "title": "A Federated Learning Algorithms Development Paradigm"}, {"paperId": "7bc4857e9256adf472a2d95e7f487a7c10631b67", "title": "Epidemic Learning: Boosting Decentralized Learning with Randomized Communication"}, {"paperId": "67e6af57493c5849d72422b73d3f938bf9a94f17", "title": "Correct orchestration of Federated Learning generic algorithms: formalisation and verification in CSP"}, {"paperId": "91350630040c35a7ae2ff1ff552d79e11cd8d23e", "title": "A2CiD2: Accelerating Asynchronous Communication in Decentralized Deep Learning"}, {"paperId": "57b3491dd7b2faacde921f5f7051c2c591682ea0", "title": "Decentralized SGD and Average-direction SAM are Asymptotically Equivalent"}, {"paperId": "a3a6c3f0bdd831c4bf3f8e8d202bfb9425273a39", "title": "DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm"}, {"paperId": "6b3efc9c78fc312da883ff07241bf8fd93576254", "title": "A Simple Python Testbed for Federated Learning Algorithms"}, {"paperId": "18f869b06fee6303ffebc56eb75b6dfbfb9a3200", "title": "Optimal Complexity in Non-Convex Decentralized Learning over Time-Varying Networks"}, {"paperId": "14062d7cbf9ce13d7c3494b4bb5eb580ee0debbd", "title": "Communication-Efficient Topologies for Decentralized Learning with O(1) Consensus Rate"}, {"paperId": "68f166d1110267bead7ff7b8eba22d6d6c0c89fa", "title": "On the Performance of Gradient Tracking with Local Updates"}, {"paperId": "5b7d648bdad5273de5b92a2619a31779b960646e", "title": "Topology-aware Generalization of Decentralized SGD"}, {"paperId": "716c9b82ce0c6eeea96d70f08da9ebf82f468a86", "title": "On the (In)security of Peer-to-Peer Decentralized Machine Learning"}, {"paperId": "3ebd5a4121c37262fa9be74c413d1ef86c172511", "title": "Heavy-Tail Phenomenon in Decentralized SGD"}, {"paperId": "4ef2e765a3d39a4d59c6f84f90981bab19aeded3", "title": "Byzantine-Robust Decentralized Learning via ClippedGossip"}, {"paperId": "f0974cca1c0989f8d3472d34681241ac7c1c4b46", "title": "Exponential Graph is Provably Efficient for Decentralized Deep Training"}, {"paperId": "efea624caa48c0f0c14b68f444f37d41f1ad32e8", "title": "Removing Data Heterogeneity Influence Enhances Network Topology Dependence of Decentralized SGD"}, {"paperId": "abdef8f87d023bc2e49b725fdc7f5d3b10681956", "title": "On the Privacy of Decentralized Machine Learning"}, {"paperId": "5273a3706a1cb40426a110b09eef7ad3ad684321", "title": "Byzantine-Robust Decentralized Learning via Self-Centered Clipping"}, {"paperId": "13b70f20c75758dfecd9f761d9104062d4b5dfb8", "title": "Removing Data Heterogeneity In\ufb02uence Enhances Network Topology Dependence of Decentralized SGD"}]}
