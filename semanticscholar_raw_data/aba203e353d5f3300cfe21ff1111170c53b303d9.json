{"paperId": "aba203e353d5f3300cfe21ff1111170c53b303d9", "publicationVenue": {"id": "ffe5bb5c-04ed-488e-985d-d3a7b39542cf", "name": "IEEE International Conference on Distributed Computing Systems", "type": "conference", "alternate_names": ["International Conference on Distributed Computing Systems", "IEEE Int Conf Distrib Comput Syst", "Int Conf Device Circuit Syst", "ICDCS", "Int Conf Distrib Comput Syst", "International Conference on Devices, Circuits and Systems"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000213/all-proceedings"}, "title": "Collage Inference: Using Coded Redundancy for Lowering Latency Variation in Distributed Image Classification Systems", "abstract": "MLaaS (ML-as-a-Service) offerings by cloud computing platforms are becoming increasingly popular. Hosting pre-trained machine learning models in the cloud enables elastic scalability as the demand grows. But providing low latency and reducing the latency variance is a key requirement. Variance is harder to control in a cloud deployment due to uncertain-ties in resource allocations across many virtual instances. We propose the collage inference technique, which uses a novel convolutional neural network model, collage-cnn, to provide low-cost redundancy. A collage-cnn model takes a collage image formed by combining multiple images and performs multi-image classification in one shot, albeit at slightly lower accuracy. We augment a collection of traditional single image classifier models with a single collage-cnn classifier, which acts as their low-cost redundant backup. Collage-cnn provides backup classification results if any single image classification requests experience a slowdown. Deploying the collage-cnn models in the cloud, we demonstrate that the 99th percentile tail latency of inference can be reduced by 1.2x to 2x compared to replication-based approaches while providing high accuracy. Variation in inference latency can be reduced by 1.8x to 15x.", "venue": "IEEE International Conference on Distributed Computing Systems", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-01", "journal": {"name": "2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)", "pages": "453-463"}, "authors": [{"authorId": "2429287", "name": "Hemanth Narra"}, {"authorId": "46268272", "name": "Zhifeng Lin"}, {"authorId": "2849491", "name": "Ganesh Ananthanarayanan"}, {"authorId": "5877233", "name": "A. Avestimehr"}, {"authorId": "145599558", "name": "M. Annavaram"}], "citations": [{"paperId": "e7a572d1a16c22addcf6c98ed88d0344de4c2f97", "title": "On expanding the toolkit of locality-based coded computation to the coordinates of inputs"}, {"paperId": "46ae9785fc280e9edce3b8b4a9ff4deaf6dbd1a3", "title": "Compression-Informed Coded Computing"}, {"paperId": "973007aef333350731bbf173e9d7abd312ab00d9", "title": "Learning-Augmented Streaming Codes are Approximately Optimal for Variable-Size Messages"}, {"paperId": "09ce40faf1b73337d83e84f8b449ba7192c6fd82", "title": "ApproxIFER: A Model-Agnostic Approach to Resilient and Robust Prediction Serving Systems"}, {"paperId": "48b002743303ecec287ee50abe5609dcfe17c05f", "title": "Adaptive Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning"}, {"paperId": "42fdd52682f0ec996b90c34ac29278dbda590459", "title": "Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning"}]}
