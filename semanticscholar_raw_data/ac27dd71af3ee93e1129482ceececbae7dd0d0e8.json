{"paperId": "ac27dd71af3ee93e1129482ceececbae7dd0d0e8", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation", "abstract": "The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as\"jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. Our code is available at https://github.com/Princeton-SysML/Jailbreak_LLM.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-11", "journal": {"name": "ArXiv", "volume": "abs/2310.06987"}, "authors": [{"authorId": "108053318", "name": "Yangsibo Huang"}, {"authorId": "2143081868", "name": "Samyak Gupta"}, {"authorId": "67284811", "name": "Mengzhou Xia"}, {"authorId": "2158261179", "name": "Kai Li"}, {"authorId": "50536468", "name": "Danqi Chen"}], "citations": [{"paperId": "edfa8625b43264a29d79580c3f88856153befeba", "title": "Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large Language Models through Logic Chain Injection"}, {"paperId": "627a5edf93091a4a50c9501c5ae5541fde393fa3", "title": "JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks"}, {"paperId": "48a26fc1ad809f42afb0786beb0dd13fac490e8f", "title": "What's in Your\"Safe\"Data?: Identifying Benign Data that Breaks Safety"}, {"paperId": "553c86ba967f55db9caffe08240061b1282da893", "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection"}, {"paperId": "33c8910107f3fcb17d140cc88554652508ae3674", "title": "Detoxifying Large Language Models via Knowledge Editing"}, {"paperId": "15029800813f06968c6596a748053cd98cb19c2d", "title": "Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal"}, {"paperId": "41a50ee713ac6dd556e63bfcf2cf74b78055f1a5", "title": "Jailbreaking is Best Solved by Definition"}, {"paperId": "2f4cc3f4a1c70cd5aca14c1304037491cd3aeb9b", "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content"}, {"paperId": "425d3c381dde8f576b6237a6d443b97880dd6bc2", "title": "Tastle: Distract Large Language Models for Automatic Jailbreak Attack"}, {"paperId": "7709a9eefa9a67510111aef2877f2834a76c8829", "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences"}, {"paperId": "0a6a350653369dc92fde4cf9992951534ed1f169", "title": "Automatic and Universal Prompt Injection Attacks against Large Language Models"}, {"paperId": "21f8977648e25ce8b95020e6f01988af99209c82", "title": "A Safe Harbor for AI Evaluation and Red Teaming"}, {"paperId": "97c17683dbd3bea6972b93f5ab8ebb208b344ee6", "title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs"}, {"paperId": "8b1378a728ac223309e5e4c6d2006654b2d469bf", "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models"}, {"paperId": "6fe7e6ce3cc0ebd038caa456d73fd7472e7d6c38", "title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment"}, {"paperId": "9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2", "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates"}, {"paperId": "db68cc363587bf82acf5a373b68bbf8a6bc11ac9", "title": "Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue"}, {"paperId": "b548a4f8ecc95c252b2e1c039926db492b09066c", "title": "PromptSet: A Programmer's Prompting Dataset"}, {"paperId": "72f51c3ef967f7905e3194296cf6fd8337b1a437", "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models"}, {"paperId": "4f1ebe6806489a86072ddcab46f05eeaa839dcf9", "title": "WIPI: A New Web Threat for LLM-Driven Web Agents"}, {"paperId": "e519699816d358783f41d4bd50fd3465d9fa51bd", "title": "Fast Adversarial Attacks on Language Models In One GPU Minute"}, {"paperId": "d0746668f85fcdbe00306bd7486a24f8750207d5", "title": "How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries"}, {"paperId": "09812e529903ff67c5fc5f1dcb2b3586eb3ffd23", "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment"}, {"paperId": "8832f073c4d5d7ae26d6b5252b00bf8d8531f2e6", "title": "LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study"}, {"paperId": "4699f20628ebb318005b3cce9005c78847ca6daf", "title": "Coercing LLMs to do and reveal (almost) anything"}, {"paperId": "4ebfb0589ba587d6912661c1fe1082db705476af", "title": "Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs"}, {"paperId": "50ceabc6aa41e08480fa5976342bfe04bb47bce3", "title": "Defending Jailbreak Prompts via In-Context Adversarial Game"}, {"paperId": "61567da465bf0121d5b02fc6894e4cca59f7b5d3", "title": "Is the System Message Really Important to Jailbreaks in Large Language Models?"}, {"paperId": "fbceb7ffd77ca45eaba297f3f421f65df4feb5cf", "title": "Query-Based Adversarial Prompt Generation"}, {"paperId": "c3f079f9f59f255e032a1239aea02a2affe93be8", "title": "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding"}, {"paperId": "a10325ad749859e5c857ecdb10f3c5a674f2e0a4", "title": "How Susceptible are Large Language Models to Ideological Manipulation?"}, {"paperId": "9f25324c89686495afb697cdb79f98d79092b843", "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages"}, {"paperId": "f2e88c26bc1ebdd4adc5f83ab56cb4276120745d", "title": "A StrongREJECT for Empty Jailbreaks"}, {"paperId": "76132c4494ef9d4792b7aeb7a868ffe45ca850a8", "title": "A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents"}, {"paperId": "9a7187386eb6ea93d41d7e2baa88accedc702fc2", "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space"}, {"paperId": "37560b8b09f7c859ba45cbadaf712f2c38727dbe", "title": "Exploring the Adversarial Capabilities of Large Language Models"}, {"paperId": "1b95053af03b5a06809a4967c6cf5ca137bbcde4", "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks"}, {"paperId": "2139e414bdf6a5ea7ec4052d3f65a8d49991494b", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"}, {"paperId": "b7ef6182f617ef3e7cc9682f562f794115a4c62c", "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability"}, {"paperId": "b0ada492ba48e85016cbbfd95ec7180fb7e79648", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast"}, {"paperId": "d0b02eb4f0d3efd884b49450efc88145bdf49abc", "title": "Rethinking Machine Unlearning for Large Language Models"}, {"paperId": "5c204b2421d05b83d3c96a6c515cc03143073935", "title": "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models"}, {"paperId": "02fe1c5eeb30cdf0a574b9c3e185f3a4b3b9d594", "title": "Comprehensive Assessment of Jailbreak Attacks Against LLMs"}, {"paperId": "f75f401f046d508753d6b207f3f19414f489bd08", "title": "Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia"}, {"paperId": "aa6a03f3368cbb4a413f7e11650fb8a6a2b71de1", "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications"}, {"paperId": "868fcbc6127e9cf79990e92116ec482051d470f3", "title": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models"}, {"paperId": "b82ccc66c14f531a444c74d2a9a9d86a86a8be99", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal"}, {"paperId": "acbce5ebf3f254188d10f6ba7de1ba716db89774", "title": "A Closer Look at the Limitations of Instruction Tuning"}, {"paperId": "e9cb528aeef1f27e9cb4463f4e0dc629b6b177c8", "title": "On Catastrophic Inheritance of Large Foundation Models"}, {"paperId": "b1f243b586e87fe12ff8fe1a501f11ea5fc5ad44", "title": "On Prompt-Driven Safeguarding for Large Language Models"}, {"paperId": "88d59e31575f5b3dd88a2c2033b55f628c2adbc9", "title": "Weak-to-Strong Jailbreaking on Large Language Models"}, {"paperId": "dd35405d8e562fa1df4338839878e9c94817cfdd", "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks"}, {"paperId": "4fda99880cdbf8f178f01eb4c8dbdae7f959ea94", "title": "Red-Teaming for Generative AI: Silver Bullet or Security Theater?"}, {"paperId": "a6a8896dea728310d1bfe829e027e10cccdf4974", "title": "Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models"}, {"paperId": "f21d0177e9374bb8579c1d9c71319f212f62b3d5", "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance"}, {"paperId": "732ce53c573475f2691a7cfc716cf4f568d17360", "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs"}, {"paperId": "0251bb95be75d472c8d5b873751615e7fe2feb1d", "title": "A Comprehensive Study of Knowledge Editing for Large Language Models"}, {"paperId": "dfd925a945303ae447d6c395d4aeaeb101d60198", "title": "LLM-based Vulnerability Detection"}, {"paperId": "600d9287efc4703bdb99ce39b5e8b37da0baa6f6", "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning"}, {"paperId": "1a5a79b393b3f00eb5a47243ee031ad799d2f641", "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"}, {"paperId": "263a58f4fd32caca1dad2351af4d711aec451fe6", "title": "Evil Geniuses: Delving into the Safety of LLM-based Agents"}, {"paperId": "54c9a97637822c9e1956b1ec70b0c9a0f2338d2c", "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking"}, {"paperId": "05d2ced6a4fb7efb8d527a228ad792526a202235", "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities"}, {"paperId": "b78b5ce5f21f46d8149824463f8eebd6103d49aa", "title": "FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"}, {"paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2", "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"}, {"paperId": "58006b293d82f141f0c44be2a5639f2f19252ffc", "title": "Improving Interpersonal Communication by Simulating Audiences with Language Models"}, {"paperId": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2", "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models"}, {"paperId": "606aec47eb05e3536d11ff7904905945009c712d", "title": "An International Consortium for Evaluations of Societal-Scale Risks from Advanced AI"}, {"paperId": "897940fb5dd4d739b88c4659c4565d05f48d06b8", "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"}, {"paperId": "4acdc5859d86c8fd1e14afa83c46f0340ba70ebb", "title": "Linguistic Obfuscation Attacks and Large Language Model Uncertainty"}, {"paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4", "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models"}]}
