{"paperId": "ff8c9f1ce9072f3cda7e87f3d6a70519e586160e", "publicationVenue": null, "title": "IMPROVING HADOOP THROUGH DATA PLACEMENT STRATEGY", "abstract": "Hadoop assumes that the computing capability of cluster's nodes is similar. In such a homogeneous environment, every node is assigned an identical load, such approach allows total use of cluster's resources and minimize multiple idle or over headed nodes. However, in real world applications, clusters are frequently deployed in a heterogeneous context [9,13\u201315]. In such environments, there is possibly multiple different physical servers and different virtual nodes specifications, which provide per consequence different services capabilities. Therefore, Hadoop still uses the same native strategy that distributes data blocks equally among each DataNode, similarly the load is evenly assigned between nodes, then the basic overall performance of Hadoop may also be reduced. The main objective during the phases of this research is to find an optimal scheme and an improved architecture to optimize the classical architecture of HADOOP and MapReduce, by focusing mainly on the algorithm of locality and data distribution in a heterogeneous ecosystem composed of several nodes heterogeneous. Despite of the native Data Placement strategy that Hadoop framework maintains by default, we present in the following a new approach that take in consideration the difference between the cluster nodes computing capabilities, with respect to the nature of tasks to adjust data blocks distribution. The design of our solution is presented under two major phases, the first one is implemented during the HDFS input, and the second one is implemented while processing tasks are initiated.", "venue": "", "year": null, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2130931816", "name": "Mohamed Eddoujaji"}, {"authorId": "2062870188", "name": "Hassan Samadi"}, {"authorId": "3044958", "name": "M. Bouhorma"}], "citations": []}
