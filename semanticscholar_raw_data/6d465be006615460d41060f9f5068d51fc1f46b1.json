{"paperId": "6d465be006615460d41060f9f5068d51fc1f46b1", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models", "abstract": "The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box methods based on prompt learning and a white-box defense method based on fine-tuning with adversarial training accordingly. Experimental results demonstrate that black-box defenses are highly effective in mitigating these attacks, while the white-box defense reduces the attack success rate to near-zero levels. Overall, our work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-12-21", "journal": {"name": "ArXiv", "volume": "abs/2312.14197"}, "authors": [{"authorId": "2087122322", "name": "Jingwei Yi"}, {"authorId": "2154871075", "name": "Yueqi Xie"}, {"authorId": "2276416062", "name": "Bin Zhu"}, {"authorId": "2276204594", "name": "Keegan Hines"}, {"authorId": "2264962872", "name": "Emre Kiciman"}, {"authorId": "2273404944", "name": "Guangzhong Sun"}, {"authorId": "2276268072", "name": "Xing Xie"}, {"authorId": "2397264", "name": "Fangzhao Wu"}], "citations": [{"paperId": "32316f1d08911b397f52f8644f46f5c129619383", "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting"}, {"paperId": "b26602dd5ec777659ce46fec86da132ac680d2ee", "title": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?"}, {"paperId": "0a6a350653369dc92fde4cf9992951534ed1f169", "title": "Automatic and Universal Prompt Injection Attacks against Large Language Models"}, {"paperId": "c8eee9766f0968e8f1b1be0731bc70b85be0ac97", "title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents"}, {"paperId": "1cef01ab4db546659f42de237a54dd510f9906cb", "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems"}, {"paperId": "7b00cb1fe1773f964d123dab6d4812c7bc63de06", "title": "Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems"}, {"paperId": "4f1ebe6806489a86072ddcab46f05eeaa839dcf9", "title": "WIPI: A New Web Threat for LLM-Driven Web Agents"}, {"paperId": "0b5a05dddfd06b27a8cdb7d65648efbcef78b017", "title": "GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis"}, {"paperId": "f5e7e22036c3fe7d6660eee90642f716c3b303f5", "title": "StruQ: Defending Against Prompt Injection with Structured Queries"}, {"paperId": "8d28d2ef602e8b518b7daecc39a0f2f8d2caaa09", "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance"}]}
