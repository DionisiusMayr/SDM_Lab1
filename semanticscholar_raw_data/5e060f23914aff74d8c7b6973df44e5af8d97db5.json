{"paperId": "5e060f23914aff74d8c7b6973df44e5af8d97db5", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis", "abstract": "Although demonstrating superb performance on various NLP tasks, large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs. To measure the level of hallucination of LLMs, previous works first categorize the hallucination according to the phenomenon similarity, then quantify the proportion that model outputs contain hallucinatory contents. However, such hallucination rates could easily be distorted by confounders. Moreover, such hallucination rates could not reflect the reasons for the hallucination, as similar hallucinatory phenomena may originate from different sources. To address these issues, we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of LLMs with a set of risk factors. In this way, we are able to observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor, meanwhile excluding the confounding effect of other factors. Additionally, by recognizing the risk factors according to a taxonomy of model capability, we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-11", "journal": {"name": "ArXiv", "volume": "abs/2309.05217"}, "authors": [{"authorId": "49134163", "name": "LI DU"}, {"authorId": "6285226", "name": "Yequan Wang"}, {"authorId": "2267725682", "name": "Xingrun Xing"}, {"authorId": "2238952996", "name": "Yiqun Ya"}, {"authorId": "48868160", "name": "Xiang Li"}, {"authorId": "145820291", "name": "Xin Jiang"}, {"authorId": "2238406105", "name": "Xuezhi Fang"}], "citations": [{"paperId": "9a741f33aa4d782639e1f81a7e9c341b58b6ed2a", "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices"}, {"paperId": "00af36ae2b615a8300348386052e38f4ddeb32e3", "title": "DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection"}, {"paperId": "5d3bdae6cfb2239af70dfe8ae5cf1a4958330ac4", "title": "A Survey of Large Language Models Attribution"}, {"paperId": "6f75e8b61f13562237851d8119cb2f9d49e073fb", "title": "Can LLM-Generated Misinformation Be Detected?"}]}
