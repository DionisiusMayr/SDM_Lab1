{"paperId": "0dab58e476f3f0e6f580a295f7c4756c86f1f198", "publicationVenue": {"id": "11e94c04-6b12-41c9-963d-1a08cdbc306d", "name": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming", "type": "conference", "alternate_names": ["PPoPP", "ACM SIGPLAN Symp Princ Pract Parallel Program", "ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming", "ACM SIGPLAN Symp Princ  Pract Parallel Program"], "url": "http://www.acm.org/sigplan/ppopp.htm"}, "title": "FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models", "abstract": "The current trend in deep learning is to scale models to extremely large sizes with the objective of increasing their accuracy. Mixture-of-Expert (MoE) is the most popular pre-trained model that makes feasible the training of models with parameters beyond trillion-scale. Thanks to the dynamic activation of experts, i.e., shallow layers specialized in certain domains, it allows for sparse training of bigger models, removing the linearity between model size and computation. However, different from traditional deep learning models, it draws huge challenges to the efficiency of these training systems, including dynamic load imbalance, inefficient synchronous execution mode, and congested all-to-all communication. To address these challenges, we first propose a performance model that can both accurately predict the latency of different operations of a specific training task, and intuitively analyze its end-to-end performance via a novel roofline-like model. Then, guided by this model, we invent a dynamic shadowing approach to cope with load imbalance, and a smart fine-grained schedule that splits different operations and executes them concurrently. We design a congestion-avoiding expert selection strategy that relieves network congestion for the lower latency of iterations, when modification of expert selection is allowed. We implement and integrate the above optimizations as a general system, FasterMoE, empowering efficient distributed MoE model training. FasterMoE is evaluated on different cluster systems using up to 64 GPUs. It achieves 1.37X - 17.87X speedup compared with state-of-the-art systems for large models, including ZeRO, GShard, and BASE Layer. Source code of FasterMoE is now available at https://github.com/thu-pacman/FasterMoE.", "venue": "ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2022-03-28", "journal": {"name": "Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming"}, "authors": [{"authorId": "151050884", "name": "Jiaao He"}, {"authorId": "2467444", "name": "Jidong Zhai"}, {"authorId": "2160556245", "name": "Tiago Antunes"}, {"authorId": "2130322731", "name": "Haojie Wang"}, {"authorId": "2160560153", "name": "Fuwen Luo"}, {"authorId": "2160570472", "name": "Shangfeng Shi"}, {"authorId": "2127426060", "name": "Qingwen Li"}], "citations": [{"paperId": "3940c3071e343e00d0a1d8c129854eee9430e3fb", "title": "Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts"}, {"paperId": "8046d516d395df434ff2b0c65c15f55e8e85dfe5", "title": "A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters"}, {"paperId": "59d2cfb02f40363fbc48c1ba4d769d0e1c0e93f2", "title": "m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers"}, {"paperId": "dab65cdf3dcbe780ca5201d4576e8429e5fa1e07", "title": "Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference"}, {"paperId": "0d558136634156da3677d89d7ad9654342334c4d", "title": "Training and Serving System of Foundation Models: A Comprehensive Survey"}, {"paperId": "25067493e2a969217e6c8b8115462184390bb2bd", "title": "Prophet: Fine-grained Load Balancing for Parallel Training of Large-scale MoE Models"}, {"paperId": "74b15e66c5f4b8d66b9613ac09b738f8165dc25e", "title": "Modularity in Deep Learning: A Survey"}, {"paperId": "feae1cd9e3fdeb77c3ff6e03831f09474a143c39", "title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy"}, {"paperId": "a32476f93be0e8707cc1b99c2f506e60d61715a4", "title": "Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models"}, {"paperId": "3ed178316be914658a80e561bf00576577f34389", "title": "Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"}, {"paperId": "2a55a14eb403128f9be2b85329202f501eb19cd8", "title": "STR: Hybrid Tensor Re-Generation to Break Memory Wall for DNN Training"}, {"paperId": "92a95c5d3ea87e08ac527d8ce25383ff8c1015be", "title": "ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer"}, {"paperId": "479f22891a50a553ae46ca32e7bc7e195d9293fc", "title": "PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining"}, {"paperId": "6e7b0c540f250bfff4c0acc33d524158296b8619", "title": "MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism"}, {"paperId": "dbbc5003af690799fa4fe6330fb795311cde106f", "title": "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement"}, {"paperId": "174ae9800f6359520d900d19890acfcf46709107", "title": "Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference"}, {"paperId": "2446074fc72ef0744153b916dc21be6fca90aad6", "title": "Elastic Averaging for Efficient Pipelined DNN Training"}, {"paperId": "a34384389f74b7b2c31c696b0db0bf813e8bb301", "title": "TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training"}, {"paperId": "c72b859ff9a349f6e0f2021a4eb9542316509949", "title": "Auto-Parallelizing Large Models with Rhino: A Systematic Approach on Production AI Platform"}, {"paperId": "43014fc85c4860487336579ec98f509fec1803f7", "title": "MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"}, {"paperId": "bd9e1f82df52010294bc404df2f38da47aa25854", "title": "Lita: Accelerating Distributed Training of Sparsely Activated Models"}, {"paperId": "c2efc25fa1a2b0192b95dc92d9dccf90e602c74c", "title": "Efficient Methods for Natural Language Processing: A Survey"}, {"paperId": "2e700ff36108119f5ed19a53bd2eaa22b42ec3d8", "title": "Tutel: Adaptive Mixture-of-Experts at Scale"}, {"paperId": "6d088e8d785a57b50c2b0e465e2460e09ced48d7", "title": "Accelerating Distributed MoE Training and Inference with Lina"}, {"paperId": "5e34bb55de5b7e56d5e41e4a582edfb695e8731b", "title": "This paper is included in the Proceedings of the 17th USENIX Symposium on Operating Systems Design and Implementation"}, {"paperId": "fb09869b0133ef6cd984e1ce9c7fa1bb6d67ab14", "title": "the 2023 USENIX"}]}
