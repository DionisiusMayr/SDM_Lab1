{"paperId": "7817d4dbe7f85f77b6ce6719ab5f6e2423e5724e", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "The Confidence-Competence Gap in Large Language Models: A Cognitive Study", "abstract": "Large Language Models (LLMs) have acquired ubiquitous attention for their performances across diverse domains. Our study here searches through LLMs' cognitive abilities and confidence dynamics. We dive deep into understanding the alignment between their self-assessed confidence and actual performance. We exploit these models with diverse sets of questionnaires and real-world scenarios and extract how LLMs exhibit confidence in their responses. Our findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly. This is reminiscent of the Dunning-Kruger effect observed in human psychology. In contrast, there are cases where models exhibit low confidence with correct answers revealing potential underestimation biases. Our results underscore the need for a deeper understanding of their cognitive processes. By examining the nuances of LLMs' self-assessment mechanism, this investigation provides noteworthy revelations that serve to advance the functionalities and broaden the potential applications of these formidable language models.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-28", "journal": {"name": "ArXiv", "volume": "abs/2309.16145"}, "authors": [{"authorId": "2248402933", "name": "Aniket Kumar Singh"}, {"authorId": "2006648414", "name": "Suman Devkota"}, {"authorId": null, "name": "Bishal Lamichhane"}, {"authorId": "2248169666", "name": "Uttam Dhakal"}, {"authorId": "2248179222", "name": "Chandra Dhakal"}], "citations": [{"paperId": "f0bd4c8d5a7e644c54bdb1324467dadd47b2bcc4", "title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, Evaluation Strategies, and Future Challenges"}, {"paperId": "812b87876b22260fe046af447c5f40a4ce2906ea", "title": "Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models"}, {"paperId": "c8918e70c2a91b09dbc86150355cb01e942ddda3", "title": "GPT-4's assessment of its performance in a USMLE-based case study"}, {"paperId": "64affc9d608066c3d0a361fb583a6252444ce564", "title": "Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation"}]}
