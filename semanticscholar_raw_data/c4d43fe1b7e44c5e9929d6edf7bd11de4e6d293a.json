{"paperId": "c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning", "abstract": "The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at \\url{https://github.com/sail-sg/sdft}.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-21", "journal": {"name": "ArXiv", "volume": "abs/2402.13669"}, "authors": [{"authorId": "2284865968", "name": "Zhaorui Yang"}, {"authorId": "2237416167", "name": "Qian Liu"}, {"authorId": "19201674", "name": "Tianyu Pang"}, {"authorId": "2285032001", "name": "Han Wang"}, {"authorId": "46854712", "name": "H. Feng"}, {"authorId": "145314938", "name": "Minfeng Zhu"}, {"authorId": "2256716159", "name": "Wei Chen"}], "citations": []}
