{"paperId": "b574245f3db22b5eb7fe64bd8b0a147dab467b60", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "RAIN: Your Language Models Can Align Themselves without Finetuning", "abstract": "Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-13", "journal": {"name": "ArXiv", "volume": "abs/2309.07124"}, "authors": [{"authorId": "2192674200", "name": "Yuhui Li"}, {"authorId": "2239197291", "name": "Fangyun Wei"}, {"authorId": "2256929424", "name": "Jinjing Zhao"}, {"authorId": "2256776221", "name": "Chao Zhang"}, {"authorId": "40975176", "name": "Hongyang Zhang"}], "citations": [{"paperId": "627a5edf93091a4a50c9501c5ae5541fde393fa3", "title": "JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks"}, {"paperId": "66e7edf09589527ebb58418632418758cee668cd", "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"}, {"paperId": "db68cc363587bf82acf5a373b68bbf8a6bc11ac9", "title": "Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue"}, {"paperId": "97c203e47b8fd987b6a2e7505669e3b9ae9a147d", "title": "LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper"}, {"paperId": "8832f073c4d5d7ae26d6b5252b00bf8d8531f2e6", "title": "LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study"}, {"paperId": "0a691e58a36cdcdaaf72294e88420f79e61e85c7", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs"}, {"paperId": "d9cfa9d7dabd39b66a8c11e5dde69ba45e91093d", "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic"}, {"paperId": "e79671a83e25288fedd897e1c9e6152f70f7f52e", "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey"}, {"paperId": "9a7187386eb6ea93d41d7e2baa88accedc702fc2", "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space"}, {"paperId": "041ab8f72343db5d50769eeb725398c689b2850c", "title": "ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization"}, {"paperId": "2139e414bdf6a5ea7ec4052d3f65a8d49991494b", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"}, {"paperId": "e9640cd4bdb0fa93a94151ec00259909b5e88d6d", "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation"}, {"paperId": "aa6a03f3368cbb4a413f7e11650fb8a6a2b71de1", "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications"}, {"paperId": "8ee0d8f6a35f66d8bb97e3388b85dba10d8d22d2", "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science"}, {"paperId": "c362015d426c90ec01e1ad02bf3fd66ab8fd0fd9", "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models"}, {"paperId": "dd35405d8e562fa1df4338839878e9c94817cfdd", "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks"}, {"paperId": "67eab08db30e397e400e3b36b3afd7526df83314", "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback"}, {"paperId": "f21d0177e9374bb8579c1d9c71319f212f62b3d5", "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance"}, {"paperId": "732ce53c573475f2691a7cfc716cf4f568d17360", "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs"}, {"paperId": "8fd29e810540c40846cddce3cbdf5060cd59fb57", "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender"}, {"paperId": "5708f725e13362da80a1062f51df118fca3529ab", "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples"}, {"paperId": "66ea57809a718f2634e4f2065569c0ba24659d44", "title": "Align on the Fly: Adapting Chatbot Behavior to Established Norms"}, {"paperId": "600d9287efc4703bdb99ce39b5e8b37da0baa6f6", "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning"}, {"paperId": "1a5a79b393b3f00eb5a47243ee031ad799d2f641", "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"}, {"paperId": "e92e8ff1becb9a9e4a7dd09878eaacb2a62ffb6b", "title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"}, {"paperId": "248c9663001cddba588709ac5fb67f2a549c01a0", "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks"}, {"paperId": "f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776", "title": "Frontier Language Models are not Robust to Adversarial Arithmetic, or \"What do I need to say so you agree 2+2=5?"}, {"paperId": "40ee4949c1050a465d418deb6dd7ea6304a3bc29", "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats"}, {"paperId": "590b826d09fb26c6495c28ba6bf6f4bed6deb68b", "title": "LUNA: A Model-Based Universal Analysis Framework for Large Language Models"}, {"paperId": "6b135e922a0c673aeb0b05c5aeecdb6c794791c6", "title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations"}, {"paperId": "84b7c486c56bd3880cb8eb01de9ae90ba3ebdaed", "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models"}, {"paperId": "764fc56883bf83392cac99a7b5a264ac9fe2cdc5", "title": "Low-Resource Languages Jailbreak GPT-4"}, {"paperId": "a5d27bf7a2155d4ca016565a78b52ee90f81624c", "title": "Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning"}, {"paperId": "34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61", "title": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked"}, {"paperId": "9cd329e3b86e6869e73a91c467459b1947655b07", "title": "Self-Prompting Large Language Models for Zero-Shot Open-Domain QA"}]}
