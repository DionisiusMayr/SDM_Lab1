{"paperId": "5b8f0460d408a8688d9ee0cba127c779d3291d99", "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods in Natural Language Processing", "type": "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "title": "Aligning Large Language Models through Synthetic Feedback", "abstract": "Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. The code is available at https://github.com/naver-ai/almost", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-05-23", "journal": {"name": "ArXiv", "volume": "abs/2305.13735"}, "authors": [{"authorId": "2829848", "name": "Sungdong Kim"}, {"authorId": "152846184", "name": "Sanghwan Bae"}, {"authorId": "51228826", "name": "Jamin Shin"}, {"authorId": "2149085001", "name": "Soyoung Kang"}, {"authorId": "10469987", "name": "Donghyun Kwak"}, {"authorId": "31760501", "name": "Kang Min Yoo"}, {"authorId": "4418074", "name": "Minjoon Seo"}], "citations": [{"paperId": "d9ef2a095d49f738cb7b538ffb86f79b31f32213", "title": "Robust Preference Optimization with Provable Noise Tolerance for LLMs"}, {"paperId": "3cb1328a9d5f62cfe87ea01ca8f471e3022efc55", "title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback"}, {"paperId": "a0748478cd2752b733b4183dbd0dcd1031c38b6e", "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback"}, {"paperId": "66e7edf09589527ebb58418632418758cee668cd", "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"}, {"paperId": "78f0f1ba187359b4c48264d3d7ee838d470d587a", "title": "Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization"}, {"paperId": "c14b58a49667fb0990759905ea3b874d50a983d1", "title": "Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "27a8f308ee592b8c664b7592b51efb655111dde9", "title": "Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search"}, {"paperId": "329a949f3d67fa345db5f7b807faef022e92d364", "title": "Preference-free Alignment Learning with Regularized Relevance Reward"}, {"paperId": "57781664a2066ca67df69ab2ae89ed4b54ea6534", "title": "West-of-N: Synthetic Preference Generation for Improved Reward Modeling"}, {"paperId": "0f9995ec08e95bea09d512c59e40d19f0f44d7bb", "title": "Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language"}, {"paperId": "715cd3244bd58ed966bf07c5a3871a46f2666cbe", "title": "Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning"}, {"paperId": "8ce31d72dcfcd888015646b15f201d49aa71c648", "title": "Unpacking the Ethical Value Alignment in Big Models"}, {"paperId": "9c0102443a1b5adc0c2235fab23a80bf8122ce72", "title": "CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment"}, {"paperId": "c7ad19da81e24c387f0377fef6d19b0fce2cf470", "title": "Self-Guard: Empower the LLM to Safeguard Itself"}, {"paperId": "9bf00afb0efb02a263fa3ddea1e768677498536c", "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"}, {"paperId": "9ebf47129c15f61f4b77bbfe305c522480c20347", "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models"}, {"paperId": "24df244bf7a6e8c93c5f183d3f62d39c0f773c68", "title": "SALMON: Self-Alignment with Principle-Following Reward Models"}, {"paperId": "749d59f887c8ac83fd4f5178465e8b03e463358c", "title": "Large Language Model Alignment: A Survey"}, {"paperId": "a0d83f9e15e722f23c14eb83cb2f87c1d1ea6400", "title": "EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria"}, {"paperId": "c12db2e67d1fb289266faa5507ff112c9a062465", "title": "Efficient RLHF: Reducing the Memory Usage of PPO"}, {"paperId": "f8b90d640158f61c4553518a8554a73b540e07e7", "title": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models"}, {"paperId": "fd712e06f87124dedc771526f797555c432cec42", "title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation"}, {"paperId": "f313a8599e6a85ec97126e9a99f4b3dc04a428a5", "title": "FootGPT : A Large Language Model Development Experiment on a Minimal Setting"}, {"paperId": "c65a64ea1be8dd654e2685f9bfea4c5118a98804", "title": "Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation"}, {"paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623", "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"}, {"paperId": "b5069352383579c6464d8e5ec34eab693c45f59a", "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets"}, {"paperId": "ca31b8584b6c022ef15ddfe994fe361e002b7729", "title": "A Comprehensive Overview of Large Language Models"}, {"paperId": "d3e1b1025e6fc73850156da60f85dde91334663e", "title": "On the Limitations of Simulating Active Learning"}]}
