{"paperId": "ce0219aab283c07369187348d72f8c5fe9898e9b", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Large Language Models are Geographically Biased", "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-05", "journal": {"name": "ArXiv", "volume": "abs/2402.02680"}, "authors": [{"authorId": "2161717022", "name": "Rohin Manvi"}, {"authorId": "2249576900", "name": "Samar Khanna"}, {"authorId": "2257185967", "name": "Marshall Burke"}, {"authorId": "2251309767", "name": "David B. Lobell"}, {"authorId": "2269095529", "name": "Stefano Ermon"}], "citations": [{"paperId": "491e435b337f122c50dba7d96a8bc909c34d0866", "title": "The Impact of Unstated Norms in Bias Analysis of Language Models"}, {"paperId": "cc72e18fa40327fa616fd348b87592bf9cc60e5b", "title": "Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore"}, {"paperId": "065ea4dc2af8660647163572f933f3257b0714a2", "title": "Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?"}, {"paperId": "586a7b66348cf33859df680ca05ce8ddf0bb13fb", "title": "Exploring Value Biases: How LLMs Deviate Towards the Ideal"}]}
