{"paperId": "72883ceab65262f1e38ab1cd4262a326425488f1", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models", "abstract": "Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which refers to maximizing a (possibly learned) reward function using policy gradient algorithms. This work identifies a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts. Overall, our results emphasize that being mindful for inputs whose expected gradient vanishes, as measured by the reward standard deviation, is crucial for successful execution of RFT.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science", "Mathematics"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-31", "journal": {"name": "ArXiv", "volume": "abs/2310.20703"}, "authors": [{"authorId": "1388726511", "name": "Noam Razin"}, {"authorId": "2261393101", "name": "Hattie Zhou"}, {"authorId": "2438203", "name": "O. Saremi"}, {"authorId": "3042871", "name": "Vimal Thilak"}, {"authorId": "2261389630", "name": "Arwen Bradley"}, {"authorId": "2181918", "name": "Preetum Nakkiran"}, {"authorId": "2243336902", "name": "Josh Susskind"}, {"authorId": "1762320", "name": "Etai Littwin"}], "citations": [{"paperId": "9732c864d1d4161fcb106f2961d9a80dd4fffc9a", "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback"}, {"paperId": "67f03ac399693393116076c0b8ec8ea05b910685", "title": "WARM: On the Benefits of Weight Averaged Reward Models"}]}
