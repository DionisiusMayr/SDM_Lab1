{"paperId": "c262df730ed671d77d3a25615965341464116eae", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Hijack Vertical Federated Learning Models with Adversarial Embedding", "abstract": "Vertical federated learning (VFL) is an emerging paradigm that enables collaborators to build machine learning models together in a distributed fashion. In general, these parties have a group of users in common but own different features. Existing VFL frameworks use cryptographic techniques to provide data privacy and security guarantees, leading to a line of works studying computing efficiency and fast implementation. However, the security of VFL\u2019s model remains underexplored. On the other hand, recent years have witnessed the explosive growth of deep neural networks (DNNs) in industry. However, a line of works has revealed they are fragile to two attack vectors, i.e., adversarial and poisoning attacks, which aim to induce the model to give expected predictions of adversaries. Following this, a natural and interesting question is how harmful are these attacks to VFL? Our pre-study finds that existing attacks suffer from high performance degradation under VFL\u2019s mechanism. One important reason is that the multiparty collaboration mechanism weakens the adversary\u2019s influence on the final prediction results. After analyzing the challenges encountered by these attacks, we propose two new attacks, i.e., replay attack and generation attack , against VFL to reveal its potential risks of being maliciously manipulated by one participant. Specifically, the former searches for benign robust features in existing samples to determine the desired class, while the latter obtains them through adversarial generation. Furthermore, to avoid these features being suppressed by the features of other parties, we take the joint force of adversarial and poisoning attacks, i.e., implanting \u2018trigger-like\u2019 features in training. Finally, the adversary combines the robust features with the implanted trigger-like features to replace a target sample\u2019s features, which results in the adversary\u2019s desired prediction. Evaluation results demonstrate the effectiveness of our attacks, e.g., the adversary holding only 10% of the features can achieve an attack success rate close to", "venue": "arXiv.org", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"name": "ArXiv", "volume": "abs/2212.00322"}, "authors": [{"authorId": "144883941", "name": "Pengyu Qiu"}, {"authorId": "2160107708", "name": "Xuhong Zhang"}, {"authorId": "2081160", "name": "S. Ji"}, {"authorId": "2145413923", "name": "Changjiang Li"}, {"authorId": "2184142286", "name": "Yuwen Pu"}, {"authorId": "2191392055", "name": "Xing Yang"}, {"authorId": "2155389584", "name": "Ting Wang"}], "citations": [{"paperId": "fdcf0bc6611364dfe304d513c606e843de579777", "title": "Multimodal Federated Learning: A Survey"}, {"paperId": "efcfea1e569daffb4ed1b13a39d80ead23eb3160", "title": "Vertical Federated Learning: Taxonomies, Threats, and Prospects"}, {"paperId": "b0692968e63b742312e6c03ccaf6abc65a0df7a6", "title": "An Embarrassingly Simple Backdoor Attack on Self-supervised Learning"}]}
