{"paperId": "0c4fe1f1a8043e8f4175b21faca1b72bff8033e6", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Enabling the Adoption of Processing-in-Memory: Challenges, Mechanisms, Future Research Directions", "abstract": "Poor DRAM technology scaling over the course of many years has caused DRAM-based main memory to increasingly become a larger system bottleneck. A major reason for the bottleneck is that data stored within DRAM must be moved across a pin-limited memory channel to the CPU before any computation can take place. This requires a high latency and energy overhead, and the data often cannot benefit from caching in the CPU, making it difficult to amortize the overhead. \nModern 3D-stacked DRAM architectures include a logic layer, where compute logic can be integrated underneath multiple layers of DRAM cell arrays within the same chip. Architects can take advantage of the logic layer to perform processing-in-memory (PIM), or near-data processing. In a PIM architecture, the logic layer within DRAM has access to the high internal bandwidth available within 3D-stacked DRAM (which is much greater than the bandwidth available between DRAM and the CPU). Thus, PIM architectures can effectively free up valuable memory channel bandwidth while reducing system energy consumption. \nA number of important issues arise when we add compute logic to DRAM. In particular, the logic does not have low-latency access to common CPU structures that are essential for modern application execution, such as the virtual memory and cache coherence mechanisms. To ease the widespread adoption of PIM, we ideally would like to maintain traditional virtual memory abstractions and the shared memory programming model. This requires efficient mechanisms that can provide logic in DRAM with access to CPU structures without having to communicate frequently with the CPU. To this end, we propose and evaluate two general-purpose solutions that minimize unnecessary off-chip communication for PIM architectures. We show that both mechanisms improve the performance and energy consumption of many important memory-intensive applications.", "venue": "arXiv.org", "year": 2018, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-02-01", "journal": {"name": "ArXiv", "volume": "abs/1802.00320"}, "authors": [{"authorId": "33801185", "name": "Saugata Ghose"}, {"authorId": "145115422", "name": "Kevin Hsieh"}, {"authorId": "2675748", "name": "Amirali Boroumand"}, {"authorId": "1999972", "name": "Rachata Ausavarungnirun"}, {"authorId": "145929920", "name": "O. Mutlu"}], "citations": [{"paperId": "7321488629d0583fc40c50178856f8e2db0cc7a8", "title": "Data Locality Aware Computation Offloading in Near Memory Processing Architecture for Big Data Applications"}, {"paperId": "766ae36a563a58d9fa2a5abd27324f673b47a327", "title": "OpenFAM: Programming disaggregated memory"}, {"paperId": "d6d4ca3a7af898732aee6ed32a26eef6f3dc04e3", "title": "ALP: Alleviating CPU-Memory Data Movement Overheads in Memory-Centric Systems"}, {"paperId": "95284b2b5fbd9d6a14edab5a1070de604637df19", "title": "SAPIVe: Simple AVX to PIM Vectorizer"}, {"paperId": "bd930010bb1d6cc99b3605c58a1464aa080c2420", "title": "On Consistency for Bulk-Bitwise Processing-in-Memory"}, {"paperId": "97aa5a45c500b54cd3f47e623d52e283c3a71e27", "title": "gem5-ndp: Near-Data Processing Architecture Simulation From Low Level Caches to DRAM"}, {"paperId": "87b07a8268cc912825a68c9778e454b2d1fb99dd", "title": "Near-memory Computing on FPGAs with 3D-stacked Memories: Applications, Architectures, and Optimizations"}, {"paperId": "2aee1899e87c3bc8cd9af32001d1550abc373551", "title": "SparseP: Efficient Sparse Matrix Vector Multiplication on Real Processing-In-Memory Architectures"}, {"paperId": "fdb71b3a8984340a10802e34ac25e27d75331e48", "title": "CompressDB: Enabling Efficient Compressed Data Direct Processing for Various Databases"}, {"paperId": "27d95b88a7c1bd45589a84118eea56d91a14afd0", "title": "Low-power Near-data Instruction Execution Leveraging Opcode-based Timing Analysis"}, {"paperId": "00c6c8b3ac452a6710af1fd33011f24da5eccce9", "title": "DR-STRaNGe: End-to-End System Design for DRAM-based True Random Number Generators"}, {"paperId": "2d96f79f33af42a75a9e6d8b6eb0dfc675b914e3", "title": "Casper: Accelerating Stencil Computations Using Near-Cache Processing"}, {"paperId": "1d5a2ced6de660d0d3ca6f037610676d1bfddb58", "title": "Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine Learning Inference Bottlenecks"}, {"paperId": "d65bf0acce7f2f3435866da21d92ba573697cd9d", "title": "FlexPushdownDB: Hybrid Pushdown and Caching in a Cloud DBMS"}, {"paperId": "77c2faaf3f170822c3e37c4ecb4c8f13894b27ab", "title": "CODIC: A Low-Cost Substrate for Enabling Custom In-DRAM Functionalities and Optimizations"}, {"paperId": "d441e66baa101273fc5323a1fb05f5141f5a8a51", "title": "QUAC-TRNG: High-Throughput True Random Number Generation Using Quadruple Row Activation in Commodity DRAM Chips"}, {"paperId": "8e87c31c9174c254b824910e28b5bf66f274650e", "title": "Benchmarking a New Paradigm: An Experimental Analysis of a Real Processing-in-Memory Architecture"}, {"paperId": "8e85281351edfd13081f1175fb102836ccc4baca", "title": "DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks"}, {"paperId": "f57c030777ae6722278ddc380314e241e84545eb", "title": "pLUTo: In-DRAM Lookup Tables to Enable Massively Parallel General-Purpose Computation"}, {"paperId": "7a210280d4775b1dfb36401a1007ac57602ab1e5", "title": "pLUTo: Enabling Massively Parallel Computation in DRAM via Lookup Tables"}, {"paperId": "af7f0dc80d37e5ebdcea3213c0966b9ecf8549a4", "title": "A Compute Cache System for Signal Processing Applications"}, {"paperId": "075acc89d8a3b4d8c001f0dbbde7ad11172febab", "title": "Enabling fast and energy-efficient FM-index exact matching using processing-near-memory"}, {"paperId": "7bb6015f45457fdbe3bb39e17dc9cb0786ecd688", "title": "A Modern Primer on Processing in Memory"}, {"paperId": "3c5069333a94c07fc1930c9f3e1d0bf5a01a8901", "title": "Low power general purpose loop acceleration for NDP applications"}, {"paperId": "f90f526b101cb8a0260f5165a3875928c58ae48a", "title": "NATSA: A Near-Data Processing Accelerator for Time Series Analysis"}, {"paperId": "166371897b69779201a9b4e12cd1cf379238d9dc", "title": "Key issues in exascale computing"}, {"paperId": "563085fdeda1025d228928ea5c3c286993073f62", "title": "Towards a portable hierarchical view of distributed shared memory systems: challenges and solutions"}, {"paperId": "687202004831cab67a2ba2c0438f737499b77541", "title": "PushdownDB: Accelerating a DBMS Using S3 Computation"}, {"paperId": "1cc473ad0210cd818612a8b502e581f8d2a0bdd5", "title": "Coherency overhead of Processing-in-Memory in the presence of shared data"}, {"paperId": "98ad59ec94125c80289056f6c1f5b59c10d8c887", "title": "Enhancing Programmability, Portability, and Performance with Rich Cross-Layer Abstractions"}, {"paperId": "03514b508a8b4c32d3d02b9f2dc7256420cf1a80", "title": "Machine Learning Systems for Highly-Distributed and Rapidly-Growing Data"}, {"paperId": "ed283e07932bd8e40a9eb7a3c2b183f318445ac0", "title": "Processing-in-memory: A workload-driven perspective"}, {"paperId": "07fa2422258c602c1b06e86f86d0315e933a4ebf", "title": "Near-Memory Computing: Past, Present, and Future"}, {"paperId": "0a934d8cd093763be705346e1cabbdc22753704f", "title": "An Ultra-Area-Efficient 1024-Point In-Memory FFT Processor"}, {"paperId": "49532cb16924c06454301f989e2c37c9accc6cd3", "title": "A Workload and Programming Ease Driven Perspective of Processing-in-Memory"}, {"paperId": "1928b6fcc16cb7af51330643c940ac1120d02c24", "title": "Accelerating time series motif discovery in the Intel Xeon Phi KNL processor"}, {"paperId": "3db4177910977a5c496ce67fa4ffb6ef61e7aefd", "title": "A neuromorphic boost to RNNs using low pass filters"}, {"paperId": "13fa796b1595d6b2282a13e040516f336ecc4f74", "title": "Mapping high-performance RNNs to in-memory neuromorphic chips"}, {"paperId": "8758325f78b05ba600e8adb597b13fb5c15e2f17", "title": "BLADE: A BitLine Accelerator for Devices on the Edge"}, {"paperId": "c7c1069796612559bfe572d03b7ffdc68674ed3f", "title": "Moving Processing to Data: On the Influence of Processing in Memory on Data Management"}, {"paperId": "6d0d20d8a38c23ec7963151534617a3624e41f38", "title": "Low-Energy Acceleration of Binarized Convolutional Neural Networks Using a Spin Hall Effect Based Logic-in-Memory Architecture"}, {"paperId": "066d4969831b1c568b893fab45775397fc9390d0", "title": "INVITED: Enabling Practical Processing in and near Memory for Data-Intensive Computing"}, {"paperId": "4f17bd15a6f86730ac2207167ccf36ec9e6c2391", "title": "Processing Data Where It Makes Sense: Enabling In-Memory Computation"}, {"paperId": "5dde03da15de3dece649593193b8ff44a6d10423", "title": "Towards Energy Efficient non-von Neumann Architectures for Deep Learning"}, {"paperId": "337e89c579c0f49dfcb5588942dcd1b83362dce2", "title": "EFFICIENT SECURITY IN EMERGING MEMORIES"}, {"paperId": "5535fec7822abc2edc8e4a45df5432426b45e138", "title": "D-RaNGe: Using Commodity DRAM Devices to Generate True Random Numbers with Low Latency and High Throughput"}, {"paperId": "b5ce5ae81038531ec5ea3ce9df80493b5231f56f", "title": "A Review of Near-Memory Computing Architectures: Opportunities and Challenges"}, {"paperId": "869ffc81aad12cc59e87ed658ea6489849543fad", "title": "Google Workloads for Consumer Devices: Mitigating Data Movement Bottlenecks"}, {"paperId": "ecf5efd5fe18860b42a1abd198e94a868dbf944c", "title": "Zorua: Enhancing Programming Ease, Portability, and Performance in GPUs by Decoupling Programming Models from Resource Management"}, {"paperId": "eb3bb240125b81064428e0fa1849d6b05242ee1e", "title": "NDPmulator: Enabling Full-System Simulation for Near-Data Accelerators From Caches to DRAM"}, {"paperId": "084809bcb66911c8cef94879311beb96d11bc6a6", "title": "Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System"}, {"paperId": "d6fab34242c701acfe042f74ac86ca0d7bc00a73", "title": "NodeFetch: High Performance Graph Processing using Processing in Memory"}, {"paperId": "2604382f4a87b4b94fcac90ce33929c2d37e2104", "title": "Near-Memory/In-Memory Computing: Pillars and Ladders"}, {"paperId": "c3931d10640213621d5fa1f6163d39d776c80ef0", "title": "Exploring the Ef\ufb01ciency of Bioinformatics Applications with CPUs, Integrated GPUs and Processing-in-Memory"}]}
