{"paperId": "08de6aedc612e9956ad33ee48481d2ffff1e59f1", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Efficient Data-Plane Memory Scheduling for In-Network Aggregation", "abstract": "As the scale of distributed training grows, communication becomes a bottleneck. To accelerate the communication, recent works introduce In-Network Aggregation (INA), which moves the gradients summation into network middle-boxes, e.g., programmable switches to reduce the traffic volume. However, switch memory is scarce compared to the volume of gradients transmitted in distributed training. Although literature applies methods like pool-based streaming or dynamic sharing to tackle the mismatch, switch memory is still a potential performance bottleneck. Furthermore, we observe the under-utilization of switch memory due to the synchronization requirement for aggregator deallocation in recent works. To improve the switch memory utilization, we propose ESA, an $\\underline{E}$fficient Switch Memory $\\underline{S}$cheduler for In-Network $\\underline{A}$ggregation. At its cores, ESA enforces the preemptive aggregator allocation primitive and introduces priority scheduling at the data-plane, which improves the switch memory utilization and average job completion time (JCT). Experiments show that ESA can improve the average JCT by up to $1.35\\times$.", "venue": "arXiv.org", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-01-17", "journal": {"name": "ArXiv", "volume": "abs/2201.06398"}, "authors": [{"authorId": null, "name": "Hao Wang"}, {"authorId": "2107504584", "name": "Y. Qin"}, {"authorId": "2446719", "name": "Chon-In Lao"}, {"authorId": "2738554", "name": "Yanfang Le"}, {"authorId": "1752668", "name": "Wenfei Wu"}, {"authorId": "2118438639", "name": "Kai Chen"}], "citations": [{"paperId": "2b69d57d04ae5f528c04d737882c41607e1f6b24", "title": "Identifying Performance Bottleneck in Shared In-Network Aggregation during Distributed Training"}, {"paperId": "421ee1695343f0dac697f3bfd25f74248c15afa7", "title": "Multi-Switch Cooperative In-Network Aggregation for Distributed Deep Learning"}, {"paperId": "3558599bd03e4b5c6ea238c5a56b9f2ddadc65cb", "title": "PA-ATP: Progress-Aware Transmission Protocol for In-Network Aggregation"}, {"paperId": "d8df642bc998760779c6b127f97634de18c570c5", "title": "Traffic-Aware In-Network Aggregation Placement for Multi-Tenant Distributed Machine Learning"}, {"paperId": "64e4aff81788ca0b31d94a7d592a1870ffa2389d", "title": "GOAT: Gradient Scheduling with Collaborative In-Network Aggregation for Distributed Training"}, {"paperId": "8a040d0d99fa248c415f0aa90a083123320bf21a", "title": "Offloading Machine Learning to Programmable Data Planes: A Systematic Survey"}, {"paperId": "6e307d4955f004972e5d24bf39891fd554dacf44", "title": "Serene: Handling the Effects of Stragglers in In-Network Machine Learning Aggregation"}]}
