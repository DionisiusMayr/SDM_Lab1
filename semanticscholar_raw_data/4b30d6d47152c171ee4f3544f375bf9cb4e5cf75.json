{"paperId": "4b30d6d47152c171ee4f3544f375bf9cb4e5cf75", "publicationVenue": {"id": "9448f839-459b-45f3-8573-5eff3f032334", "name": "USENIX Annual Technical Conference", "type": "conference", "alternate_names": ["USENIX Annu Tech Conf", "USENIX", "USENIX ATC"], "url": "https://www.usenix.org/conferences/byname/131"}, "title": "Memory Harvesting in Multi-GPU Systems with Hierarchical Unified Virtual Memory", "abstract": "With the ever-growing demands for GPUs, most organizations allow users to share the multi-GPU servers. However, we observe that the memory space across GPUs is not effectively utilized enough when consolidating various workloads that exhibit highly varying resource demands. This is because the current memory management techniques were designed solely for individual GPUs rather than shared multi-GPU environments. This study introduces a novel approach to provide an illusion of virtual memory space for GPUs, called hierarchical unified virtual memory (HUVM), by incorporating the temporarily idle memory of neighbor GPUs. Since modern GPUs are connected to each other through a fast interconnect, it provides lower access latency to neighbor GPU\u2019s memory compared to the host memory via PCIe. On top of HUVM, we design a new memory manager, called memHarvester, to effectively and efficiently harvest the temporarily available neighbor GPUs\u2019 memory. For diverse consolidation scenarios with DNN training and graph analytics workloads, our experimental result shows up to 2.71 \u00d7 performance improvement compared to the prior approach in multi-GPU environments.", "venue": "USENIX Annual Technical Conference", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "625-638"}, "authors": [{"authorId": "2108734546", "name": "Sang-Jun Choi"}, {"authorId": "2509132", "name": "Taeksoo Kim"}, {"authorId": "1683954", "name": "Jinwoo Jeong"}, {"authorId": "1999972", "name": "Rachata Ausavarungnirun"}, {"authorId": "2421588", "name": "Myeongjae Jeon"}, {"authorId": "1716807", "name": "Youngjin Kwon"}, {"authorId": "3037858", "name": "Jeongseob Ahn"}], "citations": [{"paperId": "b6476f998c29577358942be9b25eb904632075ea", "title": "ATOM: Asynchronous Training of Massive Models for Deep Learning in a Decentralized Environment"}, {"paperId": "9f5b740169b1557852e945ba745a7a2c05c56dcc", "title": "Stateful Large Language Model Serving with Pensieve"}, {"paperId": "5498977871f3cd6bb1488687f663b08a46af1a9e", "title": "FaaSwap: SLO-Aware, GPU-Efficient Serverless Inference via Model Swapping"}, {"paperId": "0c85e45b254c912fd573eb81a6a03fdda3d72277", "title": "Early-Adaptor: An Adaptive Framework forProactive UVM Memory Management"}, {"paperId": "bda02351d387bc63265f9e5671a69be8d386272c", "title": "Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization"}, {"paperId": "77706c91f7116965951fff98ce1c8abacc64f817", "title": "Spy in the GPU-box: Covert and Side Channel Attacks on Multi-GPU Systems"}]}
