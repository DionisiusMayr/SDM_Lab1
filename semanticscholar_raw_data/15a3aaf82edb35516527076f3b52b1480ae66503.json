{"paperId": "15a3aaf82edb35516527076f3b52b1480ae66503", "publicationVenue": {"id": "7c9d091e-015e-4e5d-a11f-9bc369fcf414", "name": "IEEE Transactions on Parallel and Distributed Systems", "type": "journal", "alternate_names": ["IEEE Trans Parallel Distrib Syst"], "issn": "1045-9219", "url": "http://www.computer.org/tpds", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=71"]}, "title": "HashCache: Accelerating Serverless Computing by Skipping Duplicated Function Execution", "abstract": "Serverless computing is a leading force behind deploying and managing software in cloud computing. One inherent challenge in serverless computing is the increased overall latency due to duplicate computations. Our initial investigation into the function invocations of serverless applications reveals an abundance of duplicate invocations. Inspired by this critical observation, we introduce HashCache, a system designed to cache duplicate function invocations, thereby mitigating duplicate computations. In HashCache, serverless functions are classified into three categories, namely, computational functions, stateful functions, and environment-related functions. On the grounds of such a function classification, HashCache associates the stateful functions and their states to build an adaptive synchronization mechanism. With this support, HashCache exploits the cached results of computational and stateful functions to serve upcoming invocation requests to the same functions, thereby reducing duplicate computations. Moreover, HashCache stores remote files probed by stateful functions into a local cache layer, which further curtails invocation latency. We implement HashCache within the Apache OpenWhisk to forge a cache-enabled serverless computing platform. We conduct extensive experiments to quantitatively evaluate the performance of HashCache in terms of invocation latency and resource utilization. We compare HashCache against two state-of-the-art approaches - FaaSCache and OpenWhisk. The experimental results unveil that our HashCache remarkably reduces invocation latency and resource overhead. More specifically, HashCache curbs the 99-tail latency of FaaSCache and OpenWhisk by up to 91.37% and 95.96% in real-world serverless applications. HashCache also slashes the resource utilization of FaaSCache and OpenWhisk by up to 31.62% and 35.51%, respectively.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-12-01", "journal": {"name": "IEEE Transactions on Parallel and Distributed Systems", "pages": "3192-3206", "volume": "34"}, "authors": [{"authorId": "2146045399", "name": "Zhaorui Wu"}, {"authorId": "2171195413", "name": "Yuhui Deng"}, {"authorId": "2044335124", "name": "Yi Zhou"}, {"authorId": "2190044417", "name": "Lin Cui"}, {"authorId": "2153438883", "name": "Xiao Qin"}], "citations": []}
