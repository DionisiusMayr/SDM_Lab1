{"paperId": "00309b5b1fa392c3eead280c4483ff9d7dda7616", "publicationVenue": {"id": "34342fcb-3fe0-45c6-a017-6e65b73d030f", "name": "IEEE International Conference on e-Science", "type": "conference", "alternate_names": ["e-Science", "Int Conf e-science", "IEEE Int Conf e-science", "E-Science", "International Conference on e-Science"], "url": "https://escience-conference.org/"}, "title": "FLoX: Federated Learning with FaaS at the Edge", "abstract": "Federated learning (FL) is a technique for distributed machine learning that enables the use of siloed and distributed data. With FL, individual machine learning models are trained separately and then only model parameters (e.g., weights in a neural network) are shared and aggregated to create a global model, allowing data to remain in its original environment. While many applications can benefit from FL, existing frameworks are incomplete, cumbersome, and environment-dependent. To address these issues, we present FLoX, an FL framework built on the funcX federated serverless computing platform. FLoX decouples FL model training/inference from infrastructure management and thus enables users to easily deploy FL models on one or more remote computers with a single line of Python code. We evaluate FLoX using three benchmark datasets deployed on ten heterogeneous and distributed compute endpoints. We show that FLoX incurs minimal overhead, especially with respect to the large communication overheads between endpoints for data transfer. We show how balancing the number of samples and epochs with respect to the capacities of participating endpoints can significantly reduce training time with minimal reduction in accuracy. Finally, we show that global models consistently outperform any single model on average by 8%.", "venue": "IEEE International Conference on e-Science", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-10-01", "journal": {"name": "2022 IEEE 18th International Conference on e-Science (e-Science)", "pages": "11-20"}, "authors": [{"authorId": "2196954314", "name": "Nikita Kotsehub"}, {"authorId": "46202504", "name": "Matt Baughman"}, {"authorId": "36319017", "name": "Ryan Chard"}, {"authorId": "48808283", "name": "Nathaniel Hudson"}, {"authorId": "151473641", "name": "Panos Patros"}, {"authorId": "144024945", "name": "Omer F. Rana"}, {"authorId": "2065944820", "name": "I. Foster"}, {"authorId": "3091414", "name": "K. Chard"}], "citations": [{"paperId": "115efb6260148f95ac6b93438bd18cb4adfdc77d", "title": "Modern Computing: Vision and Challenges"}, {"paperId": "453d85311169486ad77980db3466677f2b627b9e", "title": "Tournament-Based Pretraining to Accelerate Federated Learning"}, {"paperId": "17d2696c60b1df14354d7914595a6401aa2edea1", "title": "Lazy Python Dependency Management in Large-Scale Systems"}, {"paperId": "640bbcb60145dad6ef36817e01c3fcfcf332f9e8", "title": "Accelerating Communications in Federated Applications with Transparent Object Proxies"}, {"paperId": "6a4ec0780a0b7468d42efc8f9e5818e58ede71bb", "title": "Balancing Federated Learning Trade-Offs for Heterogeneous Environments"}, {"paperId": "5cee03d74fec943533c3ad7a6eac4010364312b8", "title": "Hierarchical and Decentralised Federated Learning"}, {"paperId": "ede11eabf280ddf23c9488b007eef868a936c6ce", "title": "Exploring Tradeoffs in Federated Learning on Serverless Computing Architectures"}, {"paperId": "b02364ab877be7f412027c769eb368a8c4c10d35", "title": "Telematics and Informatics Reports"}]}
