{"paperId": "4e97b8bf4ecaacb6b8524b21b0b215c8ded1d6fe", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue", "abstract": "The advent of large language models (LLMs) has made it possible to generate natural written dialogues between two agents. However, generating human-like spoken dialogues from these written dialogues remains challenging. Spoken dialogues have several unique characteristics: they frequently include backchannels and laughter, and the smoothness of turn-taking significantly influences the fluidity of conversation. This study proposes CHATS - CHatty Agents Text-to-Speech - a discrete token-based system designed to generate spoken dialogues based on written dialogues. Our system can generate speech for both the speaker side and the listener side simultaneously, using only the transcription from the speaker side, which eliminates the need for transcriptions of backchannels or laughter. Moreover, CHATS facilitates natural turn-taking; it determines the appropriate duration of silence after each utterance in the absence of overlap, and it initiates the generation of overlapping speech based on the phoneme sequence of the next utterance in case of overlap. Experimental evaluations indicate that CHATS outperforms the text-to-speech baseline, producing spoken dialogues that are more interactive and fluid while retaining clarity and intelligibility.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science", "Engineering"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-02", "journal": {"name": "ArXiv", "volume": "abs/2310.01088"}, "authors": [{"authorId": "2059317757", "name": "Kentaro Mitsui"}, {"authorId": "30775505", "name": "Yukiya Hono"}, {"authorId": "2253396533", "name": "Kei Sawada"}], "citations": [{"paperId": "e1ffbf6e1b93d5ac3466d6d7ef3638a399083f05", "title": "Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations"}, {"paperId": "f04e58782380383f4edaaa899fd85bfb3d20c2e1", "title": "SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems"}]}
