{"paperId": "a1e8a8842888c7cffecce53a87a800729e90c36d", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "R-Tuning: Teaching Large Language Models to Refuse Unknown Questions", "abstract": "Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the knowledge gap between parametric knowledge and the instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate this new instruction tuning approach effectively improves a model's ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty during training displays a better ability to estimate uncertainty than uncertainty-based testing. Our code will be released at https://github.com/shizhediao/R-Tuning.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-16", "journal": {"name": "ArXiv", "volume": "abs/2311.09677"}, "authors": [{"authorId": "2267057406", "name": "Hanning Zhang"}, {"authorId": "50826757", "name": "Shizhe Diao"}, {"authorId": "2238123947", "name": "Yong Lin"}, {"authorId": "51135899", "name": "Y. Fung"}, {"authorId": "2237987004", "name": "Qing Lian"}, {"authorId": "2144803999", "name": "Xingyao Wang"}, {"authorId": "123331686", "name": "Yangyi Chen"}, {"authorId": "2243197103", "name": "Heng Ji"}, {"authorId": "2266465257", "name": "Tong Zhang"}], "citations": [{"paperId": "0eb8d2e9cdee108b500726d18eaa59fc48ec959b", "title": "From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models"}, {"paperId": "4e274196324bad3e750e83235516af06733e701d", "title": "Unfamiliar Finetuning Examples Control How Language Models Hallucinate"}, {"paperId": "6fe7e6ce3cc0ebd038caa456d73fd7472e7d6c38", "title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment"}, {"paperId": "3c585441b4607b34f8bf4e352ed6e36753fe21ce", "title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap"}, {"paperId": "3bdd3d56ef9054aba47f83879b531a4842640295", "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning"}, {"paperId": "f7d8fa345e4ff788707eb132a4a30c830e3f648d", "title": "Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions"}, {"paperId": "00134f96e2188eff33516ec1d2ddd998d48d2b23", "title": "Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A"}, {"paperId": "6e6fa087861921b5209ec2d958e2140288e380c1", "title": "Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable"}, {"paperId": "4315094bec44f310d662b349aff6305b3dde9a07", "title": "EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries"}, {"paperId": "00af0a5eed378f6d98c178cd5fc5ead0facfeed5", "title": "L*LM: Learning Automata from Examples using Natural Language Oracles"}, {"paperId": "a890ddb2693570272da80a126dd194967533c524", "title": "EntGPT: Linking Generative Large Language Models with Knowledge Bases"}, {"paperId": "2fb593ca4b6d2631832d6424e238c32db3db5434", "title": "Factuality of Large Language Models in the Year 2024"}, {"paperId": "25cee84e3a1541697a7c97443d7526574127c344", "title": "Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration"}, {"paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f", "title": "Knowledge Verification to Nip Hallucination in the Bud"}, {"paperId": "221a0e0c798d4bbc8a057d869b7251e03f1b0790", "title": "ChatQA: Building GPT-4 Level Conversational QA Models"}, {"paperId": "a9f29902ad22356fe4e77cb10e3ea30432a0ce70", "title": "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers"}, {"paperId": "5272acad9e4201e93dabe3fd99bd7ead9b1a544d", "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models"}, {"paperId": "cfce709a65f90312d2bdc1a6cf0380c19becf694", "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models"}, {"paperId": "1834f8126e97057e321149b50e342754a096d14d", "title": "Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge"}, {"paperId": "23e5395b4c6249c2873666bff73e203821c14719", "title": "Instruction Tuning with Human Curriculum"}]}
