{"paperId": "cf58cbdaf475109da7c528e6d5d390ed97fba6b2", "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods in Natural Language Processing", "type": "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "title": "Multi-Modal Open-Domain Dialogue", "abstract": "Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to human preference.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-10-02", "journal": {"pages": "4863-4883"}, "authors": [{"authorId": "35752280", "name": "Kurt Shuster"}, {"authorId": "51324296", "name": "Eric Michael Smith"}, {"authorId": "3092435", "name": "Da Ju"}, {"authorId": "145183709", "name": "J. Weston"}], "citations": [{"paperId": "3cea0fe210566d9bbe36a11c5524512122f0e450", "title": "Personality prediction from task-oriented and open-domain human\u2013machine dialogues"}, {"paperId": "964dfafb4efadfd2652c30f68a997f2edb35ebbd", "title": "DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever"}, {"paperId": "bb3b8b31e80cd1b306b15eb54fceac09af2e4a61", "title": "Beyond Words: An Intelligent Human-Machine Dialogue System with Multimodal Generation and Emotional Comprehension"}, {"paperId": "c4ef333deeeee9cc8cbf16a9e76bc4a60a43ae03", "title": "Post Turing: Mapping the landscape of LLM Evaluation"}, {"paperId": "b1d2a29860e69c6ce9987ddefbe112feb1efa16a", "title": "Large Language Models can Share Images, Too!"}, {"paperId": "b60a8ddc0bb2ae2dc03a297852266917ecec3579", "title": "Real-time Emotion Pre-Recognition in Conversations with Contrastive Multi-modal Dialogue Pre-training"}, {"paperId": "bee14772bc2101011a74db0ce5b8444a1af54a6a", "title": "Personality-aware Natural Language Generation for Task-oriented Dialogue using Reinforcement Learning"}, {"paperId": "0983883619a0ca597d055d0e58da2f514052913d", "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"}, {"paperId": "d7fb8ecc2cf19687d5a56b93c19e0669961e7791", "title": "PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts"}, {"paperId": "c22a8e36b7ffa69da0d70c9db58c78252567400a", "title": "ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue"}, {"paperId": "cb861bd77070e4441d66ffee0801f7048a9eacbe", "title": "i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data"}, {"paperId": "974a619c37a1015bb755eebce7ed5ca86c860a14", "title": "Synthetic Cross-language Information Retrieval Training Data"}, {"paperId": "8c236be5cb8073cb3db317919ceb55130ab66dbe", "title": "Champagne: Learning Real-world Conversation from Large-Scale Web Videos"}, {"paperId": "d4e3568b5ec91f751863130cba125c5201780594", "title": "Natural conversations with a virtual being: How user experience with a current conversational AI model compares to expectations"}, {"paperId": "7fabf538d7d61038dafc6aeb844ba1aa06cf7b6b", "title": "TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World"}, {"paperId": "a85ae093657f62bf13bf18b0652402026dd4186d", "title": "DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset"}, {"paperId": "48cf122e20f1ae75f5f4aec1280ed30a3e6e9576", "title": "Multimodal Dialogue Generation Based on Transformer and Collaborative Attention"}, {"paperId": "36342da8ef61828be9c756f225e13c21616e41d3", "title": "VILT: Video Instructions Linking for Complex Tasks"}, {"paperId": "44e08d3ba95032ea9a2b185e15bd34740093f579", "title": "TaskMAD: A Platform for Multimodal Task-Centric Knowledge-Grounded Conversational Experimentation"}, {"paperId": "7ef43bacd43393ff116e6fcda6a52a6902e016d7", "title": "\u201cI\u2019m sorry to hear that\u201d: Finding New Biases in Language Models with a Holistic Descriptor Dataset"}, {"paperId": "4ae200e3e33045130f7abd1d38a82a8355dc6273", "title": "PANGUBOT: Efficient Generative Dialogue Pre-training from Pre-trained Language Model"}, {"paperId": "6b9a5f63ef4c443eb8402f05052ae6a20f7f6ede", "title": "Learning towards conversational AI: A survey"}, {"paperId": "073e8e9c37cb5c13ed6586e182a5455bdad8c33f", "title": "ViCA: Combining visual, social, and task-oriented conversational AI in a healthcare setting"}, {"paperId": "30873c32db5a219a58be928d5692cce48be1d3a0", "title": "Few-Shot Bot: Prompt-Based Learning for Dialogue Systems"}, {"paperId": "450adc746ec2f2df5201c96c74861b4a44bf650a", "title": "MMChat: Multi-Modal Chat Dataset on Social Media"}, {"paperId": "8f31038de5cadc3171735c0410511c044d216463", "title": "Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images"}, {"paperId": "f0bcc910c7510e0b09801c880445e37ea520c22d", "title": "Modeling Text-visual Mutual Dependency for Multi-modal Dialog Generation"}, {"paperId": "830dfc2bb6bfafa9609893227d519ac3c22b5fab", "title": "How do people talk about images? A study on open-domain conversations with images."}, {"paperId": "ab06a5c808bfd10680057b8b9899e669bcbc3e51", "title": "\"I'm sorry to hear that\": finding bias in language models with a holistic descriptor dataset"}, {"paperId": "eb1ac44bbc0fe07c5f31f459c7199211239e90b8", "title": "Open-domain Dialogue Generation: What We Can Do, Cannot Do, And Should Do Next"}, {"paperId": "2223e80b00eadf1ec6b4ea62fda2c2eeae3bbcbf", "title": "How do people talk about images? A study on open-domain conversation on images."}]}
