{"paperId": "d3e9f636727b0b8f47a968c2b90435fe1c5aa9b2", "publicationVenue": {"id": "0be0e7dd-a7b1-46dc-abb3-5226ed0e2289", "name": "IEEE International Conference on Cluster Computing", "type": "conference", "alternate_names": ["Int Conf Clust Comput", "IEEE Int Conf Clust Comput", "International Conference on Cluster Computing", "CLUSTER"], "url": "http://www.clustercomp.org/"}, "title": "Efficient Execution of Dynamic Programming Algorithms on Apache Spark", "abstract": "One of the most important properties of distributed computing systems (e.g., Apache Spark, Apache Hadoop, etc) on clusters and computation clouds is the ability to scale out by adding more compute nodes to the cluster. This important feature can lead to performance gain provided the computation (or the algorithm) itself can scale out. In other words, the computation (or the algorithm) should be easily decomposable into smaller units of work to be distributed among the workers based on the hardware/software configuration of the cluster or the cloud. Additionally, on such clusters, there is an important trade-off between communication cost, parallelism, and memory requirement. Due to the scalability need as well as this trade-off, it is crucial to have a well-decomposable, adaptive, tunable, and scalable program. Tunability enables the programmer to find an optimal point in the trade-off spectrum to execute the program efficiently on a specific cluster. We design and implement well-decomposable and tunable dynamic programming algorithms from the Gaussian Elimination Paradigm (GEP), such as Floyd-Warshall's all-pairs shortest path and Gaussian elimination without pivoting, for execution on Apache Spark. Our implementations are based on parametric multi-way recursive divide-&-conquer algorithms. We explain how to map implementations of those grid-based parallel algorithms to the Spark framework. Finally, we provide experimental results illustrating the performance, scalability, and portability of our Spark programs. We show that offloading the computation to an OpenMP environment (by running parallel recursive kernels) within Spark is at least partially responsible for a $2-5\\times$ speedup of the DP benchmarks.", "venue": "IEEE International Conference on Cluster Computing", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-01", "journal": {"name": "2020 IEEE International Conference on Cluster Computing (CLUSTER)", "pages": "337-348"}, "authors": [{"authorId": "2945230", "name": "M. Javanmard"}, {"authorId": "2074027510", "name": "Zafar Ahmad"}, {"authorId": "2738904", "name": "J. Zola"}, {"authorId": "1793611", "name": "L. Pouchet"}, {"authorId": "7536938", "name": "R. Chowdhury"}, {"authorId": "145253928", "name": "R. Harrison"}], "citations": [{"paperId": "8875e7ffb03955278b2dcafae7458a6c440d6f1d", "title": "Understanding Recursive Divide-and-Conquer Dynamic Programs in Fork-Join and Data-Flow Execution Models"}]}
