{"paperId": "0dda8952b94af0cf52515d89ebeda1d6644c86c6", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "A Resource Model For Neural Scaling Law", "abstract": "Neural scaling laws characterize how model performance improves as the model size scales up. Inspired by empirical observations, we introduce a resource model of neural scaling. A task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks). On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons. (2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants. We hypothesize these findings to be generally true and build a model to predict neural scaling laws for general composite tasks, which successfully replicates the neural scaling law of Chinchilla models reported in arXiv:2203.15556. We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural networks.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-07", "journal": {"name": "ArXiv", "volume": "abs/2402.05164"}, "authors": [{"authorId": "2283449829", "name": "Jinyeop Song"}, {"authorId": "2145253202", "name": "Ziming Liu"}, {"authorId": "2253461463", "name": "Max Tegmark"}, {"authorId": "2283307022", "name": "Jeff Gore"}], "citations": []}
