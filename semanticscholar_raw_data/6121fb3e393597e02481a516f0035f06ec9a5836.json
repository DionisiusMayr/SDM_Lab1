{"paperId": "6121fb3e393597e02481a516f0035f06ec9a5836", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "FinGPT: Democratizing Internet-scale Data for Financial Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available, and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, Financial Generative Pre-trained Transformer (FinGPT), that automates the collection and curation of real-time financial data from 34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes have been open-sourced.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science", "Economics"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-07-19", "journal": {"name": "ArXiv", "volume": "abs/2307.10485"}, "authors": [{"authorId": "4029028", "name": "Xiao-Yang Liu"}, {"authorId": "2184568949", "name": "Guoxuan Wang"}, {"authorId": "1759658", "name": "D. Zha"}], "citations": [{"paperId": "4c79dbd16ce257243ae53abf3cb905990015ef55", "title": "GreedLlama: Performance of Financial Value-Aligned Large Language Models in Moral Reasoning"}, {"paperId": "15b0a6ccb198b2936e36266be992da78a29953fd", "title": "FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications"}, {"paperId": "d36bbbe2eb83981c4e714f1d4688334f2aae6369", "title": "GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture"}, {"paperId": "e53412b4f0f15330f8efa84e3c8f649bbe5e26f7", "title": "An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide"}, {"paperId": "3d6197e4ab55a3a2785ce5934e48cfbe9fe9bf04", "title": "Revolutionizing Finance with LLMs: An Overview of Applications and Insights"}, {"paperId": "c3bacc93d8c0c6ed31763d0c0fc7951f0ab82ed8", "title": "Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning"}, {"paperId": "7d011d6a9e1704acc29bab88d616089089ea1006", "title": "PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation"}, {"paperId": "74b0976a3a7b7013fd468a043a940dcf401e66f1", "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions"}, {"paperId": "5b038c1a93967072cc76689fd805e756f804cc42", "title": "Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"}, {"paperId": "6ad6291894cf356c255466c04a684c8285503e03", "title": "Assessing Social Media\u2019s Impact on Stock Market Predictions: Financial Sentiment Embedding Approaches"}, {"paperId": "24de1048791bac4972ecc16d1c3c1de23691407d", "title": "Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"}, {"paperId": "bb83db3f3183c847b2394ee3a8c13670bf34620f", "title": "GPT-Neo-CRV: Elevating Information Accuracy in GPT-Neo with Cross-Referential Validation"}]}
