{"paperId": "49f302e9a76eb39c88fcd861bdd0d954bd3d76b0", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Large Language Model for Multi-objective Evolutionary Optimization", "abstract": "Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the search operators need a carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well on new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose a new version of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on different test benchmarks show that our proposed method can achieve competitive performance with widely used MOEAs. It is also promising to see the operator only learned from a few instances can have robust generalization performance on unseen problems with quite different patterns and settings. The results reveal the potential benefits of using pre-trained LLMs in the design of MOEAs.To foster reproducibility and accessibility, the source code is https://github.com/FeiLiu36/LLM4MOEA.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-19", "journal": {"name": "ArXiv", "volume": "abs/2310.12541"}, "authors": [{"authorId": "2256759231", "name": "Fei Liu"}, {"authorId": "2109496037", "name": "Xi Lin"}, {"authorId": "2257343800", "name": "Zhenkun Wang"}, {"authorId": "2244321811", "name": "Shunyu Yao"}, {"authorId": "153919866", "name": "Xialiang Tong"}, {"authorId": "2260338665", "name": "Mingxuan Yuan"}, {"authorId": "2248220773", "name": "Qingfu Zhang"}], "citations": [{"paperId": "01bc5281e65273718750619f23483974c3f98868", "title": "How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems"}, {"paperId": "b671934871fb9a2db3ba729647f298d46958804d", "title": "Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism"}, {"paperId": "d2503ddd1bcd4b33ac1c703c3475d6aae8abf483", "title": "LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation"}, {"paperId": "be7a88babf78512b545f585517704cb597388cbc", "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution"}, {"paperId": "2c3152761de1dec262ed6a0375cca9c4d334a52f", "title": "A match made in consistency heaven: when large language models meet evolutionary algorithms"}, {"paperId": "478f71f1cd9bad0435560544a9dce7ca49d97766", "title": "Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap"}, {"paperId": "6d91f344e9c80df5c69311abfc63b74b42c735c3", "title": "An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search"}, {"paperId": "6a4719871c5d51ad2cd140561652500df06409bd", "title": "Algorithm Evolution Using Large Language Model"}, {"paperId": "1c259361caa85c2d95a7d04e5e42fa98693da85b", "title": "Large Language Models as Evolutionary Optimizers"}]}
