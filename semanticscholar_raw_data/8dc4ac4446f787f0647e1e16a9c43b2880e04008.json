{"paperId": "8dc4ac4446f787f0647e1e16a9c43b2880e04008", "publicationVenue": null, "title": "Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer", "abstract": "This paper explores cost-efficient methods to adapt pretrained Large Language Models (LLMs) to new lower-resource languages, with a specific focus on Estonian. Leveraging the Llama 2 model, we investigate the impact of combining cross-lingual instruction-tuning with additional monolingual pretraining. Our results demonstrate that even a relatively small amount of additional monolingual pretraining followed by cross-lingual instruction-tuning significantly enhances results on Estonian. Furthermore, we showcase cross-lingual knowledge transfer from high-quality English instructions to Estonian, resulting in improvements in commonsense reasoning and multi-turn conversation capabilities. Our best model, named \\textsc{Llammas}, represents the first open-source instruction-following LLM for Estonian. Additionally, we publish Alpaca-est, the first general task instruction dataset for Estonia. These contributions mark the initial progress in the direction of developing open-source LLMs for Estonian.", "venue": "", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2024-04-05", "journal": null, "authors": [{"authorId": "2137354484", "name": "Hele-Andra Kuulmets"}, {"authorId": "2167124642", "name": "Taido Purason"}, {"authorId": "88740779", "name": "Agnes Luhtaru"}, {"authorId": "2032659", "name": "Mark Fishel"}], "citations": []}
