{"paperId": "e61462184a6dce9259e76c9069c5747a420b3c0d", "publicationVenue": {"id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e", "name": "Proceedings of the VLDB Endowment", "type": "journal", "alternate_names": ["Proceedings of The Vldb Endowment", "Proc VLDB Endow", "Proc Vldb Endow"], "issn": "2150-8097", "url": "http://dl.acm.org/toc.cfm?id=J1174", "alternate_urls": ["http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"]}, "title": "SDPipe: A Semi-Decentralized Framework for Heterogeneity-aware Pipeline-parallel Training", "abstract": "\n The increasing size of both deep learning models and training data necessitates the ability to scale out model training through pipeline-parallel training, which combines pipelined model parallelism and data parallelism. However, most of them assume an ideal homogeneous dedicated cluster. As for real cloud clusters, these approaches suffer from the intensive model synchronization overheads due to the dynamic environment heterogeneity. Such a huge challenge leaves the design in a dilemma: either the performance bottleneck of the central parameter server (PS) or severe performance degradation caused by stragglers for decentralized synchronization (like All-Reduce). This approach presents SDPipe, a new\n semi-decentralized\n framework to get the best of both worlds, achieving both high heterogeneity tolerance and convergence efficiency in pipeline-parallel training. To provide high performance, we decentralize the communication model synchronization, which accounts for the largest proportion of synchronization overhead. In contrast, we centralize the process of group scheduling, which is lightweight but needs a global view for better performance and convergence speed against heterogeneity. We show via a prototype implementation the significant advantage of SDPipe on performance and scalability, facing different environments.\n", "venue": "Proceedings of the VLDB Endowment", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-05-01", "journal": {"name": "Proc. VLDB Endow.", "pages": "2354-2363", "volume": "16"}, "authors": [{"authorId": "1720763480", "name": "Xupeng Miao"}, {"authorId": "2118897651", "name": "Yining Shi"}, {"authorId": "2109540175", "name": "Zhi Yang"}, {"authorId": "2143385941", "name": "Bin Cui"}, {"authorId": "2072782550", "name": "Zhihao Jia"}], "citations": [{"paperId": "965427d702a8a09945449ce09be8c7b46dbabb98", "title": "Communication-Efficient Federated Optimization over Semi-Decentralized Networks"}, {"paperId": "d42c50b154dbc41cf869c702f9e4c40fd0cea4da", "title": "OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning"}]}
