{"paperId": "a122863d239643453195424c04067e89406246e1", "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods in Natural Language Processing", "type": "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations", "abstract": "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-05-23", "journal": {"pages": "3029-3051"}, "authors": [{"authorId": "46649145", "name": "Ning Ding"}, {"authorId": "2135835258", "name": "Yulin Chen"}, {"authorId": "2052218689", "name": "Bokai Xu"}, {"authorId": "50625437", "name": "Yujia Qin"}, {"authorId": "2115548452", "name": "Zhi Zheng"}, {"authorId": "1576223501", "name": "Shengding Hu"}, {"authorId": "2141313179", "name": "Zhiyuan Liu"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "2218723159", "name": "Bowen Zhou"}], "citations": [{"paperId": "97dc1b2b7910c7a946fead4fe6b53240b398301a", "title": "Binary Classifier Optimization for Large Language Model Alignment"}, {"paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641", "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"}, {"paperId": "f873f40d769295f7cf3f574ab65d0d8f0e424041", "title": "Investigating Regularization of Self-Play Language Models"}, {"paperId": "f840fdcd9e95bba5476030c000d82a172e984181", "title": "Exploring Backdoor Vulnerabilities of Chat Models"}, {"paperId": "a22afef5675c92bc270eefdd13fcf7f00c824571", "title": "MIMIR: A Streamlined Platform for Personalized Agent Tuning in Domain Expertise"}, {"paperId": "ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0", "title": "Advancing LLM Reasoning Generalists with Preference Trees"}, {"paperId": "81168eb40a7e2fab2a537c3e6d05204b10606636", "title": "DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations"}, {"paperId": "6fd5dbea7588ee6bca703aa3fea9a487006dba29", "title": "Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order"}, {"paperId": "8e6124739e03c9d7ea4903de00c3370d2f1a8387", "title": "Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback"}, {"paperId": "40e996a7c3e914a67c708704fa9b4c54ea70f36e", "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference"}, {"paperId": "2d1adda7a9b0596d8b470c065da5a6528b364a7f", "title": "Automated Data Curation for Robust Language Model Fine-Tuning"}, {"paperId": "76b65c248677314865a110424542c220886dbb67", "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions"}, {"paperId": "9f7db53338c8e76d708faf1a126e7b43331a57b2", "title": "Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities"}, {"paperId": "bb5393126610ab89983b29d8934b45f67a16241d", "title": "What Was Your Prompt? A Remote Keylogging Attack on AI Assistants"}, {"paperId": "6d8bc48bd0993f590dd39607dfdbcfdcf5d7dd2c", "title": "Komodo: A Linguistic Expedition into Indonesia's Regional Languages"}, {"paperId": "a8b1484d3ee6b9ad6e4c48d6bcdbd1048493599d", "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models"}, {"paperId": "917aa8165c0ab778ea3aea7a2b286c74cd2889f0", "title": "SMART: Submodular Data Mixture Strategy for Instruction Tuning"}, {"paperId": "7709a9eefa9a67510111aef2877f2834a76c8829", "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences"}, {"paperId": "973814cd535facbf4f27c3de477b05bf19366030", "title": "ORPO: Monolithic Preference Optimization without Reference Model"}, {"paperId": "f9c353dc639cced8cd098fe6d9c92bdef53ebc19", "title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents"}, {"paperId": "c0b454e0a6aa51ff3ba56778787d0c43932ef6ba", "title": "Yi: Open Foundation Models by 01.AI"}, {"paperId": "13f44206745d20971ca271401eff6772aa80de80", "title": "SaulLM-7B: A pioneering Large Language Model for Law"}, {"paperId": "a915b3b2d0fdfefbf038a8746ceaf001859f3431", "title": "CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following"}, {"paperId": "97c17683dbd3bea6972b93f5ab8ebb208b344ee6", "title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs"}, {"paperId": "6fe7e6ce3cc0ebd038caa456d73fd7472e7d6c38", "title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment"}, {"paperId": "db5516b3fa82ed1f63cc951a5f99578332ad37b9", "title": "Do Large Language Models Mirror Cognitive Language Processing?"}, {"paperId": "e53aa81923958e8e247bb9d09250b8ccf0848513", "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards"}, {"paperId": "cc49514a9859ae171a7cd7eaae111990605a5e4e", "title": "Tower: An Open Multilingual Large Language Model for Translation-Related Tasks"}, {"paperId": "02ac355296f001a010b1db115d909c052767ccb3", "title": "Stable LM 2 1.6B Technical Report"}, {"paperId": "978040c55f9094212d15b7ffadfab1440913b6cf", "title": "CodeS: Towards Building Open-source Language Models for Text-to-SQL"}, {"paperId": "433b22bdb9eb7b87afd3f6e77f643c3413ef33f9", "title": "Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?"}, {"paperId": "9978f937189a279a726545a734d20558095e7a0a", "title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues"}, {"paperId": "b5142d7f7282be6aec15c1d30091eeb442268bbf", "title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models"}, {"paperId": "1c1b5bc728cb6c59574e77987441ec066bea9109", "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "3d4fbb81345bac2a4ec3e6d89e36adf42b214fae", "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models"}, {"paperId": "087699924e3dc468a486e0763f1cc097824a60d2", "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models"}, {"paperId": "eb6eba90a399e6a8f5810f9b1fde6db551b4a009", "title": "Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs"}, {"paperId": "2bff22264c1cbb5e0edc98100492531653ec299c", "title": "Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement"}, {"paperId": "57a8333365bf99b65591e6b2176eacf8fd85d5da", "title": "Language Models as Science Tutors"}, {"paperId": "ad2be51acf42f686a8d1de92d7435d84274ee62d", "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math"}, {"paperId": "8e624e215908934a38044500f8434a0f88c69059", "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models"}, {"paperId": "ee85c7c666135f4aae32336968f09584029b6a35", "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning"}, {"paperId": "bddf62f93b2200d058c08983b5bb16207be73c4d", "title": "TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles"}, {"paperId": "ede28224a1d6633826198d1702a5a737ea236575", "title": "Reinforcement Learning from Human Feedback with Active Queries"}, {"paperId": "db7498f569be9852a04b2bb5bd68bd2885820bea", "title": "World Model on Million-Length Video And Language With Blockwise RingAttention"}, {"paperId": "1beaf69026af98e7ad7a028b4ed8aebb06302d6e", "title": "Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue"}, {"paperId": "7ae48b24cbf955bf9b9498fb287bf4c5cd3b73d4", "title": "OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning"}, {"paperId": "e708a73ea5f54cd944fb1e4ab36d947bc35aca2c", "title": "Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks"}, {"paperId": "32370746059bc592106688f119110e7d54fc5848", "title": "Exploring Learning Complexity for Downstream Data Pruning"}, {"paperId": "ec8e2b45c4601730015608a58e33409224a81228", "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models"}, {"paperId": "2565b1ae47fb8e47017bee18ef1c602d6e1f4e44", "title": "Rethinking Data Selection for Supervised Fine-Tuning"}, {"paperId": "c21022a4dca7b1a32d2f79818cb7e51df712372f", "title": "UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset"}, {"paperId": "b82ccc66c14f531a444c74d2a9a9d86a86a8be99", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal"}, {"paperId": "95898b1f82cf7ad7d96fcc85b4def7f086325af5", "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning"}, {"paperId": "184bfcbed3fb8dadffef665412f373cebd42ede5", "title": "Diversity Measurement and Subset Selection for Instruction Tuning Datasets"}, {"paperId": "329a949f3d67fa345db5f7b807faef022e92d364", "title": "Preference-free Alignment Learning with Regularized Relevance Reward"}, {"paperId": "341103678d9def1add23ad12a28e31a985ea50cd", "title": "CroissantLLM: A Truly Bilingual French-English Language Model"}, {"paperId": "139f3e52d1b90992d6908497ed8d6535bad258a2", "title": "H2O-Danube-1.8B Technical Report"}, {"paperId": "0cae650c4dff19057cde78bfcc6775e45bd5d3e8", "title": "TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese"}, {"paperId": "971a71c764ba85e65488f73f9a7a485912aca065", "title": "Airavata: Introducing Hindi Instruction-tuned LLM"}, {"paperId": "217c8dad8f8a6ddce907a947bf06d6f8fbb1c035", "title": "Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding"}, {"paperId": "4a776e757e46c0c41020cb15e574a62aceb20cf5", "title": "Instructional Fingerprinting of Large Language Models"}, {"paperId": "f21d0177e9374bb8579c1d9c71319f212f62b3d5", "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance"}, {"paperId": "bed8a8af3189f3846ac5fc2921b863e4c98244e1", "title": "Orion-14B: Open-source Multilingual Large Language Models"}, {"paperId": "6174f46a035c57f7ca4b9d581a31022f60dcbedd", "title": "PHOENIX: Open-Source Language Adaption for Direct Preference Optimization"}, {"paperId": "57e7af0b69325fafb371ef5d502e39ef9c90ef7e", "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"}, {"paperId": "60f47874beb5f00364f8621410d1d34c37d11007", "title": "AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets"}, {"paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5", "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"}, {"paperId": "8dce168f723158b771b526401113064c36fc875e", "title": "State of What Art? A Call for Multi-Prompt LLM Evaluation"}, {"paperId": "41113411e1748a34bb80f12c761b7af1ed6dbb90", "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning"}, {"paperId": "c65583b6848454de5d56668a2788093280e3cbb6", "title": "Typhoon: Thai Large Language Models"}, {"paperId": "5b1d02bfce832b8ab41b5209e637a9f42c26d912", "title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?"}, {"paperId": "34ad751c1e797975452c655e3912313044d1dd17", "title": "Urban Generative Intelligence (UGI): A Foundational Platform for Agents in Embodied City Environment"}, {"paperId": "b87906b53933a8e442c54bd448821f9a869c6332", "title": "PowerPulse: Power energy chat model with LLaMA model fine\u2010tuned on Chinese and power sector domain knowledge"}, {"paperId": "ec9414654469692d8f1de8e2401a3dcbc58ee11a", "title": "Demystifying Instruction Mixing for Fine-tuning Large Language Models"}, {"paperId": "6cfbbf7604adda1df65932e3c4d157770a2df000", "title": "Alignment for Honesty"}, {"paperId": "5ee871537ae51e7e2e93d2a70fff5d100649a655", "title": "Mathematical Language Models: A Survey"}, {"paperId": "959e72f501017655d167998fd307d93cb0b2cea4", "title": "\"What's important here?\": Opportunities and Challenges of Using LLMs in Retrieving Information from Web Interfaces"}, {"paperId": "a2b150e02306038389f5df683428f5a4659a468e", "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following"}, {"paperId": "117a9f9227ebe4acc67ff8a52eb9db2ab16390c8", "title": "Data Management For Large Language Models: A Survey"}, {"paperId": "600d9287efc4703bdb99ce39b5e8b37da0baa6f6", "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning"}, {"paperId": "0f9a3c5c6a54fca6be2afa0fd5fd34eed96a31e8", "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback"}, {"paperId": "e8b22bf8a78401b3807bcd46fa7c88d0c07f58ba", "title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation"}, {"paperId": "936f7f0fa77efcd322805b93a8d74c48a4108290", "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?"}, {"paperId": "e3f7ad05b1652c6ada78cffbe405bceb723bc70c", "title": "MoDS: Model-oriented Data Selection for Instruction Tuning"}, {"paperId": "d034a77636c0b155eb3f752f161203f7891135cb", "title": "Data Diversity Matters for Robust Instruction Tuning"}, {"paperId": "54c9a97637822c9e1956b1ec70b0c9a0f2338d2c", "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking"}, {"paperId": "a5c61aae22f47a7deaf7397a4a24bce42a8cc1f1", "title": "Grounding Gaps in Language Model Generations"}, {"paperId": "532c2c7a247d9e97d20abec1b2f4612984fdab93", "title": "REST: Retrieval-Based Speculative Decoding"}, {"paperId": "a768256f92d911a00389627e0d1fabaa2e0d1c32", "title": "PhoGPT: Generative Pre-training for Vietnamese"}, {"paperId": "a54761081c2b001c057fb6e1ea9a48058d5aa5e0", "title": "CLEX: Continuous Length Extrapolation for Large Language Models"}, {"paperId": "8e817e7c898a6a52f0abd5acfb9de9e313b13ccf", "title": "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI"}, {"paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84", "title": "Zephyr: Direct Distillation of LM Alignment"}, {"paperId": "b8a83b11944355b0c5e59e911af4a2a0bfa0362a", "title": "Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting"}, {"paperId": "5dd943677db684050ae9265f217a71894a58d40d", "title": "Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions"}, {"paperId": "eab54bef4b479e0b06fc65c8cab2c0400fa69b8a", "title": "On the Evaluation and Refinement of Vision-Language Instruction Tuning Datasets"}, {"paperId": "e93562137240873bf1262e769dd9d73c2dcba858", "title": "Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization"}, {"paperId": "957b601e2beefeaaf2e069e0130054051c8a7782", "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning"}, {"paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b", "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"}, {"paperId": "0ea1d396ce3804054c1919d7b78d3bcddaa761c0", "title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "84a36e19f9394f22b34f79756fa9628a795e02ea", "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset"}, {"paperId": "29f032fc875576b5c3c6b1c2d76af8639bacfb88", "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"}, {"paperId": "b549adab842e48b037d8aca84336296f84a9fa81", "title": "Can Large Language Models Understand Real-World Complex Instructions?"}, {"paperId": "894ed1aba8e42a4ec27ba53ecde383b14c5128ca", "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"}, {"paperId": "95be6a38f9b3ca3b7a9d81215e52cdcf545d554a", "title": "ORES: Open-vocabulary Responsible Visual Synthesis"}, {"paperId": "1ce1738d7f224ebd7ad98e23692404f06697b5f4", "title": "Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models"}, {"paperId": "e3052ebca5eeae6a8a73e44517903d39746f5f3a", "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning"}, {"paperId": "88b0ee59d565f2e04f62e1729ac9c681e132f103", "title": "PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator"}, {"paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f", "title": "Instruction Tuning for Large Language Models: A Survey"}, {"paperId": "dd3fb89d1201d46fa80b6a9519114599c01c11ac", "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"}, {"paperId": "f2ba9e7d9624bd94a786ea5e3161a9425a21a475", "title": "Self-Alignment with Instruction Backtranslation"}, {"paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"}, {"paperId": "46ac88bb0acbf736840ff8a392cec2bf43d917e1", "title": "Exploring Format Consistency for Instruction Tuning"}, {"paperId": "b34862afacf36e7011d40c67bb67c5ee9cf7da22", "title": "DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI"}, {"paperId": "82a9b8984e26fdf234431459bdb445fbcfc3cb76", "title": "Visual Instruction Tuning with Polite Flamingo"}, {"paperId": "f86aa25603d1f2e4066db9b6a9a6d311b4e8c491", "title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models"}, {"paperId": "807f4a368e845f7649e645f91d1d4f2ce72beee3", "title": "A Comprehensive Survey on Instruction Following"}, {"paperId": "a08086808517c1f1274a0df592cab1528797cc79", "title": "Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters"}, {"paperId": "46ae37aac816ed375a8a6134c003dedd63bb82d4", "title": "M OL -I NSTRUCTIONS : A L ARGE -S CALE B IOMOLECU - LAR I NSTRUCTION D ATASET FOR LLM S"}, {"paperId": "3d473cbb7a377cf960abff31748a1a39bb6c7d7c", "title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"}, {"paperId": "e730164e17975547564a1eaa70cea5884b16c89d", "title": "Instruction : Translate the phrase \u201d Bonne chance \u201d into English Response : Good Luck"}, {"paperId": "ac771182d1780c863954243809d1e144433919f9", "title": "Aligning Large Language Models with Human: A Survey"}, {"paperId": null, "title": "2 3 2 2 3 2 8 2 3 2 1 2 1 0 2 0 2 2 0 0 2 6 4 2 2 1 1 2 1 2 1 0 1"}, {"paperId": "80da74d006d9e9ac1252c4092bb72b9878d8bec7", "title": "W ORLD M ODEL ON M ILLION -L ENGTH V IDEO A ND L ANGUAGE W ITH R ING A TTENTION"}]}
