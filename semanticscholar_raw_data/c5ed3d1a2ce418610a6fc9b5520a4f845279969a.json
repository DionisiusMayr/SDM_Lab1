{"paperId": "c5ed3d1a2ce418610a6fc9b5520a4f845279969a", "publicationVenue": {"id": "6c6fcaa9-fd25-488b-8050-995227ac671c", "name": "Symposium on Operating Systems Principles", "type": "conference", "alternate_names": ["Symp Oper Syst Princ", "SOSP"], "url": "http://sosp.org/"}, "title": "Parity models: erasure-coded resilience for prediction serving systems", "abstract": "Machine learning models are becoming the primary work-horses for many applications. Services deploy models through prediction serving systems that take in queries and return predictions by performing inference on models. Prediction serving systems are commonly run on many machines in cluster settings, and thus are prone to slowdowns and failures that inflate tail latency. Erasure coding is a popular technique for achieving resource-efficient resilience to data unavailability in storage and communication systems. However, existing approaches for imparting erasure-coded resilience to distributed computation apply only to a severely limited class of functions, precluding their use for many serving workloads, such as neural network inference. We introduce parity models, a new approach for enabling erasure-coded resilience in prediction serving systems. A parity model is a neural network trained to transform erasure-coded queries into a form that enables a decoder to reconstruct slow or failed predictions. We implement parity models in ParM, a prediction serving system that makes use of erasure-coded resilience. ParM encodes multiple queries into a \"parity query,\" performs inference over parity queries using parity models, and decodes approximations of unavailable predictions by using the output of a parity model. We showcase the applicability of parity models to image classification, speech recognition, and object localization tasks. Using parity models, ParM reduces the gap between 99.9th percentile and median latency by up to 3.5X, while maintaining the same median. These results display the potential of parity models to unlock a new avenue to imparting resource-efficient resilience to prediction serving systems.", "venue": "Symposium on Operating Systems Principles", "year": 2019, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2019-10-27", "journal": {"name": "Proceedings of the 27th ACM Symposium on Operating Systems Principles"}, "authors": [{"authorId": "8500842", "name": "J. Kosaian"}, {"authorId": "2453556", "name": "K. V. Rashmi"}, {"authorId": "2697906", "name": "S. Venkataraman"}], "citations": [{"paperId": "357285165d8b63b4627937288f5692c7bb005fb7", "title": "The power of big models: Unleashing opportunities for cloud computing"}, {"paperId": "929ed6412136fe42e6ef1eeb7ea0b4da693dee37", "title": "SpotServe: Serving Generative Large Language Models on Preemptible Instances"}, {"paperId": "dd66f54fa721e3affa29e30e823e400c3e54a8f7", "title": "Automatic and Efficient Customization of Neural Networks for ML Applications"}, {"paperId": "928ddeb3222f36df9c1dd72d84bcde997f077e93", "title": "Efficient Fault Tolerance for Recommendation Model Training via Erasure Coding"}, {"paperId": "e7a572d1a16c22addcf6c98ed88d0344de4c2f97", "title": "On expanding the toolkit of locality-based coded computation to the coordinates of inputs"}, {"paperId": "46ae9785fc280e9edce3b8b4a9ff4deaf6dbd1a3", "title": "Compression-Informed Coded Computing"}, {"paperId": "5498977871f3cd6bb1488687f663b08a46af1a9e", "title": "FaaSwap: SLO-Aware, GPU-Efficient Serverless Inference via Model Swapping"}, {"paperId": "957c2efffec6f32ef4d5be65025904e7372d92ff", "title": "Tackling Heterogeneous Traffic in Multi-access Systems via Erasure Coded Servers"}, {"paperId": "d695705ca865d6a3f23cdfdba12c2aedd55ab1f4", "title": "Nested Gradient Codes for Straggler Mitigation in Distributed Machine Learning"}, {"paperId": "dc8d2b405d20871b177396eba65af02b59b1166e", "title": "A Fault-Tolerant and Real-Time Framework for Efficient Resilience in Edge Inference Systems"}, {"paperId": "de08ad47608bf5ce72757713b750044631fa02d9", "title": "Tackling heterogeneous traffic in multi-access systems via erasure coded servers"}, {"paperId": "745093391442971373d2af7549b00651a4636808", "title": "Deep Learning Workload Scheduling in GPU Datacenters: Taxonomy, Challenges and Vision"}, {"paperId": "44a5c1c4a98650ed42bad633e098bb98486f411f", "title": "A Learning-Based Approach to Approximate Coded Computation"}, {"paperId": "973007aef333350731bbf173e9d7abd312ab00d9", "title": "Learning-Augmented Streaming Codes are Approximately Optimal for Variable-Size Messages"}, {"paperId": "534a19bb1d8313b8db3c456c36c28dbdbc874820", "title": "Lightweight Projective Derivative Codes for Compressed Asynchronous Gradient Descent"}, {"paperId": "82c29f19a0ac9f602ba02ba07e836a39c382a695", "title": "Following the Data, Not the Function: Rethinking Function Orchestration in Serverless Computing"}, {"paperId": "09ce40faf1b73337d83e84f8b449ba7192c6fd82", "title": "ApproxIFER: A Model-Agnostic Approach to Resilient and Robust Prediction Serving Systems"}, {"paperId": "48b002743303ecec287ee50abe5609dcfe17c05f", "title": "Adaptive Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning"}, {"paperId": "17ebd2d45a4b309be45813e664d66e0dcb0b0e76", "title": "Coded-InvNet for Resilient Prediction Serving Systems"}, {"paperId": "e635a52009285e078ba37dd67e84d35a572e1c6a", "title": "$\\epsilon$-Approximate Coded Matrix Multiplication is Nearly Twice as Efficient as Exact Multiplication"}, {"paperId": "09fcc7ed1f867bcf9133ab12065ee7366cfaa652", "title": "ECRM: Efficient Fault Tolerance for Recommendation Model Training via Erasure Coding"}, {"paperId": "dcdd4e71209c896921e4e5e7a24a5116507a3647", "title": "Accelerating Deep Learning Inference via Learned Caches"}, {"paperId": "cfb2965cbd3a1e8fc8d931a410522ee59bbe1af6", "title": "Synergy via Redundancy: Adaptive Replication Strategies and Fundamental Limits"}, {"paperId": "e133e4f527e0ee4378ebd567539b0f628121a155", "title": "Robust Class Parallelism - Error Resilient Parallel Inference with Low Communication Cost"}, {"paperId": "abb12463927cb6656ae69740c7aa70b444a72082", "title": "S3ML: A Secure Serving System for Machine Learning Inference"}, {"paperId": "5d35eec761ead6e7c742c8dd51d0e2426dc7a0de", "title": "Device and Placement Aware Framework to optimize Single Failure Recoveries and Reads for Erasure Coded Storage System with Heterogeneous Storage Devices"}, {"paperId": "7db5a4b19302403975e6f88a297542dbd09804ee", "title": "Regulating Accuracy-Efficiency Trade-Offs in Distributed Machine Learning Systems"}, {"paperId": "8fb930f3098e890dcdce4eb709ef9b918d666ba4", "title": "Optimizing Prediction Serving on Low-Latency Serverless Dataflow"}, {"paperId": "7267c8343bd28b472e5b0684e98f067dfd70d5d4", "title": "Accuracy-Efficiency Trade-Offs and Accountability in Distributed ML Systems"}, {"paperId": "dc163f51cdc358b11f15b60f9c262b4e3f9432e0", "title": "Holding Real-Time Distributed ML Systems to Account"}, {"paperId": "c75390f8138e2a422956d0e1b00cb6a39579bc95", "title": "PyTorch distributed"}, {"paperId": "c9b9428708f7925274a4487f958235863e32b19f", "title": "HAMS: High Availability for Distributed Machine Learning Service Graphs"}, {"paperId": "d72a1579074a1a2bc500f257474144b1957d5166", "title": "Learning-Based Coded Computation"}, {"paperId": "9cdf512f273083efa1ea01f7b31daa97a7bbe884", "title": "A locality-based approach for coded computation"}, {"paperId": "637da8d72a9bd7a312ab0da0b08c701f1c6e7d20", "title": "Willump: A Statistically-Aware End-to-end Optimizer for Machine Learning Inference"}, {"paperId": "cc22e1baf2d1888ae09ef6b389582b65c1ab8f9f", "title": "Rateless Codes for Near-Perfect Load Balancing in Distributed Matrix-Vector Multiplication"}, {"paperId": "5b35404465568b647f2c5ee132d7a8177229d242", "title": "Rateless codes for near-perfect load balancing in distributed matrix-vector multiplication"}, {"paperId": "883b5a6cbbf3c499c8402204a657abf1e836d310", "title": "Deep Learning Workload Scheduling in GPU Datacenters: A Survey"}, {"paperId": "c7392294e27e5dec774714d351dd9edbbd170e06", "title": "GL-Cache: Group-level learning for efficient and high-performance caching"}, {"paperId": "4d6ba62aca0ffd88bd34400b16824f168285f463", "title": "This paper"}, {"paperId": "a065720ee498a7546353cc4a32da27a996a2565b", "title": "Checkpoint-Free Fault Tolerance for Recommendation System Training via Erasure Coding"}, {"paperId": "3f8c0f77d4c13d63706a922b2bdec0b584de08e3", "title": "When to Hedge in Interactive Services"}, {"paperId": "b58e1c142eb3871792f3457fcacccbf8ba2a8b73", "title": "Understanding Accuracy-Efficiency Trade-Offs as a Means for Holding Distributed ML Systems Accountable"}, {"paperId": "42fdd52682f0ec996b90c34ac29278dbda590459", "title": "Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning"}, {"paperId": "7a0210ef3e8984acf4fee8467007a969d22b2da9", "title": "MapperX: Adaptive Metadata Maintenance for Fast Crash Recovery of DM-Cache Based Hybrid Storage Devices"}, {"paperId": "339158b00fda468806cbd841239689d00cc42c96", "title": "Restructuring Serverless Computing with Data-Centric Function Orchestration"}, {"paperId": "1ba7284feeaf15e37aa056bb5438e95141a81fb0", "title": "Review of Research on Coded Computing"}, {"paperId": "ad914351676408d627a0a85439ee8bfa0eaf1759", "title": "The Design of Stateful Serverless Infrastructure"}, {"paperId": "d8802c7d1bbb2a7e9f16ee02a753af39729775b3", "title": "Understanding and Mitigating Latency Variability of Latency-Critical Applications"}, {"paperId": "60171634659cee07793a295489d8eeda9f785832", "title": "Parity Models: Erasure-Coded Resilience for Prediction Serving Systems"}, {"paperId": "b9ea1cce667cc373ee57e2ecd4ef368a268cb3c5", "title": "FlexEnt: Entropy Coding to Curb Stragglers in Large-Scale Distributed Machine Learning"}, {"paperId": "ffe6522f8a4666642919ba0ea7d8c9990b1d41fb", "title": "the Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation."}]}
