{"paperId": "b50815251c948f00baedccaf5f56c281ffa7650f", "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "title": "Staircase Attention for Recurrent Processing of Sequences", "abstract": "Attention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence), or an extreme Ladder version with a forward step of zero that simply repeats the Transformer on each step of the ladder, sharing the weights. We thus describe a family of such models that can trade off performance and compute, by either increasing the amount of recurrence through time, the amount of sequential processing via recurrence in depth, or both. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.", "venue": "Neural Information Processing Systems", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-08", "journal": {"name": "ArXiv", "volume": "abs/2106.04279"}, "authors": [{"authorId": "3092435", "name": "Da Ju"}, {"authorId": "3849208", "name": "Stephen Roller"}, {"authorId": "2265067", "name": "Sainbayar Sukhbaatar"}, {"authorId": "145183709", "name": "J. Weston"}], "citations": [{"paperId": "dbb30953d9303658586083bbd4c196f9c0895f88", "title": "Investigating Recurrent Transformers with Dynamic Halt"}, {"paperId": "2fc074288f66711e4ee37350d364e74c1c401163", "title": "Recursion in Recursion: Two-Level Nested Recursion for Length Generalization with Scalability"}, {"paperId": "81a855af89a7f836f45d232a19408190e3a08d30", "title": "Grounded Image Text Matching with Mismatched Relation Reasoning"}, {"paperId": "1db6836f61695e558608bb57166feca6876edabf", "title": "Learning to Reason and Memorize with Self-Notes"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "title": "Exploring Length Generalization in Large Language Models"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "d21364775c881b9bb6c99885652c81df29d755e5", "title": "Learning to Reason and Memorize with Self-Questioning"}]}
