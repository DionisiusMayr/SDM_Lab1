{"paperId": "11cf88dce827bd67cbfa60400306318022e736d5", "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification", "abstract": "Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.", "venue": "Neural Information Processing Systems", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-08-23", "journal": {"name": "ArXiv", "volume": "abs/2308.12284"}, "authors": [{"authorId": "2551387", "name": "Kushal Tirumala"}, {"authorId": "2257241733", "name": "Daniel Simig"}, {"authorId": "2201435", "name": "Armen Aghajanyan"}, {"authorId": "4690624", "name": "Ari S. Morcos"}], "citations": [{"paperId": "be24ab597deeb00f9ee36a1434b36c3747483c28", "title": "No\"Zero-Shot\"Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance"}, {"paperId": "c00c3fd40e4ab89faa14c0d2d34e0ea5de8e3608", "title": "Exploring the Mystery of Influential Data for Mathematical Reasoning"}, {"paperId": "76b65c248677314865a110424542c220886dbb67", "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions"}, {"paperId": "cbf8b22de75aa61a00e0100b9b4e8abb18bc11c3", "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models"}, {"paperId": "d68e93cad6a038f233cba5c7f3ca5448a1744d55", "title": "WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset"}, {"paperId": "c4ddba19c2f559b4bbc7f4596bf718a05af79683", "title": "Towards Optimal Learning of Language Models"}, {"paperId": "7b4005e0c624750252260bcac1ef67c9b804ec97", "title": "Balanced Data Sampling for Language Model Training with Clustering"}, {"paperId": "82f041ed71f8ef1cf462fa03a7e732e440259bd7", "title": "LongWanjuan: Towards Systematic Measurement for Long Text Quality"}, {"paperId": "c96b17f3362c2d795964d07427a9396a09db6b2c", "title": "Analysing The Impact of Sequence Composition on Language Model Pre-Training"}, {"paperId": "d244b2ba0fd5f573d19df0c7b362692c018748a1", "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models"}, {"paperId": "31b2918acc77682b5a499da1aa111f34e76d9603", "title": "How to Train Data-Efficient LLMs"}, {"paperId": "a73da8bdc130f0e78063b4f6efa09e9debc3569f", "title": "Scaling Laws for Downstream Task Performance of Large Language Models"}, {"paperId": "e9cb528aeef1f27e9cb4463f4e0dc629b6b177c8", "title": "On Catastrophic Inheritance of Large Foundation Models"}, {"paperId": "34ed926a6f1eb7e76685b62f6373d18a3f1fbc45", "title": "Institutional Platform for Secure Self-Service Large Language Model Exploration"}, {"paperId": "78d66de05b466b31f471348177e5eb246f0f9291", "title": "Large language models overcome the challenges of unstructured text data in ecology"}, {"paperId": "948c9d605a77d9d3c3959efecaa69d97b4d9a1de", "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents"}, {"paperId": "a0ef2ce2cc00b207e4a9792c0a826de80ac8654c", "title": "Effective pruning of web-scale datasets based on complexity of concept clusters"}, {"paperId": "d3feaa0a3b369257fdfcf9e11334cf63b2b96875", "title": "Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data"}, {"paperId": "117a9f9227ebe4acc67ff8a52eb9db2ab16390c8", "title": "Data Management For Large Language Models: A Survey"}, {"paperId": "d034a77636c0b155eb3f752f161203f7891135cb", "title": "Data Diversity Matters for Robust Instruction Tuning"}, {"paperId": "77b046c5d568b329a927cfc895ea2e6c8f43ff43", "title": "The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute"}, {"paperId": "cbc4f1f1e6e831086e05e3718520fa008071a014", "title": "RoBERTaLexPT: A Legal RoBERTa Model pretrained with deduplication for Portuguese"}, {"paperId": "24de1048791bac4972ecc16d1c3c1de23691407d", "title": "Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"}]}
