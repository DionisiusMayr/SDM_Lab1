{"paperId": "74cb59f7980fd298da53e46d95a4c2319be3bb07", "publicationVenue": null, "title": "Fast Parameter Synchronization for Distributed Learning with Selective Multicast", "abstract": "Recent advances in distributed machine learning show theoretically and empirically that, for many models, provided workers would participate in the synchronizations eventually, i) the training still converges, even if only p workers take part in each round of synchronization, and ii) a larger p generally leads to a faster rate of convergence. These findings shed light on eliminating the bottleneck effects of parameter synchronization in large-scale data-parallel distributed training, having motivated several optimization designs.In this paper, we focus on optimizing the parameter synchronization for peer-to-peer distributed learning, in which workers generally broadcast or multicast their updated parameters to others for synchronization, and propose SELMCAST, an expressive and Pareto-optimal multicast receiver selection algorithm, to achieve the goal. Compared with the state-of-the-art design that randomly selects exactly p receivers for each worker\u2019s multicast in a bandwidth-agnostic way, SELMCAST chooses receivers based on the global view of their available bandwidth and loads, yielding two advantages. Firstly, it could optimize the bottleneck sending rate, thus cutting down the time cost of parameter synchronization. Secondly, when more than p receivers are with sufficient bandwidth, they would be selected as many as possible, bringing benefits to the convergence of training. Extensive evaluations show that SELMCAST is efficient and always achieves near-optimal performance.", "venue": "ICC 2022 - IEEE International Conference on Communications", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-05-16", "journal": {"name": "ICC 2022 - IEEE International Conference on Communications", "pages": "4775-4780"}, "authors": [{"authorId": "2962874", "name": "Shouxi Luo"}, {"authorId": "145563833", "name": "P. Fan"}, {"authorId": "2149142438", "name": "Ke Li"}, {"authorId": "38092176", "name": "Huanlai Xing"}, {"authorId": "2064538", "name": "Long Luo"}, {"authorId": "49514984", "name": "Hongfang Yu"}], "citations": [{"paperId": "408a759eb02d7be41cc89365c9ad3abbbe989390", "title": "Exploring the Impact of Serverless Computing on Peer To Peer Training Machine Learning"}, {"paperId": "53e9260a6e4cd6c9aa8c8a3607c9de5ef2207593", "title": "Eliminating Communication Bottlenecks in Cross-Device Federated Learning with In-Network Processing at the Edge"}]}
