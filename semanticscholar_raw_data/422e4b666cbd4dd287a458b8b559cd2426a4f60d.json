{"paperId": "422e4b666cbd4dd287a458b8b559cd2426a4f60d", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Safer-Instruct: Aligning Language Models with Automated Preference Data", "abstract": "Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while maintaining a competitive edge in downstream tasks. Importantly, our Safer-Instruct framework is versatile and can be applied to generate preference data across various domains, extending its utility beyond safety preferences. It addresses the challenges in preference data acquisition and advances the development of more capable and responsible AI systems. For dataset and code implementation, see https://github.com/uscnlp-lime/safer-instruct", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-15", "journal": {"name": "ArXiv", "volume": "abs/2311.08685"}, "authors": [{"authorId": "2261392431", "name": "Taiwei Shi"}, {"authorId": "2266798497", "name": "Kai Chen"}, {"authorId": "2266812106", "name": "Jieyu Zhao"}], "citations": [{"paperId": "9689b5fdb0d3a1bad802d03d348bd32aa5a4c2df", "title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data"}, {"paperId": "a10325ad749859e5c857ecdb10f3c5a674f2e0a4", "title": "How Susceptible are Large Language Models to Ideological Manipulation?"}, {"paperId": "383c598625110e0a4c60da4db10a838ef822fbcf", "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"}, {"paperId": "c7ad19da81e24c387f0377fef6d19b0fce2cf470", "title": "Self-Guard: Empower the LLM to Safeguard Itself"}, {"paperId": "23e5395b4c6249c2873666bff73e203821c14719", "title": "Instruction Tuning with Human Curriculum"}]}
