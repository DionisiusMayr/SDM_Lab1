{"paperId": "dfddc6943706ff367ae9c9536b70e6da3f2a318a", "publicationVenue": {"id": "73e316c6-3e72-4c26-82ab-47e8da044b15", "name": "Symposium on Field Programmable Gate Arrays", "type": "conference", "alternate_names": ["Field Program Gate Array", "Symp Field Program Gate Array", "Field Programmable Gate Arrays", "FPGA"], "url": "http://www.wikicfp.com/cfp/program?id=1082"}, "title": "ACTS: A Near-Memory FPGA Graph Processing Framework", "abstract": "Despite the high off-chip bandwidth and on-chip parallelism offered by today's near-memory accelerators, software-based (CPU and GPU) graph processing frameworks still suffer performance degradation from under-utilization of available memory bandwidth because graph traversal often exhibits poor locality. Emerging FPGAbased graph accelerators tackle this challenge by designing specialized graph processing pipelines and application-specific memory subsystems to maximize bandwidth utilization and efficiently utilize high-speed on-chip memory. To use the limited on-chip (BRAM) memory effectively while handling larger graph sizes, several FPGAbased solutions resort to some form of graph slicing or partitioning during preprocessing to stage vertex property data into the BRAM. While this has demonstrated performance superiority for small graphs, this approach breaks down with larger graph sizes. For example, GraphLily [19], a recent high-performance FPGA-based graph accelerator, experiences up to 11X performance degradation between graphs having 3M vertices and 28M vertices. This makes prior FPGA approaches impractical for large graphs. We propose ACTS, an HBM-enabled FPGA graph accelerator, to address this problem. Rather than partitioning the graph offline to improve spatial locality, we partition vertex-update messages (based on destination vertex IDs) generated online after active edges have been processed. This optimizes read bandwidth even as the graph size scales. We compare ACTS against Gunrock, a state-of-the-art graph processing accelerator for the GPU, and GraphLily, a recent FPGA-based graph accelerator also utilizing HBM memory. Our results show a geometric mean speedup of 1.5X, with a maximum speedup of 4.6X over Gunrock, and a geometric speedup of 3.6X, with a maximum speedup of 16.5X, over GraphLily. Our results also showed a geometric mean power reduction of 50% and a mean reduction of energy-delay product of 88% over Gunrock.", "venue": "Symposium on Field Programmable Gate Arrays", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2023-02-12", "journal": {"name": "Proceedings of the 2023 ACM/SIGDA International Symposium on Field Programmable Gate Arrays"}, "authors": [{"authorId": "1395913345", "name": "Wole Jaiyeoba"}, {"authorId": "3031046", "name": "Nima Elyasi"}, {"authorId": "2114019316", "name": "Changho Choi"}, {"authorId": "1735065", "name": "K. Skadron"}], "citations": [{"paperId": "771b486e3babc5ccd2e11ed32c5764ee8cda7d0f", "title": "GraFlex: Flexible Graph Processing on FPGAs through Customized Scalable Interconnection Network"}]}
