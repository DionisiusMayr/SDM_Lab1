{"paperId": "9d793e542b757f234431d209e711c6ef88aa29de", "publicationVenue": null, "title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin", "abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Increasing instruction data substantially is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge-edge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM.", "venue": "", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2023-12-15", "journal": null, "authors": [{"authorId": "2042683163", "name": "Shihan Dou"}, {"authorId": "2240446306", "name": "Enyu Zhou"}, {"authorId": "2275033850", "name": "Yan Liu"}, {"authorId": "2181306462", "name": "Songyang Gao"}, {"authorId": "2257315251", "name": "Jun Zhao"}, {"authorId": "2248291262", "name": "Wei Shen"}, {"authorId": "2212175381", "name": "Yuhao Zhou"}, {"authorId": "2218237934", "name": "Zhiheng Xi"}, {"authorId": "2118451107", "name": "Xiao Wang"}, {"authorId": "2241140630", "name": "Xiaoran Fan"}, {"authorId": "2274941422", "name": "Shiliang Pu"}, {"authorId": "2277702055", "name": "Jiang Zhu"}, {"authorId": "2058585152", "name": "Rui Zheng"}, {"authorId": "2067331064", "name": "Tao Gui"}, {"authorId": "2257376355", "name": "Qi Zhang"}, {"authorId": "2257129989", "name": "Xuanjing Huang"}], "citations": [{"paperId": "8c307cbf2d081a3a84e30266eb340fc99576526c", "title": "Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning"}, {"paperId": "49259f30ccc0fafe6ad19c80cb18dee6ea5d3739", "title": "LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks"}, {"paperId": "08436b3ddafd2edc798753ebc87f6ceffed6e8df", "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models"}, {"paperId": "ec345204fb7a465f6263bd54270ecd6f459e48ca", "title": "LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild"}, {"paperId": "4d4d9da4f2c39089ea6c8d84e5031c195548d7b6", "title": "Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence"}, {"paperId": "2fe05b1f953da5dcf6ec5fe7bc72bfb3dbd9ea30", "title": "Model Compression and Efficient Inference for Large Language Models: A Survey"}, {"paperId": "94e531c3b139cdc50ab4b7be21a29dba8f87e3c0", "title": "Higher Layers Need More LoRA Experts"}]}
