{"paperId": "29f032fc875576b5c3c6b1c2d76af8639bacfb88", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "abstract": "Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-20", "journal": {"name": "ArXiv", "volume": "abs/2309.11235"}, "authors": [{"authorId": "2243452789", "name": "Guan Wang"}, {"authorId": "2110844331", "name": "Sijie Cheng"}, {"authorId": "2242851906", "name": "Xianyuan Zhan"}, {"authorId": "2243283400", "name": "Xiangang Li"}, {"authorId": "2243314934", "name": "Sen Song"}, {"authorId": "40457423", "name": "Yang Liu"}], "citations": [{"paperId": "1e631d2570228d5a0b5d5929e15fc588e1db09cd", "title": "Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks"}, {"paperId": "a5ed7a04640b11374871ccd6f6498bbc25aa8239", "title": "Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models"}, {"paperId": "086c6252d9b0e81a541dbad3dc5ee1fbd01330b3", "title": "Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT"}, {"paperId": "ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0", "title": "Advancing LLM Reasoning Generalists with Preference Trees"}, {"paperId": "beb1997dc3a8dca581c923eacb5370c564a6b5c8", "title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment"}, {"paperId": "af9df109e409b485e1337362431ba61623195145", "title": "Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM"}, {"paperId": "e56f14ced9f7ce344ed14bdcb46860ccac72ac83", "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge"}, {"paperId": "b34466cc6c387be3991ddb182e5749f0aca7612a", "title": "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases"}, {"paperId": "5acafd8acaebb9623af8ecc3deff8bcaeca2791e", "title": "ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy"}, {"paperId": "90107e4f29d6a22521dffe55415c0063513f82b9", "title": "Open Source Conversational LLMs do not know most Spanish words"}, {"paperId": "d5d7b26dbdf09737878644530b226a79c9f43bfb", "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"}, {"paperId": "75bae34f9361cd99d8f64bb8311feb01c7c75107", "title": "HDLdebugger: Streamlining HDL debugging with Large Language Models"}, {"paperId": "3c7b4f94a41fe6162717e5fb27a1d326a3304029", "title": "CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models"}, {"paperId": "a91d33b6f223ae145d5813a2f870e7764f036f24", "title": "MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection"}, {"paperId": "5ec2aa6c84e4b7ee51c6cc3e4cd74ed3b21c2df1", "title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability"}, {"paperId": "e7fc1b57dd032a8697902c01a9a715398b83a930", "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning"}, {"paperId": "288338c1ec81b259af5587677fdd10d5191baf71", "title": "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models"}, {"paperId": "31e27369f64c51ffbfd9bc8e3cbb20705edc6bff", "title": "Citation-Enhanced Generation for LLM-based Chatbots"}, {"paperId": "30e33e9ec257a60d8f095a285f31c702511db04a", "title": "FuseChat: Knowledge Fusion of Chat Models"}, {"paperId": "33313b413695dcd396c6f558a3153cc6a572d5c1", "title": "ChatMusician: Understanding and Generating Music Intrinsically with LLM"}, {"paperId": "d82a827fa21cce0b601ad21b6db606b8413ed547", "title": "Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement"}, {"paperId": "f200dc0b550f318412bd8484f5c51836d78d93dc", "title": "DEEM: Dynamic Experienced Expert Modeling for Stance Detection"}, {"paperId": "433b22bdb9eb7b87afd3f6e77f643c3413ef33f9", "title": "Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?"}, {"paperId": "5eac2a40422a7085cb6f03285ad08210b6f6744b", "title": "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement"}, {"paperId": "9ccaeea2c76a9072ccebf3eea3438d2ce18f5723", "title": "Unintended Impacts of LLM Alignment on Global Representation"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "66f581ae106ac28faebd3d54a01edd53cf91da30", "title": "Generative AI Security: Challenges and Countermeasures"}, {"paperId": "5e5f04a5ab1d22ffad0e96585469e269369ec676", "title": "Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages"}, {"paperId": "a7d047dd9f41d5f3e7eaa39e5ba8c97cccc7276d", "title": "Reformatted Alignment"}, {"paperId": "7cebf3afab09d990a2322ea053d0de96f31f2971", "title": "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?"}, {"paperId": "ba6643367e9150b3f4c2f3946a08e7e4f9267c35", "title": "ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model"}, {"paperId": "eb6eba90a399e6a8f5810f9b1fde6db551b4a009", "title": "Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs"}, {"paperId": "2bff22264c1cbb5e0edc98100492531653ec299c", "title": "Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement"}, {"paperId": "e12aec7435bd348ec4357faae7d7ccb8d83d954d", "title": "GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models"}, {"paperId": "ee85c7c666135f4aae32336968f09584029b6a35", "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning"}, {"paperId": "37560b8b09f7c859ba45cbadaf712f2c38727dbe", "title": "Exploring the Adversarial Capabilities of Large Language Models"}, {"paperId": "3246160ec123069e81ed06e3c3898c4cf7cd9ee0", "title": "TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation"}, {"paperId": "e3d317fca93654ffb4af005aea35b9d42441fe38", "title": "LLM Agents can Autonomously Hack Websites"}, {"paperId": "b82ccc66c14f531a444c74d2a9a9d86a86a8be99", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal"}, {"paperId": "b0486b98f5625162a88e98aa710736b008c97c21", "title": "Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach"}, {"paperId": "b1f243b586e87fe12ff8fe1a501f11ea5fc5ad44", "title": "On Prompt-Driven Safeguarding for Large Language Models"}, {"paperId": "cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3", "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"}, {"paperId": "7a38885dc66ec367cba6fbde1f88bab8dfb66ffc", "title": "RE-GAINS & EnCHANT: Intelligent Tool Manipulation Systems For Enhanced Query Responses"}, {"paperId": "e99e88257c01c4690ee5b4388a3b074da7911671", "title": "Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment"}, {"paperId": "35fb19eb94473f46ca8a0fa8e14de56f0aeb81e5", "title": "Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language Conversion for Language Models"}, {"paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f", "title": "Knowledge Verification to Nip Hallucination in the Bud"}, {"paperId": "75e78fb12a6a42ed8d61328fcca16fe460f20c5d", "title": "DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models"}, {"paperId": "18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6", "title": "Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception"}, {"paperId": "b137709522bc70b42b026cae192de2a45000b22e", "title": "MetaAID 2.5: A Secure Framework for Developing Metaverse Applications via Large Language Models"}, {"paperId": "4a836363233398c0ac27daee942cb5533f467458", "title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning"}, {"paperId": "0a693f0355e4f6bd6cbc59fc1519e1de8b7fed53", "title": "A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production"}, {"paperId": "117a9f9227ebe4acc67ff8a52eb9db2ab16390c8", "title": "Data Management For Large Language Models: A Survey"}, {"paperId": "7e17ef56273063dfa838de30b7cc0546b2e5ee10", "title": "Jellyfish: A Large Language Model for Data Preprocessing"}, {"paperId": "7e9b5dcba69b4f2b00941596e2fb9c2d9ea8daa9", "title": "Large Language Models Meet Computer Vision: A Brief Survey"}, {"paperId": "936f7f0fa77efcd322805b93a8d74c48a4108290", "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?"}, {"paperId": "e3f7ad05b1652c6ada78cffbe405bceb723bc70c", "title": "MoDS: Model-oriented Data Selection for Instruction Tuning"}, {"paperId": "48557ff1c18c4403fd05caa9249e09c756058b7b", "title": "EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models"}, {"paperId": "37680e5cb6030e01f1a44a5abe2257972196ae26", "title": "Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2"}, {"paperId": "84259db14b725853ecfe425fe85ca375b32983c2", "title": "Adversarial Preference Optimization"}, {"paperId": "4b530e7756a08af082c0ec2b242882b70873f753", "title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews"}, {"paperId": "1fc9b369f0690af9fb9b911bfff4dae0d4999083", "title": "Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons"}, {"paperId": "5088a04d1a9f42b967f3dcf791145e8aa367fc54", "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"}, {"paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b", "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"}, {"paperId": "88b0ee59d565f2e04f62e1729ac9c681e132f103", "title": "PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator"}, {"paperId": "c74a13b251b6af6dfce49eeb128b1c0e2ddf955d", "title": "A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment"}, {"paperId": "5dbf93a68b7fda600521f046dea35ea8ba9e884f", "title": "AgentBench: Evaluating LLMs as Agents"}, {"paperId": "285558c9c1a3e9b45cad1088248c939aed13c35e", "title": "Emulating Author Style: A Feasibility Study of Prompt-enabled Text Stylization with Off-the-Shelf LLMs"}, {"paperId": "df2d4846aa3b0850e27c7c49fdf61b98d5fe2a57", "title": "IIITDWD-zk@DravidianLangTech-2024: Leveraging the Power of Language Models for Hate Speech Detection in Telugu-English Code-Mixed Text"}, {"paperId": "49c0741c08ca08b0fa8afd3c74c82267699ea113", "title": "Findings of the Shared Task on Hate and Offensive Language Detection in Telugu Codemixed Text (HOLD-Telugu)@DravidianLangTech 2024"}, {"paperId": "c24c3606c0c581dc4512525a5ef41786fe43c29f", "title": "Scaling Down to Scale Up: A Cost-Benefit Analysis of Replacing OpenAI's GPT-4 with Self-Hosted Open Source SLMs in Production"}, {"paperId": "cd0f365fca59f303ce158c36faa7a7f430a5a698", "title": "ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training"}]}
