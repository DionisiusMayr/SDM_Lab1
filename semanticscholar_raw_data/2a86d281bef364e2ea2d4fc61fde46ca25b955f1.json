{"paperId": "2a86d281bef364e2ea2d4fc61fde46ca25b955f1", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs", "abstract": "Adapting a language model into a specific domain, a.k.a `domain adaption', is a common practice when specialized knowledge, e.g. medicine, is not encapsulated in a general language model like Llama2. The challenge lies in the heterogeneity of data across the two training stages, as it varies in languages, genres, or formats. To tackle this and simplify the learning protocol, we propose to transform heterogeneous data, from the both pre-training and supervised stages, into a unified, simple input-output pair format. We validate the new protocol in the domains where proprietary LLMs like ChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The developed model, HuatuoGPT-II, has shown state-of-the-art performance in Chinese medicine domain on a number of benchmarks, e.g. medical licensing exams. It even outperforms proprietary models like ChatGPT and GPT-4 in some aspects, especially in Traditional Chinese Medicine. Expert manual evaluations further validate HuatuoGPT-II's advantages over existing LLMs. Notably, HuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing Examination where it achieved the best performance, showcasing not only its effectiveness but also its generalization capabilities.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-16", "journal": {"name": "ArXiv", "volume": "abs/2311.09774"}, "authors": [{"authorId": "2108170007", "name": "Junying Chen"}, {"authorId": "2267007135", "name": "Xidong Wang"}, {"authorId": "2266840612", "name": "Anningzhe Gao"}, {"authorId": "2214807050", "name": "Feng Jiang"}, {"authorId": "2267023659", "name": "Shunian Chen"}, {"authorId": "2116271777", "name": "Hongbo Zhang"}, {"authorId": "1610927763", "name": "Dingjie Song"}, {"authorId": "2267160569", "name": "Wenya Xie"}, {"authorId": "2232952708", "name": "Chuyi Kong"}, {"authorId": "2130169642", "name": "Jianquan Li"}, {"authorId": "2101317304", "name": "Xiang Wan"}, {"authorId": "2218230700", "name": "Haizhou Li"}, {"authorId": "2267007505", "name": "Benyou Wang"}], "citations": [{"paperId": "dc0a6757dba4286a394815bbe002c81e231de25e", "title": "Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm"}, {"paperId": "acd1b8376173ff8f8715d918e7465257cbc6371a", "title": "Apollo: An Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "bd15acc371153925bf86924cb2d13114ca6c95f8", "title": "AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis"}, {"paperId": "e6e584fc07b6cc6a78cb465019871d751856ba08", "title": "Pushing The Limit of LLM Capacity for Text Classification"}, {"paperId": "dabf7edde0efb9b1e092aa27847a547cf2961192", "title": "Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback"}]}
