{"paperId": "a531e9a328ad1488567fa68c15d5bf30bfb90c78", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval", "abstract": "The effectiveness of multi-stage text retrieval has been solidly demonstrated since before the era of pre-trained language models. However, most existing studies utilize models that predate recent advances in large language models (LLMs). This study seeks to explore potential improvements that state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning the latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise reranker (RankLLaMA) for both passage retrieval and document retrieval using the MS MARCO datasets. Our findings demonstrate that the effectiveness of large language models indeed surpasses that of smaller models. Additionally, since LLMs can inherently handle longer contexts, they can represent entire documents holistically, obviating the need for traditional segmenting and pooling strategies. Furthermore, evaluations on BEIR demonstrate that our RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model checkpoints from this study are available on HuggingFace.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-12", "journal": {"name": "ArXiv", "volume": "abs/2310.08319"}, "authors": [{"authorId": "2461713", "name": "Xueguang Ma"}, {"authorId": "145769448", "name": "Liang Wang"}, {"authorId": "2242947624", "name": "Nan Yang"}, {"authorId": "2257346447", "name": "Furu Wei"}, {"authorId": "2257085301", "name": "Jimmy Lin"}], "citations": [{"paperId": "7b0f35b63637a6b308da104ff5065e0fa22090f1", "title": "Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval"}, {"paperId": "d170eb391abaa1675bde709a9aaba692090616a0", "title": "Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers"}, {"paperId": "3f769e94008f8d365c24166ff32e050cb2cbed3a", "title": "DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation"}, {"paperId": "f9c4c6c804930f751debe29780e9741315595aa8", "title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models"}, {"paperId": "77e07e5542b450a2ee3193993c552700ad9ba82d", "title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions"}, {"paperId": "9111d6632e3ad648e65c57c52fd945641ccbdac2", "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"}, {"paperId": "8c4b0f3f69e1ed36f371d91dfab7e0d9f909311c", "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering"}, {"paperId": "4ddce69eb9f77d2c1cf8a4411ca203761f54c3e0", "title": "Query Augmentation by Decoding Semantics from Brain Signals"}, {"paperId": "811df90786e5344a406964058bed01889554c1f9", "title": "Self-Retrieval: Building an Information Retrieval System with One Large Language Model"}, {"paperId": "a3c4047f82d7120e2bbc600dbe79b930e1dc4e41", "title": "Repetition Improves Language Model Embeddings"}, {"paperId": "bedfd672b897bd70201a5edb3f16c47b7661b4bf", "title": "INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models"}, {"paperId": "ad6fffaa1c3967f9a08b96b22aa66eba2797b7aa", "title": "Assessing generalization capability of text ranking models in Polish"}, {"paperId": "991c2164c85a80034afeeeee88b3208da766e76f", "title": "BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives"}, {"paperId": "0acb1b219c6c25a6d23e4267a061789a43ddd16b", "title": "Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation"}, {"paperId": "bc26a46e2c01bc4e16273d10b0f7d491d6e522d8", "title": "INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning"}, {"paperId": "d47bb3997a81e904ec696aa05bf90bfd7c22f47c", "title": "MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models"}, {"paperId": "88ca4ee548d07263386ca8e4effc4a001bb2716f", "title": "Improving Text Embeddings with Large Language Models"}, {"paperId": "4267ba44ced8e2c6afa39547aa6657aef62502e1", "title": "Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models"}, {"paperId": "daa475da0602008b72aa61a369fd8af1ebfadbfa", "title": "Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages"}, {"paperId": "d5d77b9c9e3312edd64a82f2384b328a65a2a0d3", "title": "Making Large Language Models A Better Foundation For Dense Retrieval"}, {"paperId": "c47676d655d82904d5844015a1cac08bdab2ed44", "title": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models"}, {"paperId": "5605e5bbe71a6d52be0930865f1635ed8644dea8", "title": "RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!"}, {"paperId": "ba2a7d57998bdbb024b588bc137964154329aa1a", "title": "End-to-End Retrieval with Learned Dense and Sparse Representations Using Lucene"}, {"paperId": "1445349b681b91401c630e42d3f7d5f9a6672a17", "title": "TSRankLLM: A Two-Stage Adaptation of LLMs for Text Ranking"}, {"paperId": "35bdba6f41e5432b3e80f4e5f86d1c7de6219dd4", "title": "Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers"}, {"paperId": "0b220041eb83c23b7b10d32a5d08c0309d528071", "title": "Large Language Models for Information Retrieval: A Survey"}]}
