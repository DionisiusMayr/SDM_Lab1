{"paperId": "b6887a0b4747a9fef77c877c4a56dc03d9de0eba", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Learned Autoscaling for Cloud Microservices with Multi-Armed Bandits", "abstract": "As cloud applications shift from monolithic architectures to loosely coupled microservices, several challenges in resource management arise. Application developers are tasked with determining compute capacity needed for each microservice in an application. This allocation dictates both the cost and performance of the application and typically relies on using either machine utilization (e.g. CPU, RAM) metrics. While utilization based policies are often simple to configure, easily understood, and require no training or retraining cost such policies offer no guarantees or expectations of end user latency. We design, implement and evaluate a microservice autoscaling system, COLA, which efficiently learns to manage cluster resources based on user provided end-to-end latency targets and cost objectives rather than optimizing utilization metrics. Our approach, COLA, relies on training a contextual multi armed bandit on representative workloads for an application and uses techniques to generalize performance to unseen workloads. We evaluate workloads of varying complexity including those with a fixed rate, diurnal pattern and dynamic request distribution. Across a set of five open-source microservice applications, we compare COLA against a variety of utilization and machine learning baselines. We find COLA provides the most cost effective autoscaling solution for a desired median or tail latency target on 13 of 18 workloads. On average, clusters managed by COLA cost 25.1% fewer dollars than the next closest alternative that meets a specified target latency. We discuss several optimizations, inspired by systems and machine learning literature, we make during training to efficiently explore the space of possible microservice configurations. These optimizations enable us to train our models over the course of a few hours. The cost savings from managing a cluster with COLA result in the system paying for its own training cost within a few days.", "venue": "arXiv.org", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"name": "ArXiv", "volume": "abs/2112.14845"}, "authors": [{"authorId": "2106768483", "name": "Vighnesh Sachidananda"}, {"authorId": "39118448", "name": "Anirudh Sivaraman"}], "citations": [{"paperId": "6e55491af14c3685f894f6ca3b0031811524380f", "title": "\u03bcConAdapter: Reinforcement Learning-based Fast Concurrency Adaptation for Microservices in Cloud"}, {"paperId": "6259ad4d0992252b72b0e6af0d19b03ddcba6d9e", "title": "Blueprint: A Toolchain for Highly-Reconfigurable Microservice Applications"}, {"paperId": "38ff3a01e30d2244d9439d3c7c68533e195310fb", "title": "Intelligent microservices autoscaling module using reinforcement learning"}, {"paperId": "0d2b32f8aec73f09a2d5d1e2f77b2e941ebf7002", "title": "Conditionally Risk-Averse Contextual Bandits"}, {"paperId": "564b5b241d60e76ed5fd7fbff972840880bdd0a0", "title": "Graph-PHPA: Graph-based Proactive Horizontal Pod Autoscaling for Microservices using LSTM-GNN"}]}
