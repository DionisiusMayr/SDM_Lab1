{"paperId": "d48eb2400161a2ceb1d787be580dc20f61943d73", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning", "abstract": "Instruction tuning has proven essential for enhancing the performance of large language models (LLMs) in generating human-aligned responses. However, collecting diverse, high-quality instruction data for tuning poses challenges, particularly in privacy-sensitive domains. Federated instruction tuning (FedIT) has emerged as a solution, leveraging federated learning from multiple data owners while preserving privacy. Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks. To address these issues, we propose a novel federated algorithm, FedPIT, which utilizes LLMs' in-context learning capability to self-generate task-specific synthetic data for training autonomously. Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks. Extensive experiments on real-world medical data demonstrate the effectiveness of FedPIT in improving federated few-shot performance while preserving privacy and robustness against data heterogeneity.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-03-10", "journal": {"name": "ArXiv", "volume": "abs/2403.06131"}, "authors": [{"authorId": "2197470934", "name": "Zhuo Zhang"}, {"authorId": "2290867161", "name": "Jingyuan Zhang"}, {"authorId": "2290984599", "name": "Jintao Huang"}, {"authorId": "2290739779", "name": "Lizhen Qu"}, {"authorId": "2290860069", "name": "Hongzhi Zhang"}, {"authorId": "2290852202", "name": "Zenglin Xu"}], "citations": []}
