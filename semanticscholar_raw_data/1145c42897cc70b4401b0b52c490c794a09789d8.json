{"paperId": "1145c42897cc70b4401b0b52c490c794a09789d8", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "A Survey on Knowledge Editing of Neural Networks", "abstract": "Deep neural networks are becoming increasingly pervasive in academia and industry, matching and surpassing human performance on a wide variety of fields and related tasks. However, just as humans, even the largest artificial neural networks make mistakes, and once-correct predictions can become invalid as the world progresses in time. Augmenting datasets with samples that account for mistakes or up-to-date information has become a common workaround in practical applications. However, the well-known phenomenon of catastrophic forgetting poses a challenge in achieving precise changes in the implicitly memorized knowledge of neural network parameters, often requiring a full model re-training to achieve desired behaviors. That is expensive, unreliable, and incompatible with the current trend of large self-supervised pre-training, making it necessary to find more efficient and effective methods for adapting neural network models to changing data. To address this need, knowledge editing is emerging as a novel area of research that aims to enable reliable, data-efficient, and fast changes to a pre-trained target model, without affecting model behaviors on previously learned tasks. In this survey, we provide a brief review of this recent artificial intelligence field of research. We first introduce the problem of editing neural networks, formalize it in a common framework and differentiate it from more notorious branches of research such as continuous learning. Next, we provide a review of the most relevant knowledge editing approaches and datasets proposed so far, grouping works under four different families: regularization techniques, meta-learning, direct model editing, and architectural strategies. Finally, we outline some intersections with other fields of research and potential directions for future works.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-10-30", "journal": {"name": "ArXiv", "volume": "abs/2310.19704"}, "authors": [{"authorId": "2264297388", "name": "Vittorio Mazzia"}, {"authorId": "2221286748", "name": "Alessandro Pedrani"}, {"authorId": "2221287334", "name": "Andrea Caciolai"}, {"authorId": "2264282623", "name": "Kay Rottmann"}, {"authorId": "2264271132", "name": "Davide Bernardi"}], "citations": [{"paperId": "33c8910107f3fcb17d140cc88554652508ae3674", "title": "Detoxifying Large Language Models via Knowledge Editing"}, {"paperId": "184753b614b35cde3f2e221cea3bc60fe016d29e", "title": "Editing Conceptual Knowledge for Large Language Models"}, {"paperId": "0a88726548d4ab50c3b28f1ce839220997875cf6", "title": "InstructEdit: Instruction-based Knowledge Editing for Large Language Models"}, {"paperId": "0251bb95be75d472c8d5b873751615e7fe2feb1d", "title": "A Comprehensive Study of Knowledge Editing for Large Language Models"}]}
