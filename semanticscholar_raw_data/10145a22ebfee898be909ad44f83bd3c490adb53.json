{"paperId": "10145a22ebfee898be909ad44f83bd3c490adb53", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Balancing Specialized and General Skills in LLMs: The Impact of Modern Tuning and Data Strategy", "abstract": "This paper introduces a multifaceted methodology for fine-tuning and evaluating large language models (LLMs) for specialized monetization tasks. The goal is to balance general language proficiency with domain-specific skills. The methodology has three main components: 1) Carefully blending in-domain and general-purpose data during fine-tuning to achieve an optimal balance between general and specialized capabilities; 2) Designing a comprehensive evaluation framework with 45 questions tailored to assess performance on functionally relevant dimensions like reliability, consistency, and business impact; 3) Analyzing how model size and continual training influence metrics to guide efficient resource allocation during fine-tuning. The paper details the design, data collection, analytical techniques, and results validating the proposed frameworks. It aims to provide businesses and researchers with actionable insights on effectively adapting LLMs for specialized contexts. We also intend to make public the comprehensive evaluation framework, which includes the 45 tailored questions and their respective scoring guidelines, to foster transparency and collaboration in adapting LLMs for specialized tasks.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-07", "journal": {"name": "ArXiv", "volume": "abs/2310.04945"}, "authors": [{"authorId": "2021011947", "name": "Zhengwu Zhang"}, {"authorId": "2267276054", "name": "Chen Zheng"}, {"authorId": "2256980831", "name": "Da Tang"}, {"authorId": "2257036596", "name": "Ke Sun"}, {"authorId": "2257348528", "name": "Yukun Ma"}, {"authorId": "51214420", "name": "Yingtong Bu"}, {"authorId": "2257319328", "name": "Xun Zhou"}, {"authorId": "2257314969", "name": "Liang Zhao"}], "citations": [{"paperId": "9ddfb1583ce7f5370ace2751bb5f260fa4af1961", "title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF"}, {"paperId": "4f67023517ed88b744f773ce421d3e3f80353b12", "title": "Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data"}, {"paperId": "2b28e170adf5cc68adcd158f0ba6dc8f810394d4", "title": "ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation"}, {"paperId": "a88d5fb82358d19d0daed7d35d49f03ea6796b57", "title": "Large Language Models in Cybersecurity: State-of-the-Art"}, {"paperId": "4d6b87770c60e7f89157a6dce9818a8112c958a9", "title": "A Comprehensive Evaluation of Large Language Models in Mining Gene Interactions and Pathway Knowledge"}, {"paperId": "798ece3c5491f613e5368bd2d818476a64b88905", "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers"}, {"paperId": "6c4582d2d60d7f82122b237943501502282f625c", "title": "Open-ended Commonsense Reasoning with Unrestricted Answer Scope"}]}
