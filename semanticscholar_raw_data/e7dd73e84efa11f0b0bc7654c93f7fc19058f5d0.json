{"paperId": "e7dd73e84efa11f0b0bc7654c93f7fc19058f5d0", "publicationVenue": {"id": "d4610af5-85e0-480b-8773-5c71d92a7b99", "name": "International Conference on Architectural Support for Programming Languages and Operating Systems", "type": "conference", "alternate_names": ["ASPLOS", "Int Conf Archit Support Program Lang Oper Syst", "Archit Support Program Lang Oper Syst", "Architectural Support for Programming Languages and Operating Systems"], "url": "http://www.acm.org/sigplan/"}, "title": "Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training", "abstract": "Distributed deep learning training usually adopts All-Reduce as the synchronization mechanism for data parallel algorithms due to its high performance in homogeneous environment. However, its performance is bounded by the slowest worker among all workers. For this reason, it is significantly slower in heterogeneous settings. AD-PSGD, a newly proposed synchronization method which provides numerically fast convergence and heterogeneity tolerance, suffers from deadlock issues and high synchronization overhead. Is it possible to get the best of both worlds --- designing a distributed training method that has both high performance like All-Reduce in homogeneous environment and good heterogeneity tolerance like AD-PSGD? In this paper, we propose Prague, a high-performance heterogeneity-aware asynchronous decentralized training approach. We achieve the above goal with intensive synchronization optimization by exploring the interplay between algorithm and system implementation, or statistical and hardware efficiency. To reduce synchronization cost, we propose a novel communication primitive, Partial All-Reduce, that enables fast synchronization among a group of workers. To reduce serialization cost, we propose static group scheduling in homogeneous environment and simple techniques, i.e., Group Buffer and Group Division, to largely eliminate conflicts with slightly reduced randomness. Our experiments show that in homogeneous environment, Prague is 1.2x faster than the state-of-the-art implementation of All-Reduce, 5.3x faster than Parameter Server and 3.7x faster than AD-PSGD. In a heterogeneous setting, Prague tolerates slowdowns well and achieves 4.4x speedup over All-Reduce.", "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2020-03-09", "journal": {"name": "Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems"}, "authors": [{"authorId": "31343581", "name": "Qinyi Luo"}, {"authorId": "151050884", "name": "Jiaao He"}, {"authorId": "49501910", "name": "Youwei Zhuo"}, {"authorId": "2288203548", "name": "Xuehai Qian"}], "citations": [{"paperId": "2712a7c0a8275bd0db91a61790a9e7a7aa7e74b8", "title": "HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis"}, {"paperId": "b37f3cf718c119b36474b7776e6735ddbc7e38c9", "title": "Heter-Train: A Distributed Training Framework Based on Semi-Asynchronous Parallel Mechanism for Heterogeneous Intelligent Transportation Systems"}, {"paperId": "a727d8e834f779cebdc4e5b6d76659f864b3f5be", "title": "Distributed Training of Large Language Models"}, {"paperId": "2bc05a09e6aa691016f38e0d8dc6f245d29dc32f", "title": "Resource-Aware Federated Hybrid Profiling for Edge Node Selection in Federated Patient Similarity Network"}, {"paperId": "203278b671b1894d603451b41017e2ad227a3bd3", "title": "FusionFlow: Accelerating Data Preprocessing for Machine Learning with CPU-GPU Cooperation"}, {"paperId": "59abd3a23d0ab6290d54da9fa34d8573b9e0eb9b", "title": "vTrain: A Simulation Framework for Evaluating Cost-effective and Compute-optimal Large Language Model Training"}, {"paperId": "0f9ded926e889e35695e2028a059c96476fc8daa", "title": "Asynchronous SGD on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization"}, {"paperId": "70f23d12b37a9436ebde16e99d9fdd540ef5c7b6", "title": "Dynamic Worker Classification Scheme for Addressing Straggler Problem in Distributed Deep Learning Environments"}, {"paperId": "b8913ddb4652158e7fe523a9f42a2254796dca52", "title": "Convergence Analysis of Decentralized ASGD"}, {"paperId": "83d60df5c515be6a10b71fb615c224f6ea94ab6e", "title": "PipePar: Enabling fast DNN pipeline parallel training in heterogeneous GPU clusters"}, {"paperId": "76ebc99fd8d22b63b1695d399642358fc0cbaae8", "title": "Robust Fully-Asynchronous Methods for Distributed Training over General Architecture"}, {"paperId": "a8059c10fb5c16aa1afb5f1263c4f4a1cd689595", "title": "Stability-Based Generalization Analysis of the Asynchronous Decentralized SGD"}, {"paperId": "381b3f7c1b0dc188906c2db4b6b823ddeb4f8be5", "title": "Straggler-Resilient Decentralized Learning via Adaptive Asynchronous Updates"}, {"paperId": "8e47b44f65dbbebfa671468a3548b99e63c2c40f", "title": "Delay-agnostic Asynchronous Coordinate Update Algorithm"}, {"paperId": "e61462184a6dce9259e76c9069c5747a420b3c0d", "title": "SDPipe: A Semi-Decentralized Framework for Heterogeneity-aware Pipeline-parallel Training"}, {"paperId": "3995c7a3704db252cfcce18ef56562a89f4cf693", "title": "ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning"}, {"paperId": "d5fe97309afdf0da633e04b5da4212a054661ecf", "title": "Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs"}, {"paperId": "50dbc4e4a9d3d74fc7ba0ae1e4dfcdf1ac3c5cde", "title": "Addressing Straggler Problem Through Dynamic Partial All-Reduce for Distributed Deep Learning in Heterogeneous GPU Clusters"}, {"paperId": "b92278ee33a6147bbf78255ffdb7bc81cf38d0c1", "title": "Adaptive Configuration for Heterogeneous Participants in Decentralized Federated Learning"}, {"paperId": "5c3eb5898f5e4e42c8fd94ab921b79e8a0b63485", "title": "Peer-to-peer deep learning with non-IID data"}, {"paperId": "08c9200e6bc8e2cfe380fafa577e06d96b952e9f", "title": "SWIFT: Rapid Decentralized Federated Learning via Wait-Free Model Communication"}, {"paperId": "56b597a0b3adf8ebe83bac08b4d6d77bd5164c81", "title": "Accelerating Vertical Federated Learning"}, {"paperId": "f13a1911eb59b8529c38292e029af9f3e5fc7905", "title": "GBA: A Tuning-free Approach to Switch between Synchronous and Asynchronous Training for Recommendation Model"}, {"paperId": "74cb59f7980fd298da53e46d95a4c2319be3bb07", "title": "Fast Parameter Synchronization for Distributed Learning with Selective Multicast"}, {"paperId": "39271db2458f6fb635e40372049cb1e203c7e05c", "title": "SANCUS: Staleness-Aware Communication-Avoiding Full-Graph Decentralized Training in Large-Scale Graph Neural Networks"}, {"paperId": "cbd1588837e6c6b795d593f0269d92f295c09705", "title": "Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems"}, {"paperId": "0dab58e476f3f0e6f580a295f7c4756c86f1f198", "title": "FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models"}, {"paperId": "8617b86ee2dc2203d03dd47124dc1b991df13e78", "title": "IceBreaker: warming serverless functions better with heterogeneity"}, {"paperId": "b0f5a538a7a08f82a53c1b2618a0ee0e44939cd3", "title": "p2pGNN: A Decentralized Graph Neural Network for Node Classification in Peer-to-Peer Networks"}, {"paperId": "87b6eff7ef8aa498e7e0a640ce50f876707aebb2", "title": "BlueFog: Make Decentralized Algorithms Practical for Optimization and Deep Learning"}, {"paperId": "3742cffab506ab6eb6e5e25e4a248ad13a9e74e2", "title": "Accelerating Gossip-Based Deep Learning in Heterogeneous Edge Computing Platforms"}, {"paperId": "bb5a6ccae9d2520aa97e014b7d769894b95616f8", "title": "Heterogeneity-Aware Distributed Machine Learning Training via Partial Reduce"}, {"paperId": "418f532effc3c18b22556b66f094dd4508b00fb1", "title": "Enabling energy-efficient DNN training on hybrid GPU-FPGA accelerators"}, {"paperId": "42da19a0537bcdc0d59eee3870eae7428b342b62", "title": "Accelerating Gossip SGD with Periodic Global Averaging"}, {"paperId": "055c7147829bde1a56fff853d18926928772b171", "title": "Near-Optimal Topology-adaptive Parameter Synchronization in Distributed DNN Training"}, {"paperId": "9540e0ae29bcbbf10aac4585030b423e11163fe4", "title": "DecentLaM: Decentralized Momentum SGD for Large-batch Deep Training"}, {"paperId": "0511d73d03fa736bf8ff48fc00bb3d98b9862d90", "title": "Addressing the Heterogeneity of A Wide Area Network for DNNs"}, {"paperId": "2f2fc085741d91a99edf37670d7951d60feee105", "title": "Mitigating Stragglers in the Decentralized Training on Heterogeneous Clusters"}, {"paperId": "3399ccaf78b9f2414798b4c85e68e485e847181d", "title": "Optimizing distributed training deployment in heterogeneous GPU clusters"}, {"paperId": "ea8b5587224f0fb23f1ff1f6e68d63299ce4d3c6", "title": "A Dynamic Scaling Scheme of Cloud-based DNN Training Clusters"}, {"paperId": "aba203e353d5f3300cfe21ff1111170c53b303d9", "title": "Collage Inference: Using Coded Redundancy for Lowering Latency Variation in Distributed Image Classification Systems"}, {"paperId": "1dcd3ce2221eff4213487249978f7ec844f1c611", "title": "Communication-efficient Decentralized Machine Learning over Heterogeneous Networks"}, {"paperId": "9295fc01d09d39f6e012059fb2213d1a06fdf56e", "title": "DLion: Decentralized Distributed Deep Learning in Micro-Clouds"}, {"paperId": "6907fde39248fd86cb9efd0a79a1d8cc7f85b3f1", "title": "Systematic Literature Review on Cost-Efficient Deep Learning"}, {"paperId": "618ad7369702c2de1743b7e633762324562246fb", "title": "Scale-Train: A Scalable DNN Training Framework for a Heterogeneous GPU Cloud"}, {"paperId": "d80e74866169e335e341641d4cbc1b7cb8113068", "title": "Network-Aware Distributed Machine Learning Over Wide Area Network"}, {"paperId": "96c23fbe3dcf55151ef44686f8135df41712b65d", "title": "Communication Scheduling for Gossip SGD in a Wide Area Network"}, {"paperId": "60c00529b1ddfaf8af8ca900eb87f06d1d3ca995", "title": "Resource-Aware Federated Hybrid Pro\ufb01ling for Edge Node Selection in Federated Patient Similarity Network"}]}
