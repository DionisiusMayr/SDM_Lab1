{"paperId": "528ff02f6d6505bd121107edd69ab9c777a53d7e", "publicationVenue": {"id": "e4f51561-5050-4b9c-87c2-c49957677fbf", "name": "European Conference on Computer Systems", "type": "conference", "alternate_names": ["Eur Conf Comput Syst", "EuroSys"], "url": "http://www.eurosys.org/"}, "title": "Accelerating graph sampling for graph machine learning using GPUs", "abstract": "Representation learning algorithms automatically learn the features of data. Several representation learning algorithms for graph data, such as DeepWalk, node2vec, and Graph-SAGE, sample the graph to produce mini-batches that are suitable for training a DNN. However, sampling time can be a significant fraction of training time, and existing systems do not efficiently parallelize sampling. Sampling is an \"embarrassingly parallel\" problem and may appear to lend itself to GPU acceleration, but the irregularity of graphs makes it hard to use GPU resources effectively. This paper presents NextDoor, a system designed to effectively perform graph sampling on GPUs. NextDoor employs a new approach to graph sampling that we call transit-parallelism, which allows load balancing and caching of edges. NextDoor provides end-users with a high-level abstraction for writing a variety of graph sampling algorithms. We implement several graph sampling applications, and show that NextDoor runs them orders of magnitude faster than existing systems.", "venue": "European Conference on Computer Systems", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2020-09-15", "journal": {"name": "Proceedings of the Sixteenth European Conference on Computer Systems"}, "authors": [{"authorId": "3258214", "name": "Abhinav Jangda"}, {"authorId": "1944118877", "name": "Sandeep Polisetty"}, {"authorId": "2100712", "name": "Arjun Guha"}, {"authorId": "143665011", "name": "M. Serafini"}], "citations": [{"paperId": "e467d487035f95962af39a8f7d21eccfccf9faa3", "title": "gSWORD: GPU-accelerated Sampling for Subgraph Counting"}, {"paperId": "48c35e8cfa7ac7e24c448fb21154151097e4f184", "title": "Celeritas: Out-of-Core Based Unsupervised Graph Neural Network via Cross-Layer Computing 2024"}, {"paperId": "69ee8f1f98da4847b3d7ffc9f04a68492873588b", "title": "Enhancing Graph Random Walk Acceleration via Efficient Dataflow and Hybrid Memory Architecture"}, {"paperId": "7e55d5b5630ea4e90cb6187a58a9e67465242aec", "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning"}, {"paperId": "85691014466d48647a01d8726c68d0e9258fc667", "title": "Single-GPU GNN Systems: Traps and Pitfalls"}, {"paperId": "2d082860ad89c450584010ad0312d8dc60205c1c", "title": "An adaptive graph sampling framework for graph analytics"}, {"paperId": "d10cabff491718f2b5a94b8e2d9c2a6d44aebe24", "title": "GNNFlow: A Distributed Framework for Continuous Temporal GNN Learning on Dynamic Graphs"}, {"paperId": "f0928b606aff3f02bc0724a38678d5cc54f2ceb4", "title": "NeutronOrch: Rethinking Sample-based GNN Training under CPU-GPU Heterogeneous Environments"}, {"paperId": "2f3049d3dfa761c94a54168388d9364f6e1a70ab", "title": "Distributed Matrix-Based Sampling for Graph Neural Network Training"}, {"paperId": "96a88170729021e553648a2b17c73602a3171d2f", "title": "gSampler: General and Efficient GPU-based Graph Sampling for Graph Learning"}, {"paperId": "f0f84ea6f9d0c417614d205827fecbc238f82a58", "title": "TurboGNN: Improving the End-to-End Performance for Sampling-Based GNN Training on GPUs"}, {"paperId": "51a72559c6ccdda3757c0eec1e2ea782a151d1b6", "title": "Sampling unknown large networks restricted by low sampling rates"}, {"paperId": "bbe3548b34cbe0a45f331cd81b912f4b9703a657", "title": "PGLBox: Multi-GPU Graph Learning Framework for Web-Scale Recommendation"}, {"paperId": "5acf75e1ee88a9695230708c3d6b11512583fefa", "title": "DUCATI: A Dual-Cache Training System for Graph Neural Networks on Giant Graphs with the GPU"}, {"paperId": "fa1e8a10ec22c5177a0290d4877d150a9b36dafe", "title": "The Evolution of Distributed Systems for Graph Neural Networks and Their Origin in Graph Processing and Deep Learning: A Survey"}, {"paperId": "a2769c9e722099691aefed9c2731c366a80293b8", "title": "Quiver: Supporting GPUs for Low-Latency, High-Throughput GNN Serving with Workload Awareness"}, {"paperId": "85cec0991826ffb4f44fcd11d56c4c2b1356e64c", "title": "Communication Optimization for Distributed Execution of Graph Neural Networks"}, {"paperId": "44775b43d0f55d0d8b94beb1fe8b76ad64270cbb", "title": "LightTraffic: On Optimizing CPU-GPU Data Traffic for Efficient Large-scale Random Walks"}, {"paperId": "58d8de4a46396edfe771f30722b9476a30ad22d8", "title": "FastRW: A Dataflow-Efficient and Memory-Aware Accelerator for Graph Random Walk on FPGAs"}, {"paperId": "231b88e974b3117fbbeade9fa4f758ed4f15369b", "title": "NosWalker: A Decoupled Architecture for Out-of-Core Random Walk Processing"}, {"paperId": "81d9cd7fd934f218d3428fc70fc5b8940c0a1107", "title": "GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism"}, {"paperId": "a89f2cb72006e9ddc3a1cec6486a82c923777798", "title": "Distributed Graph Embedding with Information-Oriented Random Walks"}, {"paperId": "5b671f29e7830283d983a7f18f745b12abd490f8", "title": "DSP: Efficient GNN Training with Multiple GPUs"}, {"paperId": "656a69cc79dfe67dcee2441e19ec44b43df938b4", "title": "Scalable Neural Network Training over Distributed Graphs"}, {"paperId": "46210e170045df3c0c50a17bb63e6de480d62f9d", "title": "FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training"}, {"paperId": "7ed5227dfa8026f57bad09ff28d706ca39669b6b", "title": "Machine learning for enterprise modeling assistance: an investigation of the potential and proof of concept"}, {"paperId": "1e79e33c77b2d8eaf643af0e1f5003057d7356b2", "title": "Distributed Graph Neural Network Training: A Survey"}, {"paperId": "533abb048e50d2374de17684c6ef6cb48e24225a", "title": "Characterizing the Efficiency of Graph Neural Network Frameworks with a Magnifying Glass"}, {"paperId": "e2b4fe32e257d859e02abbf1f2677b7662cb82f3", "title": "WholeGraph: A Fast Graph Neural Network Training Framework with Multi-GPU Distributed Shared Memory Architecture"}, {"paperId": "4d468c7b4345fcd25d40ca67313a743e6e164444", "title": "HGL: Accelerating Heterogeneous GNN Training with Holistic Representation and Optimization"}, {"paperId": "be5ae5f98ae4b16d14b99d5c3db2b86173382f28", "title": "Scalable Graph Sampling on GPUs with Compressed Graph"}, {"paperId": "e5adcef364120aaee7a80b7230ee5cb3c32dc674", "title": "Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU"}, {"paperId": "e4b81c6d782d4da6f4168298f4861f410a7a4f9f", "title": "Fograph: Enabling Real-Time Deep Graph Inference with Fog Computing"}, {"paperId": "740b9d4414a955a74c16f5f3617d2b5dde2e9adf", "title": "GNNLab: a factored system for sample-based GNN training over GPUs"}, {"paperId": "6a62ca8c617656caef26a31b13e1cadab6e996e3", "title": "Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations"}, {"paperId": "6a5637e617f2ef97fd8f059d6fcb46c567fd08dd", "title": "PaSca: A Graph Neural Architecture Search System under the Scalable Paradigm"}, {"paperId": "36b7419166e470d65a84dad4ce4aa7857d7573fe", "title": "MariusGNN: Resource-Efficient Out-of-Core Training of Graph Neural Networks"}, {"paperId": "1df8b8cc125f667c6495b76e347da621109b0f73", "title": "ByteGNN: Efficient Graph Neural Network Training at Large Scale"}, {"paperId": "8d9d275585601a234f646b8a533bd66557e402f9", "title": "Hardware/Software Co-Programmable Framework for Computational SSDs to Accelerate Deep Learning Service on Large-Scale Graphs"}, {"paperId": "94f0823f8db5360972a7a68b453e28ddf9c4e992", "title": "BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing"}, {"paperId": "9a50209c897274a1335fcfeabc10d406174549a8", "title": "Random Walks on Huge Graphs at Cache Efficiency"}, {"paperId": "674819a81b1f27e65a4ea3de86bfdafb52612223", "title": "Scalable Graph Neural Network Training"}, {"paperId": "ca973af3058109cf10b6a37bbeebd132f6123dba", "title": "Marius++: Large-Scale Training of Graph Neural Networks on a Single Machine"}, {"paperId": "fe3d6c9a7e76a175ca90f7ce05beb9965f148f26", "title": "the Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation."}, {"paperId": "a7eef0588a5c17f6e5d98c33038759cb1ea62054", "title": "1 E cient Execution of Large-Scale Scienti c Computation on CPUs and GPUs"}, {"paperId": "384d8378ca5d1d83a119f9cd66bc2703f7e45509", "title": "Machine Learning-Based Enterprise Modeling Assistance: Approach and Potentials"}, {"paperId": "99e568d08ad43f1dc9e9ad3fe138e90e6d5bcefe", "title": "Wukong+G: Fast and Concurrent RDF Query Processing Using RDMA-assisted GPU Graph Exploration"}, {"paperId": "8314a77b72ba8cace6f4e4968261597889de22c1", "title": "20th USENIX"}, {"paperId": "febbe50d95e27e99c46a0d431dffd371203e57a4", "title": "This paper is included in the Proceedings of the 2023 USENIX Annual Technical Conference."}]}
