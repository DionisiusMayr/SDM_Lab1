{"paperId": "c0230760f644f6b7538d93e4296a5e9aa7028e45", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch", "abstract": "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio p And REscales the remaining ones by 1/(1 - p) to approximate the original embeddings. Then, we use DARE as a versatile plug-and-play technique to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.005) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them. (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. For instance, the amalgamation of WizardLM and WizardMath significantly enhances the GSM8K zero-shot accuracy of WizardLM from 2.2 to 66.3, retaining the instruction-following proficiency while surpassing WizardMath's 64.2 performance. Our merged LM also ranks first among models with 7 billion parameters on the Open LLM Leaderboard.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-06", "journal": {"name": "ArXiv", "volume": "abs/2311.03099"}, "authors": [{"authorId": "2265527327", "name": "Le Yu"}, {"authorId": "48613402", "name": "Yu Bowen"}, {"authorId": "46493167", "name": "Haiyang Yu"}, {"authorId": "2257407873", "name": "Fei Huang"}, {"authorId": "2253881638", "name": "Yongbin Li"}], "citations": [{"paperId": "e9074f72c156c504f67a74d1bc40d3090a2085b2", "title": "MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question Answering"}, {"paperId": "def1312b504778a26736c3dc5d7405bf634e7929", "title": "Have You Merged My Model? On The Robustness of Large Language Model IP Protection Methods Against Model Merging"}, {"paperId": "6ac9a1da483b3f19f97f9b8272676eb648905348", "title": "Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better"}, {"paperId": "a49f5b8d9c2731697163fe45a52f3ec9ba0e18eb", "title": "Checkpoint Merging via Bayesian Optimization in LLM Pretraining"}, {"paperId": "97352d95cc1fd9b7d9e959d61dd751a619975bfe", "title": "Arcee's MergeKit: A Toolkit for Merging Large Language Models"}, {"paperId": "828f98e0feba2baa55a5486f354fd074cca0880c", "title": "Evolutionary Optimization of Model Merging Recipes"}, {"paperId": "e044799b6ecae8ff0e1ea549a01dfa35ccac530e", "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning"}, {"paperId": "3cfa6f1bbfbfb7a0c58b11f8142f153630a4c17b", "title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data"}, {"paperId": "08920921ce6f4efe0dd92f6005a755b07d4ce760", "title": "DPPA: Pruning Method for Large Language Model to Model Merging"}, {"paperId": "30e33e9ec257a60d8f095a285f31c702511db04a", "title": "FuseChat: Knowledge Fusion of Chat Models"}, {"paperId": "4a3caeadc881e3ed20fb8585749cf7acfc3aa833", "title": "Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models"}, {"paperId": "1de5b29e746bb33113b80a0997ef61a73ce70d78", "title": "Model Composition for Multimodal Large Language Models"}, {"paperId": "d9cfa9d7dabd39b66a8c11e5dde69ba45e91093d", "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic"}, {"paperId": "a8c422a624ad846ac4d2074a98645d9eb2f364a8", "title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models"}, {"paperId": "7d00e7337fbbccd0208361b4c204c75239a96521", "title": "LaCo: Large Language Model Pruning via Layer Collapse"}, {"paperId": "13b8934468665ecb586f491d7f9f6c460cb095e5", "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains"}, {"paperId": "a624041a028404e7d1fdb40987b1780ce4f1c842", "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit"}, {"paperId": "35cbc2f146f054ace0a113d75162f386c8aed9fd", "title": "Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm"}, {"paperId": "9a4f03d132869db4a05c6789a4a1076575e2258b", "title": "Representation Surgery for Multi-Task Model Merging"}, {"paperId": "4a776e757e46c0c41020cb15e574a62aceb20cf5", "title": "Instructional Fingerprinting of Large Language Models"}, {"paperId": "19b4072e600c3577d277da39ac6cbb36cf3996e1", "title": "Marrying Adapters and Mixup to Efficiently Enhance the Adversarial Robustness of Pre-Trained Language Models for Text Classification"}, {"paperId": "192028b7f0b920ce19cedc2dadc5992d71bc8ff6", "title": "Concrete Subspace Learning based Interference Elimination for Multi-task Model Fusion"}, {"paperId": "dff9b29918369f2ce7c06d13258ffad5c644788a", "title": "ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization"}, {"paperId": "f284407feaf17923e1aff838b9b39fb10c74f1ed", "title": "LM-Cocktail: Resilient Tuning of Language Models via Model Merging"}]}
