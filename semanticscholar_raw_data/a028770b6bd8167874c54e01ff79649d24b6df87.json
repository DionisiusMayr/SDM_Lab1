{"paperId": "a028770b6bd8167874c54e01ff79649d24b6df87", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "T-FREX: A Transformer-based Feature Extraction Method from Mobile App Reviews", "abstract": "Mobile app reviews are a large-scale data source for software-related knowledge generation activities, including software maintenance, evolution and feedback analysis. Effective extraction of features (i.e., functionalities or characteristics) from these reviews is key to support analysis on the acceptance of these features, identification of relevant new feature requests and prioritization of feature development, among others. Traditional methods focus on syntactic pattern-based approaches, typically context-agnostic, evaluated on a closed set of apps, difficult to replicate and limited to a reduced set and domain of apps. Meanwhile, the pervasiveness of Large Language Models (LLMs) based on the Transformer architecture in software engineering tasks lays the groundwork for empirical evaluation of the performance of these models to support feature extraction. In this study, we present T-FREX, a Transformer-based, fully automatic approach for mobile app review feature extraction. First, we collect a set of ground truth features from users in a real crowdsourced software recommendation platform and transfer them automatically into a dataset of app reviews. Then, we use this newly created dataset to fine-tune multiple LLMs on a named entity recognition task under different data configurations. We assess the performance of T-FREX with respect to this ground truth, and we complement our analysis by comparing T-FREX with a baseline method from the field. Finally, we assess the quality of new features predicted by T-FREX through an external human evaluation. Results show that T-FREX outperforms on average the traditional syntactic-based method, especially when discovering new features from a domain for which the model has been fine-tuned.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2024-01-08", "journal": {"name": "ArXiv", "volume": "abs/2401.03833"}, "authors": [{"authorId": "147863356", "name": "Quim Motger"}, {"authorId": "66363217", "name": "Alessio Miaschi"}, {"authorId": "2187131", "name": "F. Dell\u2019Orletta"}, {"authorId": "2269465541", "name": "Xavier Franch"}, {"authorId": "144321819", "name": "Jordi Marco"}], "citations": []}
