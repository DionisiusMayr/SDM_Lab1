{"paperId": "8b3a67c7e5289eed160d2acfd04d71cfb552c67d", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models", "abstract": "We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.", "venue": "arXiv.org", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-08-05", "journal": {"name": "ArXiv", "volume": "abs/2208.03306"}, "authors": [{"authorId": "2118481100", "name": "Margaret Li"}, {"authorId": "40895369", "name": "Suchin Gururangan"}, {"authorId": "3239480", "name": "Tim Dettmers"}, {"authorId": "35084211", "name": "M. Lewis"}, {"authorId": "1745524", "name": "Tim Althoff"}, {"authorId": "144365875", "name": "Noah A. Smith"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}], "citations": [{"paperId": "c67c4c81beed122d7f94580d8816a6dc68867ec4", "title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models"}, {"paperId": "0bd4a6f0701dd0f9f936730468699a7e7d920172", "title": "Robust Concept Erasure Using Task Vectors"}, {"paperId": "1bda8efbbf4abae6c8c1da97d6137396807b1e09", "title": "ReFT: Representation Finetuning for Language Models"}, {"paperId": "7939096322e1d4585113634b70eade1606da89b6", "title": "DiPaCo: Distributed Path Composition"}, {"paperId": "07894aeadab9158fdb97647c4792816ede1b60b9", "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"}, {"paperId": "505a111eb7b8c1e82289d40d4faef2d323d725f6", "title": "FAX: Scalable and Differentiable Federated Primitives in JAX"}, {"paperId": "303b4ae0e53cca8d007076778e5c17e28116ff7e", "title": "Learning to Decode Collaboratively with Multiple Language Models"}, {"paperId": "d9cfa9d7dabd39b66a8c11e5dde69ba45e91093d", "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic"}, {"paperId": "2faad1112335ff99d1cb7967a51ac977fc9e6804", "title": "Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning"}, {"paperId": "35cbc2f146f054ace0a113d75162f386c8aed9fd", "title": "Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm"}, {"paperId": "f951238e4f81fad89e9848c0e099da1d1056b652", "title": "InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks"}, {"paperId": "c0a30b378bf897412426ba28e65c6392a65859bc", "title": "What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition"}, {"paperId": "67f03ac399693393116076c0b8ec8ea05b910685", "title": "WARM: On the Benefits of Weight Averaged Reward Models"}, {"paperId": "8c1243e089621d09025e1e51e8e01cb2cb20eabf", "title": "Knowledge Fusion of Large Language Models"}, {"paperId": "e4b757235fdc51de0e67cce47e9b4c3a5cab551c", "title": "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models"}, {"paperId": "36c5d94a9856e9ccd86f51551b9b432fdad3b61f", "title": "Hazards from Increasingly Accessible Fine-Tuning of Downloadable Foundation Models"}, {"paperId": "3f081ee658a08d0b3ccc6358c85e618d05f74eb3", "title": "Time is Encoded in the Weights of Finetuned Language Models"}, {"paperId": "1a3f7e23ef8f0bf06d0efa0dc174e4e361226ead", "title": "Paloma: A Benchmark for Evaluating Language Model Fit"}, {"paperId": "5cba06e72c842b503f2de958d4457ffda88a675f", "title": "OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers"}, {"paperId": "ea88649be9fbc54d49833edcf8903df21afb5f6f", "title": "Leveraging Function Space Aggregation for Federated Learning at Scale"}, {"paperId": "d5da19399500a4b8728409237aa2608e47a4654f", "title": "Token-Level Adaptation of LoRA Adapters for Downstream Task Generalization"}, {"paperId": "e118080d723a62560b56c91c78986b5da6aaf21f", "title": "Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization"}, {"paperId": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31", "title": "DiLoCo: Distributed Low-Communication Training of Language Models"}, {"paperId": "9d85ea26518bace986a2f7cdffa24edad2b20c87", "title": "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion"}, {"paperId": "61dfb47883a41745760fa714f8ef29bcb61e7e8a", "title": "CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model"}, {"paperId": "9bf00afb0efb02a263fa3ddea1e768677498536c", "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"}, {"paperId": "2fc229bfe561f42aae6f3bb84598cfa5737a8b6a", "title": "Approximating Two-Layer Feedforward Networks for Efficient Transformers"}, {"paperId": "eac59779da7262968a9043985e7cd933c00247a5", "title": "MatFormer: Nested Transformer for Elastic Inference"}, {"paperId": "128217c0d1e99912ebc727c84686cc97a913b55f", "title": "Deep Model Fusion: A Survey"}, {"paperId": "cfb7948c8a09d0a64afecceb7efe3362318dbe17", "title": "SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore"}, {"paperId": "c1104befbe6b4ded6274ccca28750b20982bbcb5", "title": "Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication"}, {"paperId": "ed3101df8bd7b0addfb9ea8713ddb590a15461a2", "title": "Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness"}, {"paperId": "83eb18cae8aa425c434bf712eb2860b807ce2062", "title": "FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts"}, {"paperId": "4353d3cb9206aa088bd00e5550d92712a0b08109", "title": "Revisiting Permutation Symmetry for Merging Models between Different Datasets"}, {"paperId": "0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8", "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"}, {"paperId": "7a816dd242c4c3f652a448dba54daa53f89a9e4f", "title": "Soft Merging of Experts with Adaptive Routing"}, {"paperId": "2651f0179874bd010f58d2c9fa7d118807c80977", "title": "TIES-Merging: Resolving Interference When Merging Models"}, {"paperId": "6c57ce08546a5f996d9c487522bd9ba1d67c9eac", "title": "Dataset Efficient Training with Model Ensembling"}, {"paperId": "0b3d3b58077ebe719930e75764d18ba296cd269b", "title": "RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths"}, {"paperId": "024a25b2445ecb3a181c5e2f39fbf8b73a4c1a6f", "title": "Emergent Modularity in Pre-trained Transformers"}, {"paperId": "7283d616e40d7ab7422e3697218f3fc42f292bf2", "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts"}, {"paperId": "49cbfa5848aaa74ce01d29ee5328f1a2b466ce27", "title": "David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs"}, {"paperId": "e55fff3d7e59a7192b59b4b497c17ea9c77a9d16", "title": "CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models"}, {"paperId": "634112108d463c608769c1740811f5e3754a27f5", "title": "Transferring Learning Trajectories of Neural Networks"}, {"paperId": "5e0d3c25d375d83f0d88bfc17614dde5943c10c3", "title": "Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models"}, {"paperId": "c17b71f31fa708eb01310ff65ab660f2676a12a1", "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models"}, {"paperId": "fc586657894b6573297a030c192d32764f752567", "title": "An Empirical Study of Multimodal Model Merging"}, {"paperId": "2fb84dcdfa68e31742e20ffff3f32a04ec85d3da", "title": "PopulAtion Parameter Averaging (PAPA)"}, {"paperId": "022d117ecfc97e6a6b06efa2782b8580de39a058", "title": "ERM++: An Improved Baseline for Domain Generalization"}, {"paperId": "464770587aece80cc9e3451050058e30c2aa6666", "title": "Scaling Expert Language Models with Unsupervised Domain Discovery"}, {"paperId": "7e3fb9c21b56ee1dfb2f79d545003fb525324e8b", "title": "Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies"}, {"paperId": "1ed4bdfe692bff67ada28f4883492169103d156c", "title": "Robust Weight Signatures: Gaining Robustness as Easy as Patching Weights?"}, {"paperId": "1f346f74e8eabececa4896d734ab9b261f30830d", "title": "Modular Deep Learning"}, {"paperId": "681cee58cf7e54199191cf9e0baf6851d8356704", "title": "Complex QA and language models hybrid architectures, Survey"}, {"paperId": "629bc57782bb4326a3eb5f89314e350729c5f417", "title": "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models"}, {"paperId": "4dc162f8a42d8138c7a523eb39df43db30e9876d", "title": "Knowledge is a Region in Weight Space for Fine-tuned Language Models"}, {"paperId": "86d03160e6f05deb17d0169e515f5a55d6361f7c", "title": "Exploring the Benefits of Training Expert Language Models over Instruction Tuning"}, {"paperId": "ac667aa10db56b1483640aefeca05f2e25d8353b", "title": "Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization"}, {"paperId": "3ff08b5ca57e786d8af7b204ef94c9972bd9a61e", "title": "Dataless Knowledge Fusion by Merging Weights of Language Models"}, {"paperId": "71ba5f845bd22d42003675b7cea970ca9e590bcc", "title": "Editing Models with Task Arithmetic"}, {"paperId": "656b5cb17d28c80fefb8c2f21e8f1f79b6dcd0dc", "title": "ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning"}, {"paperId": "9a6d83c836ce6389b526b941d971eee775aa573e", "title": "ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts"}, {"paperId": "5ecf91d9f3bcc5dee65e3a8ac1ab65dacf777de6", "title": "Lo-fi: Distributed Fine-tuning without Communication"}, {"paperId": "59fed7ca092c7e83583906456756abba8ce9295a", "title": "Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities"}, {"paperId": "ca086f4c09cf8de705830ac2b70951737fab93ca", "title": "A Review of Sparse Expert Models in Deep Learning"}, {"paperId": "8bd182f01c99e643c9a5a96832dc1a16b9cd10d0", "title": "Domain-Specific Text Generation for Machine Translation"}, {"paperId": "df818dcc5bd9405b50c377796ac0ed0295937125", "title": "Lowering the Pre-training Tax for Gradient-based Subset Training: A Lightweight Distributed Pre-Training Toolkit"}, {"paperId": "5770e77ff6350c7bb3b20a3a678c0e47b22ccf8e", "title": "Retrieval-Augmented Domain Adaptation of Language Models"}, {"paperId": "ad8deea9e0ba540579000f19cf5f51ff14ef6476", "title": "Fed-ZERO: Efficient Zero-shot Personalization with Federated Mixture of Experts"}, {"paperId": "2c33767ebb48667c4b25d5adbc4d83046dee3eab", "title": "Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora"}, {"paperId": "fca2e82075b590011e597778b3054a5964af274e", "title": "Pre-train, fine-tune, interpolate: a three-stage strategy for domain generalization"}, {"paperId": "84035c68a9a5c7deff39030cac66f7f737ad327e", "title": "Recycling diverse models for out-of-distribution generalization"}, {"paperId": "38338207e9ee2591e67c926b7da2294318a0dec2", "title": "Models with Conditional Computation Learn Suboptimal Solutions"}, {"paperId": "ddd9fb4516561a36b46860b5dc102981dca34134", "title": "Modular and Parameter-Efficient Fine-Tuning for NLP Models"}]}
