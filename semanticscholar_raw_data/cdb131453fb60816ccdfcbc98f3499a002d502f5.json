{"paperId": "cdb131453fb60816ccdfcbc98f3499a002d502f5", "publicationVenue": {"id": "528ced1f-e949-4e1a-8fee-2ffbf0be551d", "name": "Conference on Innovative Data Systems Research", "type": "conference", "alternate_names": ["CIDR", "Conf Innov Data Syst Res"], "url": "http://cidrdb.org/"}, "title": "RLEX: Saftey and Data Quality in Reinforcement Learning-based and Adaptive Systems", "abstract": "The marriage of data management and machine learning is one of the most signi\ufb01cant trends in recent database research. Industry and academia have built systems to improve the ef\ufb01ciency of model training [3,5,7,20], studied problems in learning on normalized data [16,19], designed libraries for in-database learning [2,8, 10], explored how to apply data cleaning before learning [15]. As database researchers, it is natural to think machine learning as simply a form of high-dimensional aggregate query processing. However, unlike traditional SQL analytics, learning systems potentially create feedback loops where the learned models affect the future training data. For example, if a model recommends Pop music to European users\u2013future data would have fewer examples of European preferences on other genres. To address such problems, we argue that learning systems need to move beyond the supervised learning paradigm and add support for Reinforcement Learning (RL) \u2013 drawing inspiration from recent successes of Google DeepMind on AlphaGo project [1]. Rather than predicting a single label for a single feature vector, RL algorithms learn stateful policies that output a sequence of predictions (called actions). As the name implies, these policies are learned through iterative trial-and-error (called exploration); optimizing an aggregate reward (utility of the predictions). In the music recommendation example, one could imagine a system that learns to recommend songs based on those songs previously heard by a user to maximize the total aggregate time spent on the platform. Such is the promise of RL, and in principle, RL subsumes all supervised learning models. However, this generality comes at a steep price. RL algorithms can spend signi\ufb01cant amounts of time exploring before learning a policy of value. It is further the case that many state-of-the-art RL algorithms have to store the entire execution trace of the system for learning [4]. While these algorithms do exploit parallelism [17] and GPUs [18], much of these", "venue": "Conference on Innovative Data Systems Research", "year": 2017, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": null, "authors": [{"authorId": "40428703", "name": "S. Krishnan"}], "citations": [{"paperId": "95969a12427d82a6669a1ff21694913e0bc6f5b6", "title": "RLEX :"}]}
