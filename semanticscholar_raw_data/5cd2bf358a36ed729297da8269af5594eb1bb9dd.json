{"paperId": "5cd2bf358a36ed729297da8269af5594eb1bb9dd", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering", "abstract": "Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas where pre-trained models lack the required expertise, such as law, science, and engineering.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-01", "journal": {"name": "ArXiv", "volume": "abs/2311.00204"}, "authors": [{"authorId": "2263679909", "name": "Zhen Guo"}, {"authorId": "2263998481", "name": "Yining Hua"}], "citations": [{"paperId": "1b516059eee07c1f7dae4008d7109e0f6bff64bb", "title": "Large Language Models in Mental Health Care: a Scoping Review"}]}
