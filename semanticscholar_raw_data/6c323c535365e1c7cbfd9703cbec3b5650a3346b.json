{"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs", "abstract": "In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-03", "journal": {"name": "ArXiv", "volume": "abs/2310.01801"}, "authors": [{"authorId": "2243417222", "name": "Suyu Ge"}, {"authorId": "2253427836", "name": "Yunan Zhang"}, {"authorId": "2253844913", "name": "Liyuan Liu"}, {"authorId": "2253896522", "name": "Minjia Zhang"}, {"authorId": "2253929707", "name": "Jiawei Han"}, {"authorId": "2256227181", "name": "Jianfeng Gao"}], "citations": [{"paperId": "d4d920dad3e7ab80363d31d554a366d0ac076371", "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget"}, {"paperId": "1f43c1f42dd5505f27c0b36c99e6e6fa01f87ad5", "title": "Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs"}, {"paperId": "8ab5a5d05eb825b24eaca9bcbb7c25d5f0077ec2", "title": "AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving"}, {"paperId": "7a9dd64d362892e46107f03f4ab1a2d89aba996e", "title": "Language Repository for Long Video Understanding"}, {"paperId": "e9576198e9ee767ede4b1ac6a739267aa52a9832", "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"}, {"paperId": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6", "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference"}, {"paperId": "7a54aad06171f59149aca5380863c62729c70b41", "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM"}, {"paperId": "a48a3cfde9e9a6f02821ea28698012e4d3e1cd73", "title": "QAQ: Quality Adaptive Quantization for LLM KV Cache"}, {"paperId": "275b005c33a315ad603f236cd5766efe07ef6a54", "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding"}, {"paperId": "ef1b02dc1b82f9955fc4760fcefd92c0fff9f227", "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference"}, {"paperId": "f9f311472ed8c4b15cfd74e1539f0c4815e574e9", "title": "SubGen: Token Generation in Sublinear Time and Memory"}, {"paperId": "c09a17ca78f06c2e5394fec19bdfbea55a216a7b", "title": "Towards Efficient and Reliable LLM Serving: A Real-World Workload Study"}, {"paperId": "b085968c4362fb286ad6c5ef71a5db9630da0498", "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"}, {"paperId": "3e8d4062ec4353ff2701c7769336dbdb97f8814c", "title": "Transformers are Multi-State RNNs"}, {"paperId": "40e565e070fde823097507fd6830cfa6944df95d", "title": "CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving"}]}
