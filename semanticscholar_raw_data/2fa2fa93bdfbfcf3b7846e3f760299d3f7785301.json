{"paperId": "2fa2fa93bdfbfcf3b7846e3f760299d3f7785301", "publicationVenue": {"id": "8f125553-3fd5-4370-a175-f7179db29048", "name": "IEEE International Performance, Computing, and Communications Conference", "type": "conference", "alternate_names": ["International Phoenix Conference on Computers and Communications", "IPCCC", "Int Phoenix Conf Comput Commun", "International Performance, Computing, and Communications Conference", "Int Perform Comput Commun Conf", "IEEE Int Perform Comput Commun Conf"], "url": "http://www.ipccc.org/"}, "title": "Deploying Network Key-Value SSDs to Disaggregate Resources in Big Data Processing Frameworks", "abstract": "The exponential data generation embraces unstructured object storage systems as an effective solution to improve performance. Key-Value (KV) SSD object storage devices are unveiled to mitigate the shortcomings of traditional Key-Value stores on block devices, including device low-bandwidth utilization and KV-store resource-draining operations on the host CPU and block devices. Samsung KV-SSDs are built on top of NVMe over Fabric hardware, which supports storage remote access protocols (i.e., RDMA). Network Key-Value (NKV) is a software eco-system developed by Samsung that enables data distribution and storage disaggregation of KV-SSDs. Most widely used big data processing platforms, such as Hadoop, Presto, deploy Hadoop Distributed File System (HDFS) to take advantage of rapid data access by co-locating storage and compute nodes. The co-allocation of compute and storage node limits the scalability and utilization resources and thus increases the total cost of ownership. In this paper, we present a new storage disaggregation model for big data processing platforms. Our new system layout leverages resource disaggregation by separating compute infrastructure from storage infrastructure and utilizes the benefits of new evolving storage technology, i.e., KV-SSD, for large-scale data access and processing. The goal of this work is to facilitate independent scaling of storage and compute resources, and shift the data retrieval load from the hosts to storage nodes. We evaluate our designed architecture using TPC-DS benchmark. Our results show that the CPU load on compute nodes is non-negligibly released with sustaining the same performance compared to the conventional Hadoop with HDFS.", "venue": "IEEE International Performance, Computing, and Communications Conference", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-06", "journal": {"name": "2020 IEEE 39th International Performance Computing and Communications Conference (IPCCC)", "pages": "1-8"}, "authors": [{"authorId": "48001723", "name": "Mahsa Bayati"}, {"authorId": "2069494841", "name": "Harsh Roogi"}, {"authorId": "2110753930", "name": "Ron Lee"}, {"authorId": "145154819", "name": "N. Mi"}], "citations": []}
