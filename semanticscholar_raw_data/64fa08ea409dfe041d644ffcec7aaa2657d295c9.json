{"paperId": "64fa08ea409dfe041d644ffcec7aaa2657d295c9", "publicationVenue": null, "title": "Proceedings of the Annual Meeting of the Cognitive Science Society", "abstract": "Humans can readily generalize their learning to new visual concepts, and infer their associated meanings. How do people align the different conceptual systems learned from different modalities? In the present paper, we examine emojis \u2014 pictographs uniquely situated between visual and linguistic modalities\u2014to explore the role of alignment and multimodality in visual and linguistic semantics. Simulation experiments show that relational structures of emojis captured in visual and linguistic conceptual systems can be aligned, and that the ease of alignment increases as the number of emojis increases. We also found that emojis with subjective impressions of high popularity are easier to align between their visual and linguistic representations. A behavioral experiment was conducted to measure similarity patterns between 48 emojis, and to compare human similarity judgments with three models based on visual, semantic and multimodal-joint representations of emojis. We found that the model trained with multimodal data by aligning visual and semantic spaces best accounts for human judgments.", "venue": "", "year": null, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "8245643", "name": "Bryor Snefjella"}, {"authorId": "2107033757", "name": "Yiling Yun"}, {"authorId": "2072785223", "name": "Shuhao Fu"}, {"authorId": "2243401787", "name": "Hongjing Lu"}], "citations": []}
