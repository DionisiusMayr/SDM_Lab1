{"paperId": "3793e9bb830cb6961668e4c2fd33cd8d524c6308", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends", "abstract": "General large language models (LLMs), represented by ChatGPT, have demonstrated significant potential in tasks such as code generation in software engineering. This has led to the development of specialized LLMs for software engineering, known as Code LLMs. A considerable portion of Code LLMs is derived from general LLMs through model fine-tuning. As a result, Code LLMs are often updated frequently and their performance can be influenced by the base LLMs. However, there is currently a lack of systematic investigation into Code LLMs and their performance. In this study, we conduct a comprehensive survey and analysis of the types of Code LLMs and their differences in performance compared to general LLMs. We aim to address three questions: (1) What LLMs are specifically designed for software engineering tasks, and what is the relationship between these Code LLMs? (2) Do Code LLMs really outperform general LLMs in software engineering tasks? (3) Which LLMs are more proficient in different software engineering tasks? To answer these questions, we first collect relevant literature and work from five major databases and open-source communities, resulting in 134 works for analysis. Next, we categorize the Code LLMs based on their publishers and examine their relationships with general LLMs and among themselves. Furthermore, we investigate the performance differences between general LLMs and Code LLMs in various software engineering tasks to demonstrate the impact of base models and Code LLMs. Finally, we comprehensively maintained the performance of LLMs across multiple mainstream benchmarks to identify the best-performing LLMs for each software engineering task. Our research not only assists developers of Code LLMs in choosing base models for the development of more advanced LLMs but also provides insights for practitioners to better understand key improvement directions for Code LLMs.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-11-17", "journal": {"name": "ArXiv", "volume": "abs/2311.10372"}, "authors": [{"authorId": "2267902535", "name": "Zibin Zheng"}, {"authorId": "2267247555", "name": "Kaiwen Ning"}, {"authorId": "2239164852", "name": "Yanlin Wang"}, {"authorId": "2267268859", "name": "Jingwen Zhang"}, {"authorId": "2267245031", "name": "Dewu Zheng"}, {"authorId": "2187008435", "name": "Mingxi Ye"}, {"authorId": "2254800142", "name": "Jiachi Chen"}], "citations": [{"paperId": "2024c306de13395ebf723ecb89fafb9dbe309f99", "title": "Training Language Model Agents without Modifying Language Models"}, {"paperId": "9a9314ecce6c32e6cac331a712f0b920a60a3566", "title": "Large Language Models for the Automated Analysis of Optimization Algorithms"}, {"paperId": "ab83f72cb954f22c490d74f13fbe3cd10fe790e4", "title": "FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair"}]}
