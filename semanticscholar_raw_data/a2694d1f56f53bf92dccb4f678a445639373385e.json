{"paperId": "a2694d1f56f53bf92dccb4f678a445639373385e", "publicationVenue": {"id": "27b93d96-c4b9-4a23-a3a0-061ad54deebc", "name": "IEEE International Symposium on High-Performance Parallel Distributed Computing", "type": "conference", "alternate_names": ["HPDC", "IEEE Int Symp High-performance Parallel Distrib Comput"], "url": "http://www.hpdc.org/"}, "title": "Kairos: Building Cost-Efficient Machine Learning Inference Systems with Heterogeneous Cloud Resources", "abstract": "Online inference is becoming a key service product for many businesses, deployed in cloud platforms to meet customer demands. Despite their revenue-generation capability, these services need to operate under tight Quality-of-Service (QoS) and cost budget constraints. This paper introduces KAIROS, a novel runtime framework that maximizes the query throughput while meeting QoS target and a cost budget. KAIROS designs and implements novel techniques to build a pool of heterogeneous compute hardware without online exploration overhead, and distribute inference queries optimally at runtime. Our evaluation using industry-grade machine learning (ML) models shows that KAIROS yields up to 2x the throughput of an optimal homogeneous solution, and outperforms state-of-the-art schemes by up to 70%, despite advantageous implementations of the competing schemes to ignore their exploration overhead.", "venue": "IEEE International Symposium on High-Performance Parallel Distributed Computing", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2022-10-12", "journal": {"name": "Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing"}, "authors": [{"authorId": "2118425325", "name": "Baolin Li"}, {"authorId": "2331418", "name": "S. Samsi"}, {"authorId": "74882299", "name": "V. Gadepally"}, {"authorId": "34966505", "name": "Devesh Tiwari"}], "citations": [{"paperId": "b246f2616df8348e488c27576fdeb4e1564e381b", "title": "Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference"}, {"paperId": "5fd1ba8e9b9350772551539fee557cff3d9f0bcf", "title": "H-EYE: Holistic Resource Modeling and Management for Diversely Scaled Edge-Cloud Systems"}, {"paperId": "fd2514efb219f3cd49f090374ea32bc62a7b87c3", "title": "HEET: A Heterogeneity Measure to Quantify the Difference across Distributed Computing Systems"}, {"paperId": "e26b43bac071c9c5fd7b31df4cfed4add6ff8d76", "title": "Clover: Toward Sustainable AI with Carbon-Aware Machine Learning Inference Service"}, {"paperId": "883b5a6cbbf3c499c8402204a657abf1e836d310", "title": "Deep Learning Workload Scheduling in GPU Datacenters: A Survey"}]}
