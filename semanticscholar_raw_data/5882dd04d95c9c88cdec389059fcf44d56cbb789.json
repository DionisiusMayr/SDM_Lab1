{"paperId": "5882dd04d95c9c88cdec389059fcf44d56cbb789", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation", "abstract": "Language models have steadily increased in size over the past few years. They achieve a high level of performance on various natural language processing (NLP) tasks such as question answering and summarization. Large language models (LLMs) have been used for generation and can now output human-like text. Due to this, there are other downstream tasks in the realm of dialog that can now harness the LLMs' language understanding capabilities. Dialog evaluation is one task that this paper will explore. It concentrates on prompting with LLMs: BLOOM, OPT, GPT-3, Flan-T5, InstructDial and TNLGv2. The paper shows that the choice of datasets used for training a model contributes to how well it performs on a task as well as on how the prompt should be structured. Specifically, the more diverse and relevant the group of datasets that a model is trained on, the better dialog evaluation performs. This paper also investigates how the number of examples in the prompt and the type of example selection used affect the model's performance.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-01-27", "journal": {"name": "ArXiv", "volume": "abs/2301.12004"}, "authors": [{"authorId": "50306945", "name": "Jessica Huynh"}, {"authorId": "2064549240", "name": "Cathy Jiao"}, {"authorId": "1491232062", "name": "Prakhar Gupta"}, {"authorId": "32251567", "name": "Shikib Mehri"}, {"authorId": "34765717", "name": "Payal Bajaj"}, {"authorId": "113810201", "name": "Vishrav Chaudhary"}, {"authorId": "1716325", "name": "M. Esk\u00e9nazi"}], "citations": [{"paperId": "90d04e5f1cb6efb12e14e388730cf58666325edb", "title": "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators"}, {"paperId": "0edcd6dd2e952523c35e77d0f9cce5927b97d63e", "title": "CoAScore: Chain-of-Aspects Prompting for NLG Evaluation"}, {"paperId": "74b0976a3a7b7013fd468a043a940dcf401e66f1", "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions"}, {"paperId": "ef7097244ee0cc2d54649e7bec121abf7c628947", "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems"}, {"paperId": "b64b2d28e00e1bdf35393856707cbd133058abab", "title": "xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark"}, {"paperId": "bcefc74b20649fd41ea05d87a3fa512d2559fc8d", "title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation"}, {"paperId": "ec9edb906a259aec3b3855f9933bf584fe3fac00", "title": "Approximating Online Human Evaluation of Social Chatbots with Prompting"}, {"paperId": "511901aa2aeba02c555045f8841275a74964400a", "title": "Approximating Human Evaluation of Social Chatbots with Prompting"}]}
