{"paperId": "c5b2243baf88a00db2d4e4f9edb33cde08eb153f", "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE International Conference on Computer Vision", "type": "conference", "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"}, "title": "eP-ALM: Efficient Perceptual Augmentation of Language Models", "abstract": "Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with little to no effort on direct finetuning. We investigate the minimal computational effort needed to adapt unimodal models for multimodal tasks and propose a new challenging setup, alongside different approaches, that efficiently adapts unimodal pretrained models. We show that by freezing more than 99% of total parameters, training only one linear projection layer, and prepending only one trainable token, our approach (dubbed eP-ALM) significantly outperforms other baselines on VQA and Captioning across Image, Video, and Audio modalities, following the proposed setup. The code is available here: https://github.com/mshukor/eP-ALM.", "venue": "IEEE International Conference on Computer Vision", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-03-20", "journal": {"name": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)", "pages": "21999-22012"}, "authors": [{"authorId": "2067292515", "name": "Mustafa Shukor"}, {"authorId": "41020827", "name": "Corentin Dancette"}, {"authorId": "51021910", "name": "M. Cord"}], "citations": [{"paperId": "03a94b9b407e6c01e782baef273dc25b40a701cd", "title": "FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models"}, {"paperId": "4c4396c227c68bb89e1b937a7dadda73d41db33b", "title": "Improved Baselines for Data-efficient Perceptual Augmentation of LLMs"}, {"paperId": "2a76a0dbb9bb9997536b2b3b42c28a01ad27e5cf", "title": "Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI"}, {"paperId": "84d99893ee24fc825e359598d44d602c45c4865e", "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving"}, {"paperId": "41fd1bea868e487815eb20a4b6ff006233a63672", "title": "Zero-Shot Refinement of Buildings' Segmentation Models using SAM"}, {"paperId": "a5d27bf7a2155d4ca016565a78b52ee90f81624c", "title": "Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning"}, {"paperId": "16ed5f612b66cb7d91e534dd7126b69756f45c34", "title": "HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving"}, {"paperId": "0fe88452660cb8a0e37f54bcd44f3cd6504354b5", "title": "Unified Model for Image, Video, Audio and Language Tasks"}, {"paperId": "584ca135b61482fd89247113da87d784f738dbfa", "title": "Foundational Models Defining a New Era in Vision: A Survey and Outlook"}, {"paperId": "8f2f6d90de822888ec4e283788bc09917018e9d6", "title": "ImageNetVC: Zero-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories"}]}
