{"paperId": "2f65e60dfd8a05f6dfeec8a237507b029386b1eb", "publicationVenue": {"id": "1c2ab05c-7d69-465e-929d-0920857aedce", "name": "International Conference on Automated Software Engineering", "type": "conference", "alternate_names": ["Autom Softw Eng", "ASE", "Automated Software Engineering", "Int Conf Autom Softw Eng"], "url": "http://ase.informatik.uni-essen.de/"}, "title": "Performance Testing for Cloud Computing with Dependent Data Bootstrapping", "abstract": "To effectively utilize cloud computing, cloud practice and research require accurate knowledge of the performance of cloud applications. However, due to the random performance fluctuations, obtaining accurate performance results in the cloud is extremely difficult. To handle this random fluctuation, prior research on cloud performance testing relied on a non-parametric statistic tool called bootstrapping to design their stop criteria. However, in this paper, we show that the basic bootstrapping employed by prior work overlooks the internal dependency within cloud performance test data, which leads to inaccurate performance results.We then present Metior, a novel automated cloud performance testing methodology, which is designed based on statistical tools of block bootstrapping, the law of large numbers, and autocorrelation. These statistical tools allow Metior to properly consider the internal dependency within cloud performance test data. They also provide better coverage of cloud performance fluctuation and reduce the testing cost. Experimental evaluation on two public clouds showed that 98% of Metior\u2019s tests could provide performance results with less than 3% error. Metior also significantly outperformed existing cloud performance testing methodologies in terms of accuracy and cost \u2013 with up to 14% increase in the accurate test count and up to 3.1 times reduction in testing cost.", "venue": "International Conference on Automated Software Engineering", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2021-11-01", "journal": {"name": "2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)", "pages": "666-678"}, "authors": [{"authorId": "2112463596", "name": "Sen He"}, {"authorId": "2115347803", "name": "Tianyi Liu"}, {"authorId": "34718613", "name": "P. Lama"}, {"authorId": "2144713785", "name": "Jaewoo Lee"}, {"authorId": "2862224", "name": "I. Kim"}, {"authorId": "39253519", "name": "Wei Wang"}], "citations": [{"paperId": "9446bc7f530398172c2fd96672e363e934c310a8", "title": "A multi\u2010faceted analysis of the performance variability of virtual machines"}, {"paperId": "28317f0b7c644d3497f316dcacb9cb9f97160acf", "title": "SuperFlow: Performance Testing for Serverless Computing"}, {"paperId": "06ff2e93ff5000794066c83de6d3635bd24c7ab6", "title": "A Study of Java Microbenchmark Tail Latencies"}, {"paperId": "6017c63c56c9f20cede1236f79d768bcc21b5e8d", "title": "A configurable method for benchmarking scalability of cloud-native applications"}, {"paperId": "85fbb18c8663da1b27dae2cd36fb494ab04bc2b9", "title": "Revisiting Performance Evaluation in the Age of Uncertainty"}]}
