{"paperId": "63ca82b7e846468e1aa9eacec3b1a26b7ceb94c0", "publicationVenue": {"id": "af90489e-312f-4514-bea2-bcb399cb8ece", "name": "Interspeech", "type": "conference", "alternate_names": ["Conf Int Speech Commun Assoc", "INTERSPEECH", "Conference of the International Speech Communication Association"], "issn": "2308-457X", "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech", "alternate_urls": ["http://www.isca-speech.org/"]}, "title": "Disfluency Detection with Unlabeled Data and Small BERT Models", "abstract": "Disfluency detection models now approach high accuracy on English text. However, little exploration has been done in improving the size and inference time of the model. At the same time, automatic speech recognition (ASR) models are moving from server-side inference to local, on-device inference. Supporting models in the transcription pipeline (like disfluency detection) must follow suit. In this work we concentrate on the disfluency detection task, focusing on small, fast, on-device models based on the BERT architecture. We demonstrate it is possible to train disfluency detection models as small as 1.3 MiB, while retaining high performance. We build on previous work that showed the benefit of data augmentation approaches such as self-training. Then, we evaluate the effect of domain mismatch between conversational and written text on model performance. We find that domain adaptation and data augmentation strategies have a more pronounced effect on these smaller models, as compared to conventional BERT models.", "venue": "Interspeech", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-04-22", "journal": {"pages": "766-770"}, "authors": [{"authorId": "2854301", "name": "Johann C. Rocholl"}, {"authorId": "143689491", "name": "V. Zayats"}, {"authorId": "28265704", "name": "D. D. Walker"}, {"authorId": "2080123006", "name": "Noah B. Murad"}, {"authorId": "2080128523", "name": "Aaron Schneider"}, {"authorId": "1724850", "name": "Daniel J. Liebling"}], "citations": [{"paperId": "57622fd0f43bbf48a8db872dd1023d2fcb2c0870", "title": "Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation"}, {"paperId": "987c04f34d399add7cc738e03257753668560407", "title": "Boosting Disfluency Detection with Large Language Model as Disfluency Generator"}, {"paperId": "93757c4b5aa659ae6c12802a23157cd88e5936d2", "title": "Automatic Disfluency Detection from Untranscribed Speech"}, {"paperId": "a1d97a7b4890c2ef1766f35a8e9d38d3f9b13bbe", "title": "End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining"}, {"paperId": "eea1bceebc27973064760619d76546f3d2fad1b1", "title": "Adapting the NICT-JLE Corpus for Disfluency Detection Models"}, {"paperId": "9c7ea6ab27f4701a9f3c34d4d67af3942b7c8164", "title": "Toward A Multimodal Approach for Disfluency Detection and Categorization"}, {"paperId": "f631c058c2c6a6e3fe720929b96959fd93351956", "title": "MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup"}, {"paperId": "e57ab207e3b5cd4b61f630c1e16cf4e1a66efc7d", "title": "Transcription Free Filler Word Detection with Neural Semi-CRFs"}, {"paperId": "dda47c2497c03610eccc60e15672f50be0ec361d", "title": "Streaming Joint Speech Recognition and Disfluency Detection"}, {"paperId": "273991329cf51e329904afd3e064c9671dbec563", "title": "Artificial Disfluency Detection, Uh No, Disfluency Generation for the Masses"}, {"paperId": "84b20049f24fe76ffd8681703bfed3d5a1f50c76", "title": "Analysis of transcription tools for Brazilian Portuguese with focus on disfluency detection"}, {"paperId": "ba52b9ad3aa61c9718067c3f21443dc727c5b652", "title": "Enabling Off-the-Shelf Disfluency Detection and Categorization for Pathological Speech"}, {"paperId": "d56a8e779df0aaa35f38acb820730c1c5b26bc14", "title": "From Disfluency Detection to Intent Detection and Slot Filling"}, {"paperId": "ec89d19b56829daf600e9e205298dc7d3863cc3c", "title": "Turn-Taking Prediction for Natural Conversational Speech"}, {"paperId": "30a9b6da70349f1825ea53dd03d52a465b6d44b7", "title": "Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection"}, {"paperId": "06999b13336cffcbd84215bc8d47ffe8782c822d", "title": "LARD: Large-scale Artificial Disfluency Generation"}, {"paperId": "36948a888beb364764e887a731decb9d6e00136d", "title": "Barriers to Effective Evaluation of Simultaneous Interpretation"}, {"paperId": "1efe1f512f27abc6bb5f4606c76c3c151203630c", "title": "Survey: Reduplication vs Repetition classi\ufb01cation"}, {"paperId": "a49462f67d0154eb279ae131adda2459504dbcea", "title": "Incremental Disfluency Detection for Spoken Learner English"}, {"paperId": "cb79bf29dca8e2f3dfcc9b0e033c94d8b6f1f4b7", "title": "Survey: Exploring Disfluencies for Speech-to-Speech Machine Translation"}, {"paperId": "5da4d08c1f61b433cea40162b387cc5ea6764ec9", "title": "Adaptive Unsupervised Self-training for Disfluency Detection"}, {"paperId": "127762527bbfbd0708f226596e736ae5376cc28a", "title": "Disfluency Detection for Vietnamese"}]}
