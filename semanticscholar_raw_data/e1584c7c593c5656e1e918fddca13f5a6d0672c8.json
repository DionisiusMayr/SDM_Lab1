{"paperId": "e1584c7c593c5656e1e918fddca13f5a6d0672c8", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "FlowGNN: A Dataflow Architecture for Universal Graph Neural Network Inference via Multi-Queue Streaming", "abstract": "Graph neural networks (GNNs) have recently exploded in popularity thanks to their broad applicability to graph-related problems such as quantum chemistry, drug discovery, and high energy physics. However, meeting demand for novel GNN models and fast inference simultaneously is challenging because of the gap between developing ef\ufb01cient accelerators and the rapid creation of new GNN models. Prior art focuses on the acceleration of speci\ufb01c classes of GNNs, such as Graph Convolutional Network (GCN), but lacks the generality to support a wide range of existing or new GNN models. Meanwhile, most work rely on graph pre-processing to exploit data locality, making them unsuitable for real-time applications. To address these limitations, in this work, we propose a generic data\ufb02ow architecture for GNN acceleration, named FlowGNN , which can \ufb02exibly support the majority of message-passing GNNs. The contributions are three-fold. First, we propose a novel and scalable data\ufb02ow architecture, which \ufb02exibly supports a wide range of GNN models with message-passing mechanism. The architecture features a con-\ufb01gurable data\ufb02ow optimized for simultaneous computation of node embedding, edge embedding, and message passing, which is generally applicable to all models. We also propose a rich library of model-speci\ufb01c components. Second, we deliver ultra-fast real-time GNN inference without any graph pre-processing, making it agnostic to dynamically changing graph structures. Third, we verify our architecture on the Xilinx Alveo U50 FPGA board and measure the on-board end-to-end performance. We achieve a speed-up of up to 51\u2013254 \u00d7 against CPU (6226R) and 1.3\u2013", "venue": "arXiv.org", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"name": "ArXiv", "volume": "abs/2204.13103"}, "authors": [{"authorId": "2150914274", "name": "Rishov Sarkar"}, {"authorId": "2150914153", "name": "Stefan Abi-Karam"}, {"authorId": "2119289583", "name": "Yuqiang He"}, {"authorId": "2150914310", "name": "Lakshmi Sathidevi"}, {"authorId": "145462792", "name": "Cong Hao"}], "citations": [{"paperId": "ff85cbe6b374f87027b4445e126bd1a64e9504ac", "title": "MaxK-GNN: Extremely Fast GPU Kernel Design for Accelerating Graph Neural Networks Training"}, {"paperId": "df3ead351a499f5aea248224ce65561ae860ef08", "title": "Accelerating GNN-Based SAR Automatic Target Recognition on HBM-Enabled FPGA"}, {"paperId": "0bfdc7914e6a3a16ebcfe8e071da9d07e5f40cd7", "title": "Graph-OPU: A Highly Integrated FPGA-Based Overlay Processor for Graph Neural Networks"}, {"paperId": "68352ea2d6aba76afaaedd51727cd5f4dc7f4435", "title": "Exploiting On-Chip Heterogeneity of Versal Architecture for GNN Inference Acceleration"}, {"paperId": "894d61c709ec6f61899703458d90b09c663d7b11", "title": "A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware"}, {"paperId": "e50ed3682366e9872a6a95c5877e8c211f853b56", "title": "DGNN-Booster: A Generic FPGA Accelerator Framework For Dynamic Graph Neural Network Inference"}, {"paperId": "0165cf24953cfae9337c8659d9d24968b60edf12", "title": "GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization"}, {"paperId": "f2759be79298a8bea84e02d561ffd1f6783c9f7f", "title": "Dynasparse: Accelerating GNN Inference through Dynamic Sparsity Exploitation"}, {"paperId": "c6becf9b67bd7abe6e9c8c3e0074a2e86422e554", "title": "A survey of field programmable gate array (FPGA)-based graph convolutional neural network accelerators: challenges and opportunities"}, {"paperId": "b9592ce44aa3268882572c86d740b3d5450ba7b5", "title": "LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics"}, {"paperId": "8cb6dd51a76bc706f5574869bff351340f5fec1b", "title": "LL-GNN: Low Latency Graph Neural Networks on FPGAs for Particle Detectors"}]}
