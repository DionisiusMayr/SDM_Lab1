{"paperId": "fde0535d71e2e9e9273e09d3b4e3bc0591e1607e", "publicationVenue": {"id": "7fca7065-fa6f-490b-923e-3ffc07857f1c", "name": "IEEE Transactions on Cloud Computing", "type": "journal", "alternate_names": ["IEEE Trans Cloud Comput"], "issn": "2168-7161", "url": "https://www.computer.org/web/tcc", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=6245519"]}, "title": "Using Microbenchmark Suites to Detect Application Performance Changes", "abstract": "Software performance changes are costly and often hard to detect pre-release. Similar to software testing frameworks, either application benchmarks or microbenchmarks can be integrated into quality assurance pipelines to detect performance changes before releasing a new application version. Unfortunately, extensive benchmarking studies usually take several hours which is problematic when examining dozens of daily code changes in detail; hence, trade-offs have to be made. Optimized microbenchmark suites, which only include a small subset of the full suite, are a potential solution for this problem, given that they still reliably detect the majority of the application performance changes such as an increased request latency. It is, however, unclear whether microbenchmarks and application benchmarks detect the same performance problems and one can be a proxy for the other. In this paper, we explore whether microbenchmark suites can detect the same application performance changes as an application benchmark. For this, we run extensive benchmark experiments with both the complete and the optimized microbenchmark suites of the two time-series database systems InfluxDB and VictoriaMetrics and compare their results to the results of corresponding application benchmarks. We do this for 70 and 110 commits, respectively. Our results show that it is possible to detect application performance changes using an optimized microbenchmark suite if frequent false-positive alarms can be tolerated.", "venue": "IEEE Transactions on Cloud Computing", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-12-19", "journal": {"name": "IEEE Transactions on Cloud Computing", "pages": "2575-2590", "volume": "11"}, "authors": [{"authorId": "51204829", "name": "M. Grambow"}, {"authorId": "2189956242", "name": "Denis Kovalev"}, {"authorId": "10420961", "name": "Christoph Laaber"}, {"authorId": "1910406", "name": "P. Leitner"}, {"authorId": "3077067", "name": "David Bermbach"}], "citations": [{"paperId": "87da36ad28c28883d9d8f191d9b0ec58536e013a", "title": "Efficiently Detecting Performance Changes in FaaS Application Releases"}, {"paperId": "e47c9293a3a546b8c7ef5c1e28711936fadf7658", "title": "The Early Microbenchmark Catches the Bug - Studying Performance Issues Using Micro- and Application Benchmarks"}, {"paperId": "4a3c215805db716f731dc574e3407fcd3270dcc6", "title": "Evaluating Search-Based Software Microbenchmark Prioritization"}]}
