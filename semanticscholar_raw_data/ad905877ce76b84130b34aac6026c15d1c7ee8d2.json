{"paperId": "ad905877ce76b84130b34aac6026c15d1c7ee8d2", "publicationVenue": {"id": "3bcf77b3-860b-4dd7-84ae-9fe9414c6c6a", "name": "Conference on Machine Learning and Systems", "type": "conference", "alternate_names": ["MLSys", "Conf Mach Learn Syst"], "url": "https://mlsys.org/"}, "title": "SRIFTY: Swift and Thrifty Distributed Neural Network Training on the Cloud", "abstract": "Finding the best VM con\ufb01guration is key to achieve lower cost and higher throughput, two primary concerns in cloud-based distributed neural network (NN) training today. Optimal VM selection that meets user constraints requires ef\ufb01ciently navigating a large search space while controlling for the performance variance associated with sharing cloud instances and networks. In this work, we characterize this variance in the context of distributed NN training and present results of a comprehensive throughput and cost-ef\ufb01ciency study we conducted across a wide array of instances to prune for the optimal VM search space. Using insights from these studies, we built Srifty, a system that combines runtime pro\ufb01ling with learned performance models to accurately predict training performance and \ufb01nd the best VM choice that satis\ufb01es user constraints, potentially leveraging both heterogeneous setups and spot instances. We integrated Srifty with PyTorch and evaluated it on Amazon EC2. We conducted a large-scale generalization study of Srifty across more than 2K training setups on EC2. Our results show that Srifty achieves an iteration latency prediction error of 8%, and its VM instance recommendations offer signi\ufb01cant throughput gain and cost reduction while satisfying user constraints compared to existing solutions in complex, real-world scenarios.", "venue": "Conference on Machine Learning and Systems", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-11-29", "journal": null, "authors": [{"authorId": "51225788", "name": "Liangchen Luo"}, {"authorId": "2056286127", "name": "Peter West"}, {"authorId": "144695691", "name": "A. Krishnamurthy"}, {"authorId": "1717411", "name": "L. Ceze"}], "citations": [{"paperId": "e0e897d9a4a865874c2c6efe97b127e4a82dadbf", "title": "Training DNN Models over Heterogeneous Clusters with Optimal Performance"}, {"paperId": "b8f61204b38293027d7cb609f54ab13b291d5443", "title": "Stash: A Comprehensive Stall-Centric Characterization of Public Cloud VMs for Distributed Deep Learning"}, {"paperId": "97c98e57044e7523e1c61329d23992a5300815f6", "title": "spotDNN: Provisioning Spot Instances for Predictable Distributed DNN Training in the Cloud"}, {"paperId": "a16005cbea461ee0e31a6c7631bd136a43d40e82", "title": "Efficient Direct-Connect Topologies for Collective Communications"}]}
