{"paperId": "e581a00152e3764d56277a409bc2c46f6092c484", "publicationVenue": {"id": "d13e941e-4cac-4f1d-bdca-77d927e31f1b", "name": "ACM Symposium on Cloud Computing", "type": "conference", "alternate_names": ["System-on-Chip Conference", "ACM Symp Cloud Comput", "Syst Conf", "Symp Cloud Comput", "Annual IEEE International System-on-Chip Conference", "Symposium on Cloud Computing", "Annu IEEE Int Syst Conf", "SoCC"], "url": "http://www.ieee-socc.org/"}, "title": "Chronus: A Novel Deadline-aware Scheduler for Deep Learning Training Jobs", "abstract": "Modern GPU clusters support Deep Learning training (DLT) jobs in a distributed manner. Job scheduling is the key to improve the training performance, resource utilization and fairness across users. Different training jobs may require various objectives and demands in terms of completion time. How to efficiently satisfy all these requirements is not extensively studied. We present Chronus, an end-to-end scheduling system to provide deadline guarantee for SLO jobs and maximize the performance of best-effort jobs. Chronus is designed based on the unique features of DLT jobs. (1) It leverages the intra-job predictability of DLT processes to efficiently profile jobs and estimate their runtime speed with dynamic resource scaling. (2) It takes advantages of the DLT preemption feature to select jobs with a lease-based training scheme. (3) It considers the placement sensitivity of DLT jobs to allocate resources with new consolidation and local-search strategies. Large-scale simulations on real-world job traces show that Chronus can reduce the deadline miss rate of SLO jobs by up to 14.7x, and the completion time of best-effort jobs by up to 19.9x, compared to existing schedulers. We also implement a prototype of Chronus atop Kubernents in a cluster of 120 GPUs to validate its practicability.", "venue": "ACM Symposium on Cloud Computing", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2021-11-01", "journal": {"name": "Proceedings of the ACM Symposium on Cloud Computing"}, "authors": [{"authorId": "2153578116", "name": "Wei Gao"}, {"authorId": "2114133698", "name": "Zhisheng Ye"}, {"authorId": "2075416290", "name": "Peng Sun"}, {"authorId": "145868454", "name": "Yonggang Wen"}, {"authorId": "2146333441", "name": "Tianwei Zhang"}], "citations": [{"paperId": "8046d516d395df434ff2b0c65c15f55e8e85dfe5", "title": "A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters"}, {"paperId": "c267d3750fb5dd73861f1e17b35e7c107e5aa7b3", "title": "GPU Cluster Scheduling for Network-Sensitive Deep Learning"}, {"paperId": "81a9f77a1ba35e3b71bb464515dee728de459e1c", "title": "Towards providing reliable job completion time predictions using PCS"}, {"paperId": "59abd3a23d0ab6290d54da9fa34d8573b9e0eb9b", "title": "vTrain: A Simulation Framework for Evaluating Cost-effective and Compute-optimal Large Language Model Training"}, {"paperId": "8fae24ba59f06b5097ed4f8d6e2f2efde144280a", "title": "Towards GPU Memory Efficiency for Distributed Training at Scale"}, {"paperId": "42ac39327eb25d2e757b55eb5eb180f6a55dd020", "title": "On a Meta Learning-Based Scheduler for Deep Learning Clusters"}, {"paperId": "001ef084f3255f0bf6d89038dddde73cdd329d29", "title": "DeepBoot: Dynamic Scheduling System for Training and Inference Deep Learning Tasks in GPU Cluster"}, {"paperId": "e470f6611792bd2deae88772076431b8120a98a7", "title": "A Dual-Agent Scheduler for Distributed Deep Learning Jobs on Public Cloud via Reinforcement Learning"}, {"paperId": "63c1052d76098022b1d3a3811e1bfcaca9077bca", "title": "Hydra: Deadline-Aware and Efficiency-Oriented Scheduling for Deep Learning Jobs on Heterogeneous GPUs"}, {"paperId": "343867970e6feea44fb134244168d04f0f3d164b", "title": "Enabling Switch Memory Management for Distributed Training with In-Network Aggregation"}, {"paperId": "f8ae23caf5047f21acd178496da29d71a10602bd", "title": "Elastic Resource Management for Deep Learning Applications in a Container Cluster"}, {"paperId": "7a840e88e1eea1e072576068eae9a6473845c5d1", "title": "Offline performance and energy consumption prediction model of deep learning training tasks"}, {"paperId": "3995c7a3704db252cfcce18ef56562a89f4cf693", "title": "ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning"}, {"paperId": "d5fe97309afdf0da633e04b5da4212a054661ecf", "title": "Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs"}, {"paperId": "fc426ebe45e0b7dac5052947d7b482847a197da8", "title": "Tereis: A Package-Based Scheduling in Deep Learning Systems"}, {"paperId": "b4401d49eff45cd6b61a93739dca8b38b862d71d", "title": "GAS: GPU Allocation Strategy for Deep Learning Training Tasks"}, {"paperId": "063224d0626be345035db80587dae86eda161e42", "title": "PickyMan: A Preemptive Scheduler for Deep Learning Jobs on GPU Clusters"}, {"paperId": "dd6c0726a2a0ad2459de0e4024ed134cd59382b8", "title": "Tear Up the Bubble Boom: Lessons Learned From a Deep Learning Research and Development Cluster"}, {"paperId": "745093391442971373d2af7549b00651a4636808", "title": "Deep Learning Workload Scheduling in GPU Datacenters: Taxonomy, Challenges and Vision"}, {"paperId": "883b5a6cbbf3c499c8402204a657abf1e836d310", "title": "Deep Learning Workload Scheduling in GPU Datacenters: A Survey"}, {"paperId": "31d3c657987119aaa1e0b8306c0abbe84bf35222", "title": "A Survey on Scheduling Techniques in Computing and Network Convergence"}, {"paperId": "b866a5331ab660fd7937c286ef6e5c72532bcf17", "title": "ASTRAEA: A Fair Deep Learning Scheduler for Multi-tenant GPU Clusters"}]}
