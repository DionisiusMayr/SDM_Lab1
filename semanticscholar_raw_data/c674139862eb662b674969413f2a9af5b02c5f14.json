{"paperId": "c674139862eb662b674969413f2a9af5b02c5f14", "publicationVenue": {"id": "4f854c65-f7ce-4588-a3ca-d96a1c00fc6d", "name": "IEEE Symposium on Field-Programmable Custom Computing Machines", "type": "conference", "alternate_names": ["IEEE Symp Field-programmable Cust Comput Mach", "Field-programmable Cust Comput Mach", "FCCM", "Field-Programmable Custom Computing Machines"], "url": "http://www.fccm.org/"}, "title": "BoostGCN: A Framework for Optimizing GCN Inference on FPGA", "abstract": "Graph convolutional networks (GCNs) have revolutionized many big data applications, such as recommendation systems, traffic prediction, etc. However, accelerating GCN inference is challenging due to (1) massive external memory traffic and irregular memory access, (2) workload imbalance due to skewed degree distribution, and (3) intra-stage load imbalance caused by two heterogeneous computation phases of the algorithm. To address the above challenges, we propose a framework named BoostGCN to optimize GCN inference on FPGA. First, we develop a novel hardware-aware Partition-Centric Feature Aggregation (PCFA) scheme that leverages 3-D partitioning with the vertex-centric computing paradigm. This increases on-chip data reuse and reduces the total data communication volume with external memory. Second, we design a novel hardware architecture to enable pipelined execution of the two heterogeneous computation phases. We develop a low-overhead task scheduling strategy to reduce the pipeline stalls caused by the two computation phases. Third, we provide a complete GCN acceleration framework on FPGA with optimized RTL templates. It can generate hardware designs based on the customized configuration and is adaptable to various GCN models. Using our framework, we generate accelerators for various GCN models on a state-of-the-art FPGA platform and evaluate our designs using widely used datasets. Experimental results show that the accelerators produced by our framework achieve significant speedup compared with state-of-the-art implementations on CPU (\u2248 100\u00d7), GPU (\u2248 30\u00d7), prior FPGA accelerator (3-45)\u00d7.", "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2021-02-17", "journal": {"name": "2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "pages": "29-39"}, "authors": [{"authorId": "1750921072", "name": "Bingyi Zhang"}, {"authorId": "2286832947", "name": "R. Kannan"}, {"authorId": "1728271", "name": "V. Prasanna"}], "citations": [{"paperId": "85691014466d48647a01d8726c68d0e9258fc667", "title": "Single-GPU GNN Systems: Traps and Pitfalls"}, {"paperId": "0d41b3d4841bd007edc4a2ef7bbdce11ca70d255", "title": "CoGNN: An Algorithm-Hardware Co-Design Approach to Accelerate GNN Inference With Minibatch Sampling"}, {"paperId": "6b8986bd9c50e47c0cca0083c82cd412f6bc2209", "title": "HIDA: A Hierarchical Dataflow Compiler for High-Level Synthesis"}, {"paperId": "6da6e236895e0e2ea3ca824b7d1a6a809742667e", "title": "SAGA: Sparsity-Agnostic Graph Convolutional Network Acceleration with Near-Optimal Workload Balance"}, {"paperId": "1725d876df12854029982c590e1a37b017ac4f73", "title": "GNNHLS: Evaluating Graph Neural Network Inference via High-Level Synthesis"}, {"paperId": "df3ead351a499f5aea248224ce65561ae860ef08", "title": "Accelerating GNN-Based SAR Automatic Target Recognition on HBM-Enabled FPGA"}, {"paperId": "0bfdc7914e6a3a16ebcfe8e071da9d07e5f40cd7", "title": "Graph-OPU: A Highly Integrated FPGA-Based Overlay Processor for Graph Neural Networks"}, {"paperId": "a7f4d825e3088da30a1908c02db001417b1faa97", "title": "A Multicore GNN Training Accelerator"}, {"paperId": "bcec5527841a9eba23ddfd460df73454364e215a", "title": "Hardware-Friendly Block Variable-Length Sampling Pruning for Graph Neural Networks"}, {"paperId": "68352ea2d6aba76afaaedd51727cd5f4dc7f4435", "title": "Exploiting On-Chip Heterogeneity of Versal Architecture for GNN Inference Acceleration"}, {"paperId": "0a2951d2c7f1e231e794c9c4b00ab0f357bd7161", "title": "VIDGCN: Embracing input data diversity with a configurable graph convolutional network accelerator"}, {"paperId": "894d61c709ec6f61899703458d90b09c663d7b11", "title": "A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware"}, {"paperId": "fae4419bd0a28c701353e4ee10d4121dd4ebfa38", "title": "FLASH: FPGA-Accelerated Smart Switches with GCN Case Study"}, {"paperId": "6847a3fb608588ab5cf366cb2057d1800cb12753", "title": "Implementation and Analysis on 4x4 Multiplier using Genesys FPGA Board"}, {"paperId": "29953a1abd0258b79167d3b0348755a36b7a1c13", "title": "Fault Recovery from Multi-Tenant FPGA Voltage Attacks"}, {"paperId": "2edbf306e5521e42e0834dc07fbc511e603c87b7", "title": "From Acceleration to Accelerating Acceleration: Modernizing the Accelerator Landscape using High-Level Synthesis"}, {"paperId": "9c93206b8d52841b0da5dd0227048e0798ca3b96", "title": "SCV-GNN: Sparse Compressed Vector-Based Graph Neural Network Aggregation"}, {"paperId": "8a56bf49179f93feb2889e58358391b946af3168", "title": "FPGA Acceleration of GCN in Light of the Symmetry of Graph Adjacency Matrix"}, {"paperId": "0165cf24953cfae9337c8659d9d24968b60edf12", "title": "GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization"}, {"paperId": "59a640a027813d35b94eaf161f9197f370cba0c6", "title": "Multi-Flow Transmission in Wireless Interference Networks: A Convergent Graph Learning Approach"}, {"paperId": "f2759be79298a8bea84e02d561ffd1f6783c9f7f", "title": "Dynasparse: Accelerating GNN Inference through Dynamic Sparsity Exploitation"}, {"paperId": "229db0006e6f2ea8f45cf266dbdeeb720d38c2b3", "title": "GraphAGILE: An FPGA-Based Overlay Accelerator for Low-Latency GNN Inference"}, {"paperId": "2e24c192aa130183188b67f4ba4450605b6dbc6d", "title": "Software-hardware co-design for accelerating large-scale graph convolutional network inference on FPGA"}, {"paperId": "ab825d91defee0e2926b5e1090a90d4bc68c5fdf", "title": "NTGAT: A Graph Attention Network Accelerator with Runtime Node Tailoring"}, {"paperId": "c6becf9b67bd7abe6e9c8c3e0074a2e86422e554", "title": "A survey of field programmable gate array (FPGA)-based graph convolutional neural network accelerators: challenges and opportunities"}, {"paperId": "e5adcef364120aaee7a80b7230ee5cb3c32dc674", "title": "Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU"}, {"paperId": "b9592ce44aa3268882572c86d740b3d5450ba7b5", "title": "LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics"}, {"paperId": "1600b860d55ce948f4b095853d5048caa7d24bde", "title": "Modeling the Energy Efficiency of GEMM using Optical Random Access Memory"}, {"paperId": "7b20a8bfcebaded7858cdb8ec7f3ac81051c27ed", "title": "Optimizing Graph Neural Networks for Jet Tagging in Particle Physics on FPGAs"}, {"paperId": "6ac131f72be947373d3d6adc9f56498136350292", "title": "Accurate, Low-latency, Efficient SAR Automatic Target Recognition on FPGA"}, {"paperId": "6c7d5db71d5c274222804fefa40f2a4876180fba", "title": "Multi-Node Acceleration for Large-Scale GCNs"}, {"paperId": "b9b42330eeb0dddfa1e89156f872485503428249", "title": "FusedGCN: A Systolic Three-Matrix Multiplication Architecture for Graph Convolutional Networks"}, {"paperId": "4a02fde23b18eacd695901429c7f7bafca5c5dd9", "title": "H-GCN: A Graph Convolutional Network Accelerator on Versal ACAP Architecture"}, {"paperId": "de61b286616eca10e6f1e9a04f39066155718444", "title": "Low-latency Mini-batch GNN Inference on CPU-FPGA Heterogeneous Platform"}, {"paperId": "aea742df7397dd2ad26b80f13a529946b5331cf6", "title": "Reconfigurable Acceleration of Graph Neural Networks for Jet Identification in Particle Physics"}, {"paperId": "5be7d06ee1647b4b257036e8d1f3b4a0ecb0b05c", "title": "Hyperscale FPGA-as-a-service architecture for large-scale distributed graph neural network"}, {"paperId": "5f1776b961a8f3080a66a76daf3c38879ad53242", "title": "QEGCN: An FPGA-based accelerator for quantized GCNs with edge-level parallelism"}, {"paperId": "8d9b1bf8bad5fabb11a37e126706fec4371ef7b8", "title": "FlowGNN: A Dataflow Architecture for Real-Time Workload-Agnostic Graph Neural Network Inference"}, {"paperId": "bff8a216e261d03c2e6938e1a91e807a979624fc", "title": "GROW: A Row-Stationary Sparse-Dense GEMM Accelerator for Memory-Efficient Graph Convolutional Neural Networks"}, {"paperId": "a93f82c7e9fd9f4f8ab720ed8b2093bfb5566481", "title": "GenGNN: A Generic FPGA Framework for Graph Neural Network Acceleration"}, {"paperId": "3bec8b4017541b9be679eb75836de9c713968e22", "title": "HP-GNN: Generating High Throughput GNN Training Implementation on CPU-FPGA Heterogeneous Platform"}, {"paperId": "7805e7f2eff1aa52ee509debfd1a17e7b78aafd2", "title": "SH-GAT: Software-hardware co-design for accelerating graph attention networks on FPGA"}, {"paperId": "e1584c7c593c5656e1e918fddca13f5a6d0672c8", "title": "FlowGNN: A Dataflow Architecture for Universal Graph Neural Network Inference via Multi-Queue Streaming"}, {"paperId": "d1ce401eea30fff7d1d3125855471f26a3f24db4", "title": "A Generic FPGA Accelerator Framework for Ultra-Fast GNN Inference"}, {"paperId": "8cb6dd51a76bc706f5574869bff351340f5fec1b", "title": "LL-GNN: Low Latency Graph Neural Networks on FPGAs for Particle Detectors"}]}
