{"paperId": "f13a1911eb59b8529c38292e029af9f3e5fc7905", "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "title": "GBA: A Tuning-free Approach to Switch between Synchronous and Asynchronous Training for Recommendation Model", "abstract": "High-concurrency asynchronous training upon parameter server (PS) architecture and high-performance synchronous training upon all-reduce (AR) architecture are the most commonly deployed distributed training modes for recommendation models. Although synchronous AR training is designed to have higher training efficiency, asynchronous PS training would be a better choice for training speed when there are stragglers (slow workers) in the shared cluster, especially under limited computing resources. An ideal way to take full advantage of these two training modes is to switch between them upon the cluster status. However, switching training modes often requires tuning hyper-parameters, which is extremely time- and resource-consuming. We find two obstacles to a tuning-free approach: the different distribution of the gradient values and the stale gradients from the stragglers. This paper proposes Global Batch gradients Aggregation (GBA) over PS, which aggregates and applies gradients with the same global batch size as the synchronous training. A token-control process is implemented to assemble the gradients and decay the gradients with severe staleness. We provide the convergence analysis to reveal that GBA has comparable convergence properties with the synchronous training, and demonstrate the robustness of GBA the recommendation models against the gradient staleness. Experiments on three industrial-scale recommendation tasks show that GBA is an effective tuning-free approach for switching. Compared to the state-of-the-art derived asynchronous training, GBA achieves up to 0.2% improvement on the AUC metric, which is significant for the recommendation models. Meanwhile, under the strained hardware resource, GBA speeds up at least 2.4x compared to synchronous training.", "venue": "Neural Information Processing Systems", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-23", "journal": {"name": "ArXiv", "volume": "abs/2205.11048"}, "authors": [{"authorId": "2905730", "name": "Wenbo Su"}, {"authorId": "1845531686", "name": "Yuanxing Zhang"}, {"authorId": "2166059753", "name": "Yufeng Cai"}, {"authorId": "2143046142", "name": "Kaixu Ren"}, {"authorId": "2145012431", "name": "Pengjie Wang"}, {"authorId": "2067568304", "name": "Hui-juan Yi"}, {"authorId": "2152604302", "name": "Yue Song"}, {"authorId": "47740650", "name": "Jing Chen"}, {"authorId": "2642895", "name": "Hongbo Deng"}, {"authorId": "2117049789", "name": "Jian Xu"}, {"authorId": "2106414498", "name": "Lin Qu"}, {"authorId": "2093245720", "name": "Bo Zheng"}], "citations": [{"paperId": "cfb0a89ce5295ddc6ea568295611f8baf17d8c25", "title": "CO2: Efficient Distributed Training with Full Communication-Computation Overlap"}, {"paperId": "f81ea4626d83adcdaa9c1a9955d0be962457e0f4", "title": "Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis"}, {"paperId": "cce1a0b71db567af283b056edb9ed5b5286fd0ba", "title": "Towards A Platform and Benchmark Suite for Model Training on Dynamic Datasets"}]}
