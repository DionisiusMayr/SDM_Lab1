{"paperId": "5c326260baa5e91a6a39c4d87ebe6770ade86816", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Effective Theory of Transformers at Initialization", "abstract": "We perform an effective-theory analysis of forward-backward signal propagation in wide and deep Transformers, i.e., residual neural networks with multi-head self-attention blocks and multilayer perceptron blocks. This analysis suggests particular width scalings of initialization and training hyperparameters for these models. We then take up such suggestions, training Vision and Language Transformers in practical setups.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science", "Physics", "Mathematics"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-04-04", "journal": {"name": "ArXiv", "volume": "abs/2304.02034"}, "authors": [{"authorId": "31461304", "name": "Emily Dinan"}, {"authorId": "8904571", "name": "Sho Yaida"}, {"authorId": "2108244542", "name": "Susan Zhang"}], "citations": [{"paperId": "3e4785c6153d7d96304911254443187f5c66d913", "title": "Geometric Dynamics of Signal Propagation Predict Trainability of Transformers"}, {"paperId": "44aa46dc87db9b26e37388151bceb3c7fc67fa1f", "title": "Principled Architecture-aware Scaling of Hyperparameters"}, {"paperId": "f7ab1305c07a0a023080bfd56d907265322285a3", "title": "On the Parameterization of Second-Order Optimization Effective Towards the Infinite Width"}, {"paperId": "f2b4e0f5c13d10bf0a667af7dbe0fcf83237c3aa", "title": "Feature Learning and Generalization in Deep Networks with Orthogonal Weights"}, {"paperId": "f5789596531fad358c3166fdb5bd72d8e661c32c", "title": "Small-scale proxies for large-scale Transformer training instabilities"}, {"paperId": "97abbabef2ca2c9fc68b489356b1c535aa425829", "title": "Neural network field theories: non-Gaussianity, actions, and locality"}, {"paperId": "c1738f21ea2460e1015d590906a4f43e155f60c8", "title": "The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit"}, {"paperId": "89c087796ca4d99393b56448d4ba47620f105eed", "title": "Principles for Initialization and Architecture Selection in Graph Neural Networks with ReLU Activations"}, {"paperId": "b3c64a046d6cfc0f55a2aebc5176bbab69a7e021", "title": "Stable and low-precision training for large-scale vision-language models"}]}
