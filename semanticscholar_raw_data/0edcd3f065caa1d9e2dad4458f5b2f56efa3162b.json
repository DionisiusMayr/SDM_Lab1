{"paperId": "0edcd3f065caa1d9e2dad4458f5b2f56efa3162b", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking", "abstract": "Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionality: Entity tracking is performed by tracking the position of the correct entity in both the original model and its fine-tuned versions. (iii) Performance boost in the fine-tuned models is primarily attributed to its improved ability to handle the augmented positional information. To uncover these findings, we employ: Patch Patching, DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms. Our findings suggest that fine-tuning enhances, rather than fundamentally alters, the mechanistic operation of the model.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-22", "journal": {"name": "ArXiv", "volume": "abs/2402.14811"}, "authors": [{"authorId": "2284985448", "name": "Nikhil Prakash"}, {"authorId": "3459255", "name": "Tamar Rott Shaham"}, {"authorId": "2232604403", "name": "Tal Haklay"}, {"authorId": "2083259", "name": "Yonatan Belinkov"}, {"authorId": "2284996653", "name": "David Bau"}], "citations": [{"paperId": "b8f280d8bf685f8da7c83068e73f000528072d6b", "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models"}, {"paperId": "0d8f8092ea3ee98a4ab5b797a7b5c0287c73ce36", "title": "Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms"}, {"paperId": "964142d8ed43e761eaedc3df1e2d670fcf3a88f3", "title": "Defending Against Unforeseen Failure Modes with Latent Adversarial Training"}, {"paperId": "c55c3ae1b7d2c9c81bda9da322e0308b865431e9", "title": "The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models"}, {"paperId": "0044140fdd4547a380b0b82052ae0b6ffd95216c", "title": "Eight Methods to Evaluate Robust Unlearning in LLMs"}, {"paperId": "f2d65ac984651c839e0901fb07c03772a0270fac", "title": "Theses in Progress"}]}
