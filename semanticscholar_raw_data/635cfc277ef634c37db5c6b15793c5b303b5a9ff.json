{"paperId": "635cfc277ef634c37db5c6b15793c5b303b5a9ff", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication", "abstract": "Many recent breakthroughs in machine learning have been enabled by the pre-trained foundation models. By scaling up model parameters, training data, and computation resources, foundation models have significantly advanced the state-of-the-art in many applications. However, it is still an open question of how to use these models to perform downstream tasks efficiently. Knowledge distillation (KD) has been explored to tackle this challenge. KD transfers knowledge from a large teacher model to a smaller student model. While KD has been successful in improving student model performance, recent research has discovered that a powerful teacher does not necessarily lead to a powerful student, due to their huge capacity gap. In addition, the potential distribution shifts between the pre-training data and downstream tasks can make knowledge transfer in KD sub-optimal for improving downstream task performance. In this paper, we extend KD with an interactive communication process to help students of downstream tasks learn effectively from pre-trained foundation models. Our design is inspired by the way humans learn from teachers who can explain knowledge in a way that meets the students' needs. Specifically, we let each model (i.e., student and teacher) train two components: (1) an encoder encoding the model's hidden states to a message and (2) a decoder decoding any messages to its own hidden states. With encoder and decoder, not only can the teacher transfer rich information by encoding its hidden states, but also the student can send messages with information of downstream tasks to the teacher. Therefore, knowledge passing from teacher to student can be tailored to the student's capacity and downstream tasks' distributions. We conducted experiments on benchmark datasets to show that our communication mechanism outperforms state-of-the-art distillation techniques.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-04", "journal": {"name": "ArXiv", "volume": "abs/2310.03188"}, "authors": [{"authorId": "2254280917", "name": "Zhe Zhao"}, {"authorId": "2255233983", "name": "Qingyun Liu"}, {"authorId": "2254238251", "name": "Huan Gui"}, {"authorId": "2254230947", "name": "Bang An"}, {"authorId": "2217278", "name": "Lichan Hong"}, {"authorId": "2254270179", "name": "Ed H. Chi"}], "citations": [{"paperId": "fe74472b8641df4512a340a2e97be6530eba35f1", "title": "Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model"}]}
