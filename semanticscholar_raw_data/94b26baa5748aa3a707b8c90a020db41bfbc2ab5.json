{"paperId": "94b26baa5748aa3a707b8c90a020db41bfbc2ab5", "publicationVenue": {"id": "7bb54772-a70c-44df-9b9d-9f3b5354c0e2", "name": "IEEE International Parallel and Distributed Processing Symposium", "type": "conference", "alternate_names": ["IEEE Int Parallel Distrib Process Symp", "International Parallel and Distributed Processing Symposium", "IPDPS", "Int Parallel Distrib Process Symp"], "url": "http://www.ipdps.org/"}, "title": "Not All Explorations Are Equal: Harnessing Heterogeneous Profiling Cost for Efficient MLaaS Training", "abstract": "Machine-Learning-as-a-Service (MLaaS) enables practitioners and AI service providers to train and deploy ML models in the cloud using diverse and scalable compute resources. A common problem for MLaaS users is to choose from a variety of training deployment options, notably scale-up (using more capable instances) and scale-out (using more instances), subject to the budget limits and/or time constraints. State-of-the-art (SOTA) approaches employ analytical modeling for finding the optimal deployment strategy. However, they have limited applicability as they must be tailored to specific ML model architectures, training framework, and hardware. To quickly adapt to the fast evolving design of ML models and hardware infrastructure, we propose a new Bayesian Optimization (BO) based method HeterBO for exploring the optimal deployment of training jobs. Unlike the existing BO approaches for general applications, we consider the heterogeneous exploration cost and machine learning specific prior to significantly improve the search efficiency. This paper culminates in a fully automated MLaaS training Cloud Deployment system (MLCD) driven by the highly efficient HeterBO search method. We have extensively evaluated MLCD in AWS EC2, and the experimental results show that MLCD outperforms two SOTA baselines, conventional BO and CherryPick, by 3.1\u00d7 and 2.34\u00d7, respectively.", "venue": "IEEE International Parallel and Distributed Processing Symposium", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-05-01", "journal": {"name": "2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "pages": "419-428"}, "authors": [{"authorId": "2114335651", "name": "Jun Yi"}, {"authorId": "2200135947", "name": "Chengliang Zhang"}, {"authorId": null, "name": "Wei Wang"}, {"authorId": "2143635672", "name": "Cheng Li"}, {"authorId": "145552742", "name": "Feng Yan"}], "citations": [{"paperId": "b8f61204b38293027d7cb609f54ab13b291d5443", "title": "Stash: A Comprehensive Stall-Centric Characterization of Public Cloud VMs for Distributed Deep Learning"}, {"paperId": "b716d20b3fdf7907fa17ad8ecf3c3de27a2433c9", "title": "Measuring the Impact of Gradient Accumulation on Cloud-based Distributed Training"}, {"paperId": "abfc1905234674ffa10a3c92bb8765172c458b40", "title": "Serving unseen deep learning models with near-optimal configurations: a fast adaptive search approach"}, {"paperId": "76ae7271caddfaf3114634bc8c6d82544c19a607", "title": "SMLT: A Serverless Framework for Scalable and Adaptive Machine Learning Design and Training"}, {"paperId": "ad905877ce76b84130b34aac6026c15d1c7ee8d2", "title": "SRIFTY: Swift and Thrifty Distributed Neural Network Training on the Cloud"}, {"paperId": "61002abf07e388f608992a7a3e72bfdd48b2eaf4", "title": "Enabling Cost-Effective, SLO-Aware Machine Learning Inference Serving on Public Cloud"}, {"paperId": "35aaa7f1fcb9a9b08fe1e4c9dcf6df0a6eb3738b", "title": "Analysis of Machine Learning as a Service"}]}
