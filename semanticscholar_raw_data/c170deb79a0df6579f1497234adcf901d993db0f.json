{"paperId": "c170deb79a0df6579f1497234adcf901d993db0f", "publicationVenue": {"id": "7c9d091e-015e-4e5d-a11f-9bc369fcf414", "name": "IEEE Transactions on Parallel and Distributed Systems", "type": "journal", "alternate_names": ["IEEE Trans Parallel Distrib Syst"], "issn": "1045-9219", "url": "http://www.computer.org/tpds", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=71"]}, "title": "Overlapping Communication With Computation in Parameter Server for Scalable DL Training", "abstract": "Scalability of distributed deep learning (DL) training with parameter server (PS) architecture is often communication constrained in large clusters. There are recent efforts that use a layer by layer strategy to overlap gradient communication with backward computation so as to reduce the impact of communication constraint on the scalability. However, the approaches could bring significant overhead in gradient communication. Meanwhile, they cannot be effectively applied to the overlap between parameter communication and forward computation. In this article, we propose and develop iPart, a novel approach that partitions communication and computation in various partition sizes to overlap gradient communication with backward computation and parameter communication with forward computation. iPart formulates the partitioning decision as an optimization problem and solves it based on a greedy algorithm to derive communication and computation partitions. We implement iPart in the open-source DL framework BigDL and perform evaluations with various DL workloads. Experimental results show that iPart improves the scalability of a cluster of 72 nodes by up to 94 percent over the default PS and 52 percent over the layer by layer strategy.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-01", "journal": {"name": "IEEE Transactions on Parallel and Distributed Systems", "pages": "2144-2159", "volume": "32"}, "authors": [{"authorId": "1891284", "name": "Shaoqi Wang"}, {"authorId": "40377061", "name": "Aidi Pi"}, {"authorId": "46224002", "name": "Xiaobo Zhou"}, {"authorId": "37089214", "name": "Jun Wang"}, {"authorId": "144486540", "name": "Chengzhong Xu"}], "citations": [{"paperId": "174f881f77cb7d7adfadb956e00193d927ac06f6", "title": "Communication Optimization Algorithms for Distributed Deep Learning Systems: A Survey"}, {"paperId": "e77c553b71704196b2ea010651dc8f4c42c3e735", "title": "A Survey on Auto-Parallelism of Large-Scale Deep Learning Training"}, {"paperId": "e8a2e678340edfcb155e18bcf203397b245f5afa", "title": "IoTSL: Toward Efficient Distributed Learning for Resource-Constrained Internet of Things"}, {"paperId": "ac54fee4b4160e6cdf56fa95652a102abacf91d5", "title": "Accelerating Distributed DNN Training via Transport Layer Scheduling"}, {"paperId": "b5ffcbc6592a1ea5a18b8ec4157d08e31c683cea", "title": "Accelerating distributed machine learning with model compression and graph partition"}, {"paperId": "df3dd07ad01770f84178ee0b9ad0448905b6a694", "title": "Fast and Accurate Deep Leakage from Gradients Based on Wasserstein Distance"}, {"paperId": "d6d894ce888ec27dec2009fb233fd5ffa416629d", "title": "Ada-Grouper: Accelerating Pipeline Parallelism in Preempted Network by Adaptive Group-Scheduling for Micro-Batches"}, {"paperId": "e5803c4c582374f14e9c31e14740ded20ccf95cd", "title": "A Utility-Based Distributed Pattern Mining Algorithm With Reduced Shuffle Overhead"}, {"paperId": "f006792c47fdf97d26d4df49db529b80bab0e5ea", "title": "Predictive GAN-Powered Multi-Objective Optimization for Hybrid Federated Split Learning"}, {"paperId": "47bb6106f35156b1b6fcc9845dd04929472ba332", "title": "Mercury: A Simple Transport Layer Scheduler to Accelerate Distributed DNN Training"}, {"paperId": "8bf0ed2fb97381c1d9376171984afca168181378", "title": "FLSGD: free local SGD with parallel synchronization"}, {"paperId": "8b67d9535efd60194d37f6c76b67d53bd8015be5", "title": "Horus: Interference-Aware and Prediction-Based Scheduling in Deep Learning Systems"}, {"paperId": "17553366aa0b3c82d6617894073934f502ad9873", "title": "DynaComm: Accelerating Distributed CNN Training between Edges and Clouds through Dynamic Communication Scheduling"}, {"paperId": "d541bd870ae3579e4b9b31645ba919c457cb3557", "title": "MIPD: An Adaptive Gradient Sparsification Framework for Distributed DNNs Training"}]}
