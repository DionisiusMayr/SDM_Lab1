{"paperId": "128217c0d1e99912ebc727c84686cc97a913b55f", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Deep Model Fusion: A Survey", "abstract": "Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1)\"Mode connectivity\", which connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion; (2)\"Alignment\"matches units between neural networks to create better conditions for fusion; (3)\"Weight average\", a classical model fusion method, averages the weights of multiple models to obtain more accurate results closer to the optimal solution; (4)\"Ensemble learning\"combines the outputs of diverse models, which is a foundational technique for improving the accuracy and robustness of the final model. In addition, we analyze the challenges faced by deep model fusion and propose possible research directions for model fusion in the future. Our review is helpful in deeply understanding the correlation between different model fusion methods and practical application methods, which can enlighten the research in the field of deep model fusion.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-09-27", "journal": {"name": "ArXiv", "volume": "abs/2309.15698"}, "authors": [{"authorId": "2243322582", "name": "Weishi Li"}, {"authorId": "2243387143", "name": "Yong Peng"}, {"authorId": "2243404565", "name": "Miao Zhang"}, {"authorId": "46573238", "name": "Liang Ding"}, {"authorId": "2247556302", "name": "Han Hu"}, {"authorId": "2248152216", "name": "Li Shen"}], "citations": [{"paperId": "b64f9f519384802545ff74b9ed820d1c90e2255a", "title": "MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks"}, {"paperId": "d77527b74884ec647547d630839131f995dfb39f", "title": "Training-Free Pretrained Model Merging"}, {"paperId": "9a4f03d132869db4a05c6789a4a1076575e2258b", "title": "Representation Surgery for Multi-Task Model Merging"}, {"paperId": "aa0f298397a7b9cc235bc122924c0179c7fd9350", "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts"}, {"paperId": "0251bb95be75d472c8d5b873751615e7fe2feb1d", "title": "A Comprehensive Study of Knowledge Editing for Large Language Models"}, {"paperId": "05d09aebae99f024a5b41a67bb444f0aadcf262f", "title": "Multi-Task Model Fusion with Mixture of Experts Structure"}, {"paperId": "192028b7f0b920ce19cedc2dadc5992d71bc8ff6", "title": "Concrete Subspace Learning based Interference Elimination for Multi-task Model Fusion"}, {"paperId": "acea772e6f4f375f66c58bd78feb8eb6ba93cd1f", "title": "Fusing Multiple Algorithms for Heterogeneous Online Learning"}, {"paperId": "3f056c09f2cf8a23c76e57d125f4ad1e89c36fa8", "title": "Applications of Spiking Neural Networks in Visual Place Recognition"}, {"paperId": "1b97570049b72de59c6c90eff9499b0fc8a299ac", "title": "Merging Experts into One: Improving Computational Efficiency of Mixture of Experts"}, {"paperId": "7ee9301f239556896403e00404db3cfef3b60903", "title": "Learn From Model Beyond Fine-Tuning: A Survey"}, {"paperId": "a09cbe54f842b9b243de4307349cb2800f3044d3", "title": "Parameter Efficient Multi-task Model Fusion with Partial Linearization"}, {"paperId": "2ccb452691a5d3e3b600caaec119df9ff44688bd", "title": "AdaMerging: Adaptive Model Merging for Multi-Task Learning"}, {"paperId": "2651f0179874bd010f58d2c9fa7d118807c80977", "title": "TIES-Merging: Resolving Interference When Merging Models"}]}
