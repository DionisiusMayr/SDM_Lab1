{"paperId": "5b886ab54f5283096da0c42c48ec2befdf288dbe", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Low-Latency ML Inference by Grouping Correlated Data Objects and Computation", "abstract": "ML inference workflows often require low latency and high throughput, yet we lack good options for addressing this need. Techniques that reduce latency in other streaming settings (such as caching and optimization-driven scheduling) are of limited value because ML data dependencies are often very large and can change dramatically depending on the triggering event. In this work, we propose a novel correlation grouping mechanism that makes it easier for developers to express application-specific data access correlations, enabling coordinated management of data objects in server clusters hosting streaming inference tasks. Experiments based on a latency-sensitive ML-based application confirm the limitations of standard techniques while showing that our solution yields dramatically better performance. The proposed mechanism is able to maintain significantly lower and more consistent latency, achieves higher node utilization as workload and scale-out increase, and yet requires only minor changes to the code implementing the application.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-30", "journal": {"name": "ArXiv", "volume": "abs/2312.11488"}, "authors": [{"authorId": "2238047375", "name": "Thiago Garrett"}, {"authorId": "2256481601", "name": "Weijia Song"}, {"authorId": "2238052053", "name": "R. Vitenberg"}, {"authorId": "2256273701", "name": "Ken Birman"}], "citations": []}
