{"paperId": "114cc14951f65f4c3d0d4ac8e6731e8705378715", "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "title": "DS-TOD: Efficient Domain Specialization for Task-Oriented Dialog", "abstract": "Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains. In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD. Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit \u2013 resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively. We further propose a resource-efficient and modular domain specialization by means of domain adapters \u2013 additional parameter-light layers in which we encode the domain knowledge. Our experiments with prominent TOD tasks \u2013 dialog state tracking (DST) and response retrieval (RR) \u2013 encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance.", "venue": "Findings", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-10-15", "journal": {"pages": "891-904"}, "authors": [{"authorId": "11618346", "name": "Chia-Chien Hung"}, {"authorId": "29891652", "name": "Anne Lauscher"}, {"authorId": "2029669151", "name": "Simone Paolo Ponzetto"}, {"authorId": "2472657", "name": "Goran Glavas"}], "citations": [{"paperId": "a2c59567ceff5debe2cea463162d7beb5a4abfec", "title": "ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler"}, {"paperId": "3746a3b18302a4c92c494019621b5ca57eeb617c", "title": "Granular Change Accuracy: A More Accurate Performance Metric for Dialogue State Tracking"}, {"paperId": "e94847afe66490ff02f5a47bc2979a3a0f5a0e63", "title": "Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain"}, {"paperId": "c0a30b378bf897412426ba28e65c6392a65859bc", "title": "What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition"}, {"paperId": "ebdb84c437068f615f9fdf703415580cbb1d2aad", "title": "SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU"}, {"paperId": "dca146d9aa6f15f28cec21e96ce3c08c282e95a6", "title": "Multi-Source (Pre-)Training for Cross-Domain Measurement, Unit and Context Extraction"}, {"paperId": "48a3e2cfca3552ae04a021c7b0e6fec1a320a1eb", "title": "Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection"}, {"paperId": "de24a9888816de03dfabd9215ceb3443981b8e7e", "title": "TADA: Efficient Task-Agnostic Domain Adaptation for Transformers"}, {"paperId": "4fd2e9be2b785c76876a3a910669ffe1f36b5175", "title": "Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers"}, {"paperId": "64fed8c2cbf75d20f27a450e2013a36d0fd8fc9a", "title": "Back to the Future: On Potential Histories in NLP"}, {"paperId": "f07af67d3b808656dc9095e1f2e51a7d118a3d8a", "title": "Massively Multilingual Lexical Specialization of Multilingual Transformers"}, {"paperId": "ade14e974de3cde4314bea20c6bffdbc14c814b5", "title": "On the Limitations of Sociodemographic Adaptation with Transformers"}, {"paperId": "20e34ef4104d7e14f3ddc921de5946d3c9ebe00b", "title": "\u201cDo you follow me?\u201d: A Survey of Recent Approaches in Dialogue State Tracking"}, {"paperId": "e1ec84ee5c64566154f48576ac17968c39413ef0", "title": "Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog"}, {"paperId": "e76177b69a9881daa104cc2731b21b2732998abe", "title": "Parameter-Efficient Abstractive Question Answering over Tables or Text"}, {"paperId": "498b44a3b93a0733547af8dfcffb9a4f9292d8dd", "title": "Improved and Efficient Conversational Slot Labeling through Question Answering"}, {"paperId": "4ff9eb76d5243b92da102889d3005c55e362766b", "title": "Geographic Adaptation of Pretrained Language Models"}, {"paperId": "7ead21c9e67096eefa5e2cf87b0c0e4b61a28bb7", "title": "Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender"}, {"paperId": "9d6e3bede351f0beee528179da84a4c05e80ba1a", "title": "Karde\u015f-NLU: Transfer to Low-Resource Languages with Big Brother\u2019s Help \u2013 A Benchmark and Evaluation for Turkic Languages"}, {"paperId": "ac1511b4b406ba8b707355dd6b9fdf704155d475", "title": "Value type: the bridge to a better DST model"}, {"paperId": "870958cacf3a7a5d64aec977c489d4a2988c8e6d", "title": "Policy Domain Prediction from Party Manifestos with Adapters and Knowledge Enhanced Transformers"}, {"paperId": "bb04387138e997b5a3e8cdd0c4c90dd81c45debb", "title": "\u00ab Est-ce que tu me suis ? \u00bb : une revue du suivi de l\u2019\u00e9tat du dialogue (\u201cDo you follow me ?\" : a review of dialogue state tracking )"}, {"paperId": "f6fbf47adcb257f172c419ef935493c5345ad803", "title": "BabelBERT: Massively Multilingual Transformers Meet a Massively Multilingual Lexical Resource"}, {"paperId": "b79830f00d2f0783042e688e1baf4291f8fd317d", "title": "Adapters for Resource-Efficient Deployment of Intent Detection Models"}, {"paperId": "07c70984e2e0ce1784f5651d39d60b416faac011", "title": "Granular Change Accuracy: A more accurate performance metric for Dialogue State Tracking"}, {"paperId": "e4d6e27daf9a0dc45a07f9afa01fefbd19eccc8d", "title": "Parameter-Efficient Abstractive Question Answering over Tables and over Text"}]}
