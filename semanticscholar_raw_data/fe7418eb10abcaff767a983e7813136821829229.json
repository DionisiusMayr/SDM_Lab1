{"paperId": "fe7418eb10abcaff767a983e7813136821829229", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "A Survey on Decentralized Federated Learning", "abstract": "In recent years, federated learning (FL) has become a very popular paradigm for training distributed, large-scale, and privacy-preserving machine learning (ML) systems. In contrast to standard ML, where data must be collected at the exact location where training is performed, FL takes advantage of the computational capabilities of millions of edge devices to collaboratively train a shared, global model without disclosing their local private data. Specifically, in a typical FL system, the central server acts only as an orchestrator; it iteratively gathers and aggregates all the local models trained by each client on its private data until convergence. Although FL undoubtedly has several benefits over traditional ML (e.g., it protects private data ownership by design), it suffers from several weaknesses. One of the most critical challenges is to overcome the centralized orchestration of the classical FL client-server architecture, which is known to be vulnerable to single-point-of-failure risks and man-in-the-middle attacks, among others. To mitigate such exposure, decentralized FL solutions have emerged where all FL clients cooperate and communicate without a central server. This survey comprehensively summarizes and reviews existing decentralized FL approaches proposed in the literature. Furthermore, it identifies emerging challenges and suggests promising research directions in this under-explored domain.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-08-08", "journal": {"name": "ArXiv", "volume": "abs/2308.04604"}, "authors": [{"authorId": "2212869246", "name": "Edoardo Gabrielli"}, {"authorId": "82027033", "name": "Giovanni Pica"}, {"authorId": "2651748", "name": "Gabriele Tolomei"}], "citations": [{"paperId": "4561365bb73ac12b68e47c066c8f06f83e272fed", "title": "Fault-Tolerant Vertical Federated Learning on Dynamic Networks"}, {"paperId": "8cd0aa493488541bc85fc123aae22d8e3ab872df", "title": "Efficient Communication in Federated Learning Using Floating-Point Lossy Compression"}, {"paperId": "aee52d731e47eb3cff38cc566eeb0b3eceb0635b", "title": "The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies"}, {"paperId": "62ab13fa64c3dfcd90af7fe0dc282e89c6fd93fc", "title": "Asymmetrically Decentralized Federated Learning"}, {"paperId": "d70fe5a76426b303757917fdd1383686e5475409", "title": "Decentralized Constraint-Coupled Optimization with Inexact Oracle"}, {"paperId": "3915c7433cbd1df21e16a320b52da6feaa6a1b80", "title": "Federated Learning for Computer Vision"}, {"paperId": "b169cbff7d5a11afac18f929d5c69ea0933a4da1", "title": "GradientCoin: A Peer-to-Peer Decentralized Large Language Models"}, {"paperId": "34444d6eb5dbb3840ade353a13a2ef36b48b1f23", "title": "DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning"}]}
