{"paperId": "730970e348067fbd096e438f72bae81906b8938c", "publicationVenue": {"id": "8a6e871b-6c73-419c-98a8-27e437270a12", "name": "ICON", "type": "conference", "alternate_names": ["ICNLP", "Int conf nat lang process", "International conference natural language processing", "Int Conf Nat Lang Process", "TAL", "IEEE International Conference on Networks", "IEEE Int Conf Netw", "International Conference on Natural Language Processing"], "issn": "1361-8113", "url": "http://www.icohtec.org/publications-icon.html", "alternate_urls": ["https://ieeexplore.ieee.org/xpl/conhome/1000494/all-proceedings", "http://www.wikicfp.com/cfp/program?id=1360", "http://www.jstor.org/journal/icon"]}, "title": "Research on Array Circuit Design Based on In-Memory Computing", "abstract": "In recent years, with the continuous development and deepening of artificial intelligence, the amount of data required has increased dramatically. With the sharp increase in the amount of data, data access between the memory and the computing unit will cause a lot of energy consumption and increase the read and write latency, that is, the \u201cmemory wall\u201d problem occurs. In order to solve the extra energy loss caused by the \u201cmemory wall\u201d problem in the Von neumann architecture and to achieve better acceleration for neural network algorithms, the in-memory computing architecture is considered a promising processor architecture for future big data applications. In this thesis, we conducted research related to the in-memory computing neural network accelerator architecture based on three kinds of memories as Static Random-Access Memory (SRAM), Resistive Random-Access Memory (ReRAM), and Ferro-electric Field Effect Transistor (FeFET). For non volatile memory, such as ReRAM, FeFET, its non-volatile, low power consumption and other characteristics show obvious advantages. Using VGG-8, VGG-16, and AlexNet networks, the designed in-memory computing architecture is validated on three different datasets. The performance of the designed gas pedal is evaluated in terms of the acceleration effect of each layer and the number of Tile units used in each layer, read/write latency, read/write energy consumption, system occupied area, energy efficiency ratio, and other indicators, the experimental results show that the neural network accelerator based on in-memory calculation designed in this thesis has reached a relatively advanced level.", "venue": "ICON", "year": 2022, "fieldsOfStudy": null, "publicationTypes": ["Conference"], "publicationDate": "2022-03-01", "journal": {"name": "2022 4th International Conference on Natural Language Processing (ICNLP)", "pages": "537-541"}, "authors": [{"authorId": "2144114169", "name": "Jinlong Wu"}, {"authorId": "2075349908", "name": "He Wen"}, {"authorId": "95326542", "name": "Wenting Pang"}, {"authorId": "2185626111", "name": "Gege Chong"}], "citations": []}
