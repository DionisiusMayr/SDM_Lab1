{"paperId": "deddf0763b1adf673cbf9d13e72f05c0e422ffe5", "publicationVenue": null, "title": "Genomics Analyser: A Big Data Framework for Analysing Genomics Data", "abstract": "Abstract Genomics data is unstructured and mostly stored on hard disks. It is both technically and culturally residing in big data domain due to the challenges of volume, velocity and variety. Huge volumes of data are generated from diverse sources in different formats and at a high frequency. Appropriate data models are required to accommodate these data formats for analysing and producing required results with a quick response time. Genomics data can be analysed for a variety of purposes. Existing genomics data analysis pipelines are disk I/O intensive and focus on optimizing data processing for individual analysis tasks. Intensive disk I/O operations and focus on optimizing individual analysis tasks are the biggest bottleneck of existing genomics analysis pipelines. Making any updates in genomics data require reading the whole data set again. In this paper, we present a genomics data analysis framework that addresses both the issues of existing genomics analysis pipelines. It reads unstructured genomics data from sources, transforms it in a structured format and stores this data into a NoSQL database. In this way, genomics data can be queried like any other data and an update in the genomics data does not require reading the whole data set. The framework also presents an efficient analysis pipeline for analysing the genomics data for a variety of purposes like genotype clustering, gene expression microarrays, chromosome variations or gene linkage analysis. A case study of genotype clustering is presented to demonstrate and evaluate the effectiveness of the presented framework. Our results show that the framework improves overall performance of the genomics data analysis pipeline by 49% from existing genomics data analysis pipelines. Furthermore, our approach is robust and is able sustain high performance with high system workloads.", "venue": "BDCAT", "year": 2017, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2017-12-05", "journal": {"name": "Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies"}, "authors": [{"authorId": "1712242063", "name": "Tariq Abdullah"}, {"authorId": "1580475220", "name": "Ahmed Ahmet"}], "citations": [{"paperId": "cde9ea73cad7dba1a16b5e6b13dcb61562a4b090", "title": "State of the art on system architectures for data integration"}, {"paperId": "6e3b3d4a6dfef01d762a12672e61dc4bf513a107", "title": "Extracting Insights: A Data Centre Architecture Approach in Million Genome Era"}, {"paperId": "37111dd2d0ea662a35ffc619e3829cd9f72101c9", "title": "Information Requirements for Big Data Projects: A Review of State-of-the-Art Approaches"}, {"paperId": "8fdc981eac789a4c3526d83f007a3819b2d57872", "title": "Transactions on Large-Scale Data- and Knowledge-Centered Systems XLVI"}]}
