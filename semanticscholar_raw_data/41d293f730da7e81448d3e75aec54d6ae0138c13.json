{"paperId": "41d293f730da7e81448d3e75aec54d6ae0138c13", "publicationVenue": {"id": "9448f839-459b-45f3-8573-5eff3f032334", "name": "USENIX Annual Technical Conference", "type": "conference", "alternate_names": ["USENIX Annu Tech Conf", "USENIX", "USENIX ATC"], "url": "https://www.usenix.org/conferences/byname/131"}, "title": "Cachew: Machine Learning Input Data Processing as a Service", "abstract": "Processing input data plays a vital role in ML training, im-pacting accuracy, throughput, and cost. The input pipeline, which is responsible for feeding data-hungry GPUs/TPUs with training examples, is a common bottleneck. Alleviating data stalls is critical yet challenging for users. While today\u2019s frameworks provide mechanisms to maximize input pipeline throughput (e.g., distributing data processing on remote CPU workers and/or reusing cached data transformations), leveraging these mechanisms to jointly optimize training time and cost is non-trivial. Users face two key challenges. First, ML schedulers focus on GPU/TPU resources, leaving users on their own to optimize multi-dimensional resource allocations for data processing. Second, input pipelines often consume excessive compute power to repeatedly transform the same data. Deciding which source or transformed data to cache is non-trivial: large datasets are expensive to store, the compute time saved by caching is not always the bottleneck for end-to-end training, and transformations may not be deterministic, hence reusing transformed data can impact accuracy. We propose Cachew, a fully-managed service for ML data processing. Cachew dynamically scales distributed resources for data processing to avoid stalls in training jobs. The service also automatically applies caching when and where it is performance/cost-effective to reuse preprocessed data within and across jobs. Our key contributions are autoscaling and autocaching policies, which leverage domain-specific metrics collected at data workers and training clients (rather than generic resource utilization metrics) to minimize training time and cost. Compared to scaling workers with Kubernetes, Cachew\u2019s policies reduce training time by up to 4.1 \u00d7 and training cost by 1.1 \u00d7 to 3.8 \u00d7 . Abstract The artifact consists of the source code of Cachew 2 , the Cachew client binaries 3 , as well as scripts for building wheel files and Docker images. We also provide reference scripts for deploying GCE VMs for evaluation and for running the some representative experiments 4 . Do note that these scripts might not work as they depend on resources that might not be public. In these cases, experiment VMs will have to be manually set up. The evaluation focuses on reproducing key experiments and their respective results which demonstrate how the main contributions of Cachew work:", "venue": "USENIX Annual Technical Conference", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "689-706"}, "authors": [{"authorId": "2285439952", "name": "Dan Graur"}, {"authorId": "2187456856", "name": "Damien Aymon"}, {"authorId": "2145152776", "name": "Dan Kluser"}, {"authorId": "150317176", "name": "Tanguy Albrici"}, {"authorId": "1780447", "name": "C. Thekkath"}, {"authorId": "2285439911", "name": "Ana Klimovic"}], "citations": [{"paperId": "50afc14226af83b4fc4276601bb310cc8627665b", "title": "Objcache: An Elastic Filesystem over External Persistent Storage for Container Clusters"}, {"paperId": "a106da34faf0f15e1d33622b52bc0816e37edca9", "title": "Unleashing True Utility Computing with Quicksand"}, {"paperId": "5d5bbcb7b6072f3cac430855449d16719f47cbfc", "title": "Leave Nothing Idle: Filling Datacenter Resource Utilization Gaps with Quicksand"}]}
