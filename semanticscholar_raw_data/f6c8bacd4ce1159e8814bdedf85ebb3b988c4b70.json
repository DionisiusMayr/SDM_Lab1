{"paperId": "f6c8bacd4ce1159e8814bdedf85ebb3b988c4b70", "publicationVenue": null, "title": "Non-parametric Bayesian models for structured output prediction", "abstract": "Structured output prediction is a machine learning tasks in which an input object is not just assigned a single class, as in classi cation, but multiple, interdependent labels. This means that the presence or value of a given label a ects the other labels, for instance in text labelling problems, where output labels are applied to each word, and their interdependencies must be modelled. Non-parametric Bayesian (NPB) techniques are probabilistic modelling techniques which have the interesting property of allowing model capacity to grow, in a controllable way, with data complexity, while maintaining the advantages of Bayesian modelling. In this thesis, we develop NPB algorithms to solve structured output problems. We rst study a map-reduce implementation of a stochastic inference method designed for the in nite hidden Markov model, applied to a computational linguistics task, part-of-speech tagging. We show that mainstream map-reduce frameworks do not easily support highly iterative algorithms. The main contribution of this thesis consists in a conceptually novel discriminative model, GPstruct. It is motivated by labelling tasks, and combines attractive properties of conditional random elds (CRF), structured support vector machines, and Gaussian process (GP) classi ers. In probabilistic terms, GPstruct combines a CRF likelihood with a GP prior on factors; it can also be described as a Bayesian kernelized CRF. To train this model, we develop a Markov chain Monte Carlo algorithm based on elliptical slice sampling and investigate its properties. We then validate it on real data experiments, and explore two topologies: sequence output with text labelling tasks, and grid output with semantic segmentation of images. The latter case poses scalability issues, which are addressed using likelihood approximations and an ensemble method which allows distributed inference and prediction. The experimental validation demonstrates: (a) the model is exible and its constituent parts are modular and easy to engineer; (b) predictive performance and, most crucially, the probabilistic calibration of predictions are better than or equal to that of competitor models, and (c) model hyperparameters can be learnt from data.", "venue": "", "year": 2018, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2018-07-01", "journal": {"name": "", "volume": ""}, "authors": [{"authorId": "3237630", "name": "S\u00e9bastien Brati\u00e8res"}], "citations": []}
