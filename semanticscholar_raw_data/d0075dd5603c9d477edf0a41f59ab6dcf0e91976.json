{"paperId": "d0075dd5603c9d477edf0a41f59ab6dcf0e91976", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models", "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\\rightarrow$English) and a stronger distribution shift (English$\\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-03-13", "journal": {"name": "ArXiv", "volume": "abs/2403.08763"}, "authors": [{"authorId": "2291081890", "name": "Adam Ibrahim"}, {"authorId": "2217687159", "name": "Benjamin Th'erien"}, {"authorId": "2291113135", "name": "Kshitij Gupta"}, {"authorId": "2249531331", "name": "Mats L. Richter"}, {"authorId": "2291068795", "name": "Quentin Anthony"}, {"authorId": "26418330", "name": "Timoth\u00e9e Lesort"}, {"authorId": "2064781956", "name": "Eugene Belilovsky"}, {"authorId": "2239232896", "name": "Irina Rish"}], "citations": [{"paperId": "ab8d436e1792907294cb7707c94b3eb66ec0da17", "title": "Poro 34B and the Blessing of Multilinguality"}, {"paperId": "6fd5dbea7588ee6bca703aa3fea9a487006dba29", "title": "Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order"}, {"paperId": "ff703d46e90ac10812f5a5b11bc133b3eaa413a8", "title": "CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning"}]}
