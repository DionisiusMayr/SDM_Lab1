{"paperId": "5b1d02bfce832b8ab41b5209e637a9f42c26d912", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "abstract": "The vast majority of today's large language models are English-centric, having been pretrained predominantly on English text. Yet, in order to meet user expectations, models need to be able to respond appropriately in multiple languages once deployed in downstream applications. Given limited exposure to other languages during pretraining, cross-lingual transfer is important for achieving decent performance in non-English settings. In this work, we investigate just how much multilinguality is required during finetuning to elicit strong cross-lingual generalisation across a range of tasks and target languages. We find that, compared to English-only finetuning, multilingual instruction tuning with as few as three languages significantly improves a model's cross-lingual transfer abilities on generative tasks that assume input/output language agreement, while being of less importance for highly structured tasks. Our code and data is available at https://github.com/ZurichNLP/multilingual-instruction-tuning.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-12-20", "journal": {"name": "ArXiv", "volume": "abs/2312.12683"}, "authors": [{"authorId": "1583958852", "name": "Tannon Kew"}, {"authorId": "2187058204", "name": "Florian Schottmann"}, {"authorId": "2066518243", "name": "Rico Sennrich"}], "citations": [{"paperId": "223cab066bd3b0352e01954736eaa1517f6db687", "title": "Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models?"}, {"paperId": "ab8d436e1792907294cb7707c94b3eb66ec0da17", "title": "Poro 34B and the Blessing of Multilinguality"}, {"paperId": "6fd5dbea7588ee6bca703aa3fea9a487006dba29", "title": "Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order"}, {"paperId": "55267f48ac64c383af9905eadc9a8bd65131dc29", "title": "Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?"}, {"paperId": "ee414ff78922ac70dfb31abfff37bd40c661ac92", "title": "Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models"}, {"paperId": "ad9330c7cb31adb2197dae8f7f9998142e071857", "title": "Zero-shot cross-lingual transfer in instruction tuning of large language model"}, {"paperId": "116a5a23bf9a1486e41a1c3a140cc25f33a3ecea", "title": "Analysis of Multi-Source Language Training in Cross-Lingual Transfer"}, {"paperId": "5e5f04a5ab1d22ffad0e96585469e269369ec676", "title": "Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages"}, {"paperId": "b2df275f024c6c427698d3de865df2d4d0274aed", "title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?"}, {"paperId": "418b8f3b2b2b0126cd2f4732ad0a3172c60bc6ea", "title": "The Impact of Language Adapters in Cross-Lingual Transfer for NLU"}, {"paperId": "14336fbb221da89d77b1e54f1d477c0a8cb0ef85", "title": "LangBridge: Multilingual Reasoning Without Multilingual Supervision"}, {"paperId": "cd7c9fbb2acab241b0b4c7837877a19335c7284c", "title": "Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca"}]}
