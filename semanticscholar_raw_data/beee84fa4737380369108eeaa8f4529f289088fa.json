{"paperId": "beee84fa4737380369108eeaa8f4529f289088fa", "publicationVenue": {"id": "82901c74-dc2e-4d08-b633-637c7e209284", "name": "Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication", "type": "conference", "alternate_names": ["Conf Appl Technol Archit Protoc Comput Commun", "ACM Special Interest Group on Data Communication", "SIGCOMM", "ACM Sp\u00e9c Interest Group Data Commun"], "url": "https://en.wikipedia.org/wiki/SIGCOMM"}, "title": "Exoshuffle: An Extensible Shuffle Architecture", "abstract": "Shuffle is one of the most expensive communication primitives in distributed data processing and is difficult to scale. Prior work addresses the scalability challenges of shuffle by building monolithic shuffle systems. These systems are costly to develop, and they are tightly integrated with batch processing frameworks that offer only high-level APIs such as SQL. New applications, such as ML training, require more flexibility and finer-grained interoperability with shuffle. They are often unable to leverage existing shuffle optimizations. We propose an extensible shuffle architecture. We present Exoshuffle, a library for distributed shuffle that offers competitive performance and scalability as well as greater flexibility than monolithic shuffle systems. We design an architecture that decouples the shuffle control plane from the data plane without sacrificing performance. We build Exoshuffle on Ray, a distributed futures system for data and ML applications, and demonstrate that we can: (1) rewrite previous shuffle optimizations as application-level libraries with an order of magnitude less code, (2) achieve shuffle performance and scalability competitive with monolithic shuffle systems, and break the CloudSort record as the world's most cost-efficient sorting system, and (3) enable new applications such as ML training to easily leverage scalable shuffle.", "venue": "Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2022-03-09", "journal": {"name": "Proceedings of the ACM SIGCOMM 2023 Conference"}, "authors": [{"authorId": "2113951250", "name": "Frank Sifei Luan"}, {"authorId": "2117867", "name": "Stephanie Wang"}, {"authorId": "91383149", "name": "Samyukta Yagati"}, {"authorId": "2158598589", "name": "Sean Kim"}, {"authorId": "2158366607", "name": "Kenneth Lien"}, {"authorId": "2199840167", "name": "Isaac Ong"}, {"authorId": "2066927871", "name": "Tony Hong"}, {"authorId": "152898821", "name": "S. Cho"}, {"authorId": "40274002", "name": "Eric Liang"}, {"authorId": "144467753", "name": "I. Stoica"}], "citations": [{"paperId": "a96648f16b9704bc04764753e4145f3065659998", "title": "RINAS: Training with Dataset Shuffling Can Be General and Fast"}]}
