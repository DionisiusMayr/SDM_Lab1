{"paperId": "b9b8f753ae32654d46e396a11ea2a88a0312c1a8", "publicationVenue": {"id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e", "name": "Proceedings of the VLDB Endowment", "type": "journal", "alternate_names": ["Proceedings of The Vldb Endowment", "Proc VLDB Endow", "Proc Vldb Endow"], "issn": "2150-8097", "url": "http://dl.acm.org/toc.cfm?id=J1174", "alternate_urls": ["http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"]}, "title": "Serving and Optimizing Machine Learning Workflows on Heterogeneous Infrastructures", "abstract": "With the advent of ubiquitous deployment of smart devices and the Internet of Things, data sources for machine learning inference have increasingly moved to the edge of the network. Existing machine learning inference platforms typically assume a homogeneous infrastructure and do not take into account the more complex and tiered computing infrastructure that includes edge devices, local hubs, edge datacenters, and cloud datacenters. On the other hand, recent AutoML efforts have provided viable solutions for model compression, pruning and quantization for heterogeneous environments; for a machine learning model, now we may easily find or even generate a series of model variants with different tradeoffs between accuracy and efficiency.\n We design and implement JellyBean, a system for serving and optimizing machine learning inference workflows on heterogeneous infrastructures. Given service-level objectives (e.g., throughput, accuracy), JellyBean picks the most cost-efficient models that meet the accuracy target and decides how to deploy them across different tiers of infrastructures. Evaluations show that JellyBean reduces the total serving cost of visual question answering by up to 58% and vehicle tracking from the NVIDIA AI City Challenge by up to 36%, compared with state-of-the-art model selection and worker assignment solutions. JellyBean also outperforms prior ML serving systems (e.g., Spark on the cloud) up to 5x in serving costs.", "venue": "Proceedings of the VLDB Endowment", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-10", "journal": {"name": "Proc. VLDB Endow.", "pages": "406-419", "volume": "16"}, "authors": [{"authorId": "1892395585", "name": "Yongji Wu"}, {"authorId": "47992280", "name": "Matthew Lentz"}, {"authorId": "2720913", "name": "Danyang Zhuo"}, {"authorId": "2143370350", "name": "Yao Lu"}], "citations": [{"paperId": "ea63aefb4979c9e46e82247b364afb391ac8cd6a", "title": "Hydro: Adaptive Query Processing of ML Queries"}, {"paperId": "253737906fdf10851ce4360e915aa1c7f5dfa307", "title": "Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native"}, {"paperId": "886e9f83c61bdc489d9ef3423d378aad0a6d686e", "title": "OTAS: An Elastic Transformer Serving System via Token Adaptation"}, {"paperId": "4057dac4f2369ee4b072854e219c71364c9d0d87", "title": "VQPy: An Object-Oriented Approach to Modern Video Analytics"}, {"paperId": "78375c1fe88a6e66b88af90b37476fa6301bd6b4", "title": "mSIRM: Cost-Efficient and SLO-aware ML Load Balancing on Fog and Multi-Cloud Network"}, {"paperId": "4413fa78a0e655f8efdcebfdd4ee1d32ad064e77", "title": "SIRM: Cost efficient and SLO aware ML prediction on Fog-Cloud Network"}, {"paperId": "1add047298154e27d8ef732e6c73c2f1c91eba23", "title": "Optimizing Video Analytics with Declarative Model Relationships"}, {"paperId": "d473c4de05951f135d947ae8b32933c933c8abeb", "title": "Metadata Representations for Queryable Repositories of Machine Learning Models"}]}
