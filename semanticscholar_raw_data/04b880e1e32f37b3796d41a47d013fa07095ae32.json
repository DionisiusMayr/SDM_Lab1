{"paperId": "04b880e1e32f37b3796d41a47d013fa07095ae32", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning", "abstract": "Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution, and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts of user prompts, and promotes the response generation constantly conditioned on the instructions. 2) It encourages the self-attention heads to capture more word-word relationships about instruction verbs. 3) It encourages the feed-forward networks to rotate their pre-trained knowledge toward user-oriented tasks. These insights contribute to a more comprehensive understanding of instruction tuning and lay the groundwork for future work that aims at explaining and optimizing LLMs for various applications. Our code and data are publicly available at https://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-30", "journal": {"name": "ArXiv", "volume": "abs/2310.00492"}, "authors": [{"authorId": "2145346360", "name": "Xuansheng Wu"}, {"authorId": "2087264100", "name": "Wenlin Yao"}, {"authorId": "2108276402", "name": "Jianshu Chen"}, {"authorId": "2243367575", "name": "Xiaoman Pan"}, {"authorId": "2250363276", "name": "Xiaoyang Wang"}, {"authorId": "2256183798", "name": "Ninghao Liu"}, {"authorId": "2256336899", "name": "Dong Yu"}], "citations": [{"paperId": "748cd278c79118d594c3f926f271390f7458b20a", "title": "Steering Conversational Large Language Models for Long Emotional Support Conversations"}, {"paperId": "1834f8126e97057e321149b50e342754a096d14d", "title": "Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge"}, {"paperId": "f7a0501a246882103bc84cbc4f7270d1e7e428a8", "title": "Enabling Large Language Models to Learn from Rules"}, {"paperId": "26089bdfdbca1e6eaaceca71e3116b715bec6d47", "title": "Explainability for Large Language Models: A Survey"}, {"paperId": "f5ec0b22930faf15c3a7912bda367e8a9f4c8bd4", "title": "On the Unexpected Abilities of Large Language Models"}]}
