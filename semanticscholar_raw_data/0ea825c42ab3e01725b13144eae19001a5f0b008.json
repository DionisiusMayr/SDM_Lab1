{"paperId": "0ea825c42ab3e01725b13144eae19001a5f0b008", "publicationVenue": {"id": "d13e941e-4cac-4f1d-bdca-77d927e31f1b", "name": "ACM Symposium on Cloud Computing", "type": "conference", "alternate_names": ["System-on-Chip Conference", "ACM Symp Cloud Comput", "Syst Conf", "Symp Cloud Comput", "Annual IEEE International System-on-Chip Conference", "Symposium on Cloud Computing", "Annu IEEE Int Syst Conf", "SoCC"], "url": "http://www.ieee-socc.org/"}, "title": "Plumb: Efficient Processing of Multi-User Pipelines", "abstract": "As the field of big data analytics matures, workflows are increasingly complex and often include components that are shared by different users. Individual workflows often include multiple stages, and when groups build on each other\u2019s work it is easy to lose track of computation that may be shared across different groups. We propose Plumb, a workflow system for multi-stage workflow where parts of computation and output are shared across different groups. Plumb focuses on blocked, streaming workflows, a middle ground between large-file batch processing and small-record streaming. A particular challenge with this problem domain is structural and computational skew, where the computation of different stages and of different blocks in a stage can vary by a factor of ten due to differences in the work or data. Plumb\u2019s first goal is to identify duplication of computation and storage that can occur when different groups share components of a pipeline. When different users are responsible for different portions of the workflow, work done in common stages will be duplicated if each user assumes they begin with raw input, particularly as the workflow evolves over the course of development. This problem of computational sharing was recently recognized at Microsoft [2]; we identify duplication both in computation and in storage of intermediate results. While databases sometimes save and share intermediate results, automated discovery is more challenging in today\u2019s loosely structured big-data workflows, where processing modules are largely opaque to the system. The second problem we take on is skew. Prior work has identified data skew, where many data items fall into one processing bin, slowing the overall workflow [1]. We identify and address two new types of skew: computational skew and structural skew. Computational skew occurs when a bin of data takes extra long to process, not necessarily because there is more data, but because the data interacts with the processing algorithms to take extra time. Structural skew occurs when one stage of the processing pipeline is noticeably slower than other stages. We address both of these types of skew in Plumb by scheduling additional processing elements when one data block or one stage falls behind. Plumb decouples processing for each stage of the workflow, buffering output when required and allowing each stage to be scheduled independently. However, to avoid the cost of data buffering, Plumb also allows stages to run concurrently when they are well matched. This decoupling also addresses computational skew, since additional computation can be brought to bear when specific data inputs take extra time. Plumb is designed for large-block, streaming workloads. Traditional map-reduce has focused on batch processing, and systems", "venue": "ACM Symposium on Cloud Computing", "year": 2018, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2018-10-11", "journal": {"name": "Proceedings of the ACM Symposium on Cloud Computing"}, "authors": [{"authorId": "49382326", "name": "A. Qadeer"}, {"authorId": "46351573", "name": "J. Heidemann"}], "citations": []}
