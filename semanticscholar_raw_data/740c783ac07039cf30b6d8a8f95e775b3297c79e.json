{"paperId": "740c783ac07039cf30b6d8a8f95e775b3297c79e", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Language Models Represent Space and Time", "abstract": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual\"space neurons\"and\"time neurons\"that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-03", "journal": {"name": "ArXiv", "volume": "abs/2310.02207"}, "authors": [{"authorId": "2056771333", "name": "Wes Gurnee"}, {"authorId": "2253461466", "name": "Max Tegmark"}], "citations": [{"paperId": "bf64e04ad6015fd623e64784f728eeb4cc8104b6", "title": "Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation"}, {"paperId": "2ccbafd636184c6edd83e585359514923b6cf849", "title": "The Unreasonable Ineffectiveness of the Deeper Layers"}, {"paperId": "f608af95af3fbfa5dcce4dfb0a905125d0ec0c64", "title": "A comparison of human and GPT-4 use of probabilistic phrases in a coordination game"}, {"paperId": "2da19b856e6d3270f9dc4483ca2f389b7761b1f7", "title": "The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?"}, {"paperId": "9ee4ab6ac5f4e239f0617db49e4b5fb8fa2b5a97", "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models"}, {"paperId": "7e388155314475226bd001b84d8a9a69f35021a7", "title": "Language Models Represent Beliefs of Self and Others"}, {"paperId": "0044140fdd4547a380b0b82052ae0b6ffd95216c", "title": "Eight Methods to Evaluate Robust Unlearning in LLMs"}, {"paperId": "99a1060a6aa6cad15b37ac6566b20ba992b16b84", "title": "Robust agents learn causal world models"}, {"paperId": "618f0284465a8f2b4e979f0213227b7c51816565", "title": "Opening the AI black box: program synthesis via mechanistic interpretability"}, {"paperId": "13df472c3fe81bf1b615238fbd7884c8b45d8d1c", "title": "Large Language Models for Time Series: A Survey"}, {"paperId": "8e1c0a78a26a59cbe4ac82f13d9d01017a9bcb73", "title": "Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models"}, {"paperId": "d349f4e924f6cdcbafce75f159437920495cd8a1", "title": "Black-Box Access is Insufficient for Rigorous AI Audits"}, {"paperId": "436cd0fe00807bc9d14434f3313dc836530f2dae", "title": "Universal Neurons in GPT2 Language Models"}, {"paperId": "b8ca67cb22dd0fcd9c03ecfcd0c9e1f1695b886b", "title": "Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach"}, {"paperId": "592ac35991e583fc37c26ee6659d2deb85142ad9", "title": "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives"}, {"paperId": "e49a005eed92bbb8b797fb1bdc056e431f60f337", "title": "Vectorizing string entries for data processing on tables: when are larger language models better?"}, {"paperId": "e973d74a628f1c08e366b4373dfe445090184bec", "title": "Emergence and Function of Abstract Representations in Self-Supervised Transformers"}, {"paperId": "113e82ab340af5b9721b8ed50b022e3bb769170e", "title": "Large Language Models for Travel Behavior Prediction"}, {"paperId": "498decc50ccea9293f63a98c30d7c3439be074b7", "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning"}, {"paperId": "9a149300e4291c3503651b8f031533240d4e9ec4", "title": "Sparsify-then-Classify: From Internal Neurons of Large Language Models To Efficient Text Classifiers"}, {"paperId": "eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4", "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models"}, {"paperId": "d1b5151231a790c7a60f620e21860593dae9a1c5", "title": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"}, {"paperId": "9ea9406b0f78d67ee9a795847dee9c253e0b574a", "title": "Use of probabilistic phrases in a coordination game: human versus GPT-4"}, {"paperId": "a9afa75c120abed2eb74dc869c9e5189174312a9", "title": "On Generative Agents in Recommendation"}, {"paperId": "26089bdfdbca1e6eaaceca71e3116b715bec6d47", "title": "Explainability for Large Language Models: A Survey"}, {"paperId": "14532d3a54c5de7ad97e67ce7c92138c474d61bd", "title": "Relation-Oriented: Toward Causal Knowledge-Aligned AGI"}]}
