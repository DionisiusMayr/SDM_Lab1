{"paperId": "0d6c6e8d01945b9664c8cd9ac704888149dec5e2", "publicationVenue": {"id": "e4f51561-5050-4b9c-87c2-c49957677fbf", "name": "European Conference on Computer Systems", "type": "conference", "alternate_names": ["Eur Conf Comput Syst", "EuroSys"], "url": "http://www.eurosys.org/"}, "title": "DGCL: an efficient communication library for distributed GNN training", "abstract": "Graph neural networks (GNNs) have gained increasing popularity in many areas such as e-commerce, social networks and bio-informatics. Distributed GNN training is essential for handling large graphs and reducing the execution time. However, for distributed GNN training, a peer-to-peer communication strategy suffers from high communication overheads. Also, different GPUs require different remote vertex embeddings, which leads to an irregular communication pattern and renders existing communication planning solutions unsuitable. We propose the distributed graph communication library (DGCL) for efficient GNN training on multiple GPUs. At the heart of DGCL is a communication planning algorithm tailored for GNN training, which jointly considers fully utilizing fast links, fusing communication, avoiding contention and balancing loads on different links. DGCL can be easily adopted to extend existing single-GPU GNN systems to distributed training. We conducted extensive experiments on different datasets and network configurations to compare DGCL with alternative communication schemes. In our experiments, DGCL reduces the communication time of the peer-to-peer communication by 77.5% on average and the training time for an epoch by up to 47%.", "venue": "European Conference on Computer Systems", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2021-04-21", "journal": {"name": "Proceedings of the Sixteenth European Conference on Computer Systems"}, "authors": [{"authorId": "35349851", "name": "Zhenkun Cai"}, {"authorId": "145837716", "name": "Xiao Yan"}, {"authorId": "47096554", "name": "Yidi Wu"}, {"authorId": "1381894756", "name": "Kaihao Ma"}, {"authorId": "1717691", "name": "James Cheng"}, {"authorId": "2087106044", "name": "Fan Yu"}], "citations": [{"paperId": "b7cdce5df8ced4861984dcee1366ef4e79666df0", "title": "CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks"}, {"paperId": "c3364b1fb446763cf56d5532f2ded29076e6d01f", "title": "Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling"}, {"paperId": "48c35e8cfa7ac7e24c448fb21154151097e4f184", "title": "Celeritas: Out-of-Core Based Unsupervised Graph Neural Network via Cross-Layer Computing 2024"}, {"paperId": "ce84ace3bd4c675b5c9a2d7775558a349113d2f4", "title": "POSTER: ParGNN: Efficient Training for Large-Scale Graph Neural Network on GPU Clusters"}, {"paperId": "85691014466d48647a01d8726c68d0e9258fc667", "title": "Single-GPU GNN Systems: Traps and Pitfalls"}, {"paperId": "087a11fd23bbbd9e1b90b493be7af6245837f391", "title": "PARAG: PIM Architecture for Real-Time Acceleration of GCNs"}, {"paperId": "6e419d87084f8cc153f79f6c399e48092ba237b0", "title": "HongTu: Scalable Full-Graph GNN Training on Multiple GPUs"}, {"paperId": "f0928b606aff3f02bc0724a38678d5cc54f2ceb4", "title": "NeutronOrch: Rethinking Sample-based GNN Training under CPU-GPU Heterogeneous Environments"}, {"paperId": "d6a9f354e25e8d8161fc58411aee031674176e64", "title": "DTFSeg: A Dynamic Threshold Filtering Method for Semi-Supervised Semantic Segmentation"}, {"paperId": "02857e8813d7375a377f5f1ac13e9abf9e9e2385", "title": "Adaptive Load Balanced Scheduling and Operator Overlap Pipeline for Accelerating the Dynamic GNN Training"}, {"paperId": "2a72d9a7f8a3c4740fc75b8e62a6648189fb8f19", "title": "Barad-dur: Near-Storage Accelerator for Training Large Graph Neural Networks"}, {"paperId": "d43d6e4bd9dca7ebfd0978ebfa29b40d1c4b08c0", "title": "DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks"}, {"paperId": "f0f84ea6f9d0c417614d205827fecbc238f82a58", "title": "TurboGNN: Improving the End-to-End Performance for Sampling-Based GNN Training on GPUs"}, {"paperId": "75f977e49f59a1758a65e6863c1b32a511741871", "title": "Accelerating Distributed GNN Training by Codes"}, {"paperId": "38ae2071e0bbaa16d61ea007d438cc24a8a11160", "title": "SPEED: Streaming Partition and Parallel Acceleration for Temporal Interaction Graph Embedding"}, {"paperId": "6988501587a310dca1fe0a6b5d3e188b26ff8f12", "title": "Redundancy-Free High-Performance Dynamic GNN Training with Hierarchical Pipeline Parallelism"}, {"paperId": "3f57f297eb80171f9c2a900d087cfcac943c4c1e", "title": "DGI: An Easy and Efficient Framework for GNN Model Evaluation"}, {"paperId": "692955986d8aa4e5b2ccc82e6a4a100ab4cf932a", "title": "DistTGL: Distributed Memory-Based Temporal Graph Neural Network Training"}, {"paperId": "5be16c21d4b39c6f07d6e77b837113938386e3f4", "title": "AdaGL: Adaptive Learning for Agile Distributed Training of Gigantic GNNs"}, {"paperId": "54a863e3155637681e9330c04c7c1928c2d7455e", "title": "Serving Graph Neural Networks With Distributed Fog Servers for Smart IoT Services"}, {"paperId": "3229291b9b7e9a7200b543089e05a4442cbe6a9b", "title": "Scalable and Efficient Full-Graph GNN Training for Large Graphs"}, {"paperId": "5aac333b25158c4f330474608c93f461f7ae6689", "title": "Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training"}, {"paperId": "8bf5d3643a9d2b637d6f57ac5512e9ca54a51dbf", "title": "TurboMGNN: Improving Concurrent GNN Training Tasks on GPU With Fine-Grained Kernel Fusion"}, {"paperId": "258941db02068ab72d40cb7ef3118a5e3f95e9f4", "title": "Attribute-driven streaming edge partitioning with reconciliations for distributed graph neural network training"}, {"paperId": "2b8c884e3e7a60bc058775445e8e24d25ef91d4a", "title": "Communication-Efficient Graph Neural Networks with Probabilistic Neighborhood Expansion Analysis and Caching"}, {"paperId": "85cec0991826ffb4f44fcd11d56c4c2b1356e64c", "title": "Communication Optimization for Distributed Execution of Graph Neural Networks"}, {"paperId": "81d9cd7fd934f218d3428fc70fc5b8940c0a1107", "title": "GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism"}, {"paperId": "9246e567d5fa10a3b1ad1deb3433f62e42c3e12f", "title": "HitGNN: High-Throughput GNN Training Framework on CPU+Multi-FPGA Heterogeneous Platform"}, {"paperId": "c5aff3f2ab4e77f34c81e33f9eb926beb5404850", "title": "HyScale-GNN: A Scalable Hybrid GNN Training System on Single-Node Heterogeneous Architecture"}, {"paperId": "656a69cc79dfe67dcee2441e19ec44b43df938b4", "title": "Scalable Neural Network Training over Distributed Graphs"}, {"paperId": "46210e170045df3c0c50a17bb63e6de480d62f9d", "title": "FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training"}, {"paperId": "5205f96c6894b1b1fed6683d0c67f4b1f3b25b45", "title": "Scalable Graph Convolutional Network Training on Distributed-Memory Systems"}, {"paperId": "823a52b2b770d4e198d4693b27e4cebb4822e5ad", "title": "Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry"}, {"paperId": "141c3de5103c8cb11b3119470f1aad438445399a", "title": "A Comprehensive Survey on Distributed Training of Graph Neural Networks"}, {"paperId": "1e79e33c77b2d8eaf643af0e1f5003057d7356b2", "title": "Distributed Graph Neural Network Training: A Survey"}, {"paperId": "4d468c7b4345fcd25d40ca67313a743e6e164444", "title": "HGL: Accelerating Heterogeneous GNN Training with Holistic Representation and Optimization"}, {"paperId": "1b2576c824a8c71e559eea72486092575e5dbbde", "title": "GNN at the Edge: Cost-Efficient Graph Neural Network Processing Over Distributed Edge Servers"}, {"paperId": "17f3d9b84c21cfc313d5c11f9efba37605cade94", "title": "Software Systems Implementation and Domain-Specific Architectures towards Graph Analytics"}, {"paperId": "00304f852a35dc32fd1a0e6b6c6226c7153e3ca9", "title": "MGG: Accelerating Graph Neural Networks with Fine-Grained Intra-Kernel Communication-Computation Pipelining on Multi-GPU Platforms"}, {"paperId": "0a942d68964ba104cfd49b675c976c1a1dbb65d1", "title": "MG-GCN: A Scalable multi-GPU GCN Training Framework"}, {"paperId": "5e302753eee290fed68e9f22a8764879639ada1b", "title": "Graphite: optimizing graph neural networks on CPUs through cooperative software-hardware techniques"}, {"paperId": "e08e552c62b40c7ee2ec660b365d11956ef1d40d", "title": "NeutronStar: Distributed GNN Training with Hybrid Dependency Management"}, {"paperId": "8018a561d2ed821cfb4be7ce04993cabf9932c2f", "title": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis"}, {"paperId": "21913eb287f8fc33db8f6274fd2a07072c4e11eb", "title": "Trustworthy Graph Neural Networks: Aspects, Methods and Trends"}, {"paperId": "39271db2458f6fb635e40372049cb1e203c7e05c", "title": "SANCUS: Staleness-Aware Communication-Avoiding Full-Graph Decentralized Training in Large-Scale Graph Neural Networks"}, {"paperId": "e4b81c6d782d4da6f4168298f4861f410a7a4f9f", "title": "Fograph: Enabling Real-Time Deep Graph Inference with Fog Computing"}, {"paperId": "e7c462bf521add5615d5716069508648b283afe8", "title": "Optimizing Task Placement and Online Scheduling for Distributed GNN Training Acceleration"}, {"paperId": "a2adce2cb9e906ae2d90ee9686203c57419686c5", "title": "BRGraph: An efficient graph neural network training system by reusing batch data on GPU"}, {"paperId": "740b9d4414a955a74c16f5f3617d2b5dde2e9adf", "title": "GNNLab: a factored system for sample-based GNN training over GPUs"}, {"paperId": "1df8b8cc125f667c6495b76e347da621109b0f73", "title": "ByteGNN: Efficient Graph Neural Network Training at Large Scale"}, {"paperId": "5e3105370d6c83dc9d97fb860d5be25b6a3df7a1", "title": "ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems"}, {"paperId": "5df9e659ee931c4aed4ea5ce6ba514fbe0a51e3d", "title": "Elastic Deep Learning in Multi-Tenant GPU Clusters"}, {"paperId": "94f0823f8db5360972a7a68b453e28ddf9c4e992", "title": "BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing"}, {"paperId": "6cdc6eebdec0e5c1ba47db4a0d0c4381cc82d0ed", "title": "FedGraph: Federated Graph Learning With Intelligent Sampling"}, {"paperId": "0ee96b161c5ce7e377ba93e7f74a7182ecb2a6ac", "title": "GNNear: Accelerating Full-Batch Training of Graph Neural Networks with near-Memory Processing"}, {"paperId": "0aa0e47b15b59e5508bea390b196f8cd3758b6dc", "title": "Vertex-Centric Visual Programming for Graph Neural Networks"}, {"paperId": "3308eeac303fd042f8d7659d9c43faa35173c39b", "title": "Seastar: vertex-centric programming for graph neural networks"}, {"paperId": "1c63814a9ca53984330e23050b73c9abd0ebdc30", "title": "GNNPipe: Accelerating Distributed Full-Graph GNN Training with Pipelined Model Parallelism"}, {"paperId": "40f2b55a2fc836bb2b05d1f6f86e0300ec213e12", "title": "Graphiler: Optimizing Graph Neural Networks with Message Passing Data Flow Graph"}, {"paperId": "e0c94287ec33ca5caf94149c84ae58c6dc8eb76d", "title": "Empowering GNNs with Fine-grained Communication-Computation Pipelining on Multi-GPU Platforms"}, {"paperId": "cfa781219d01e95b86a7e331c60ede83bbcb8d29", "title": "Dominating Vertical Collaborative Learning Systems"}, {"paperId": "fe3d6c9a7e76a175ca90f7ce05beb9965f148f26", "title": "the Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation."}, {"paperId": "65582abd2ea66caee431bcfe29b99cb811b40a9c", "title": "GCNear: A Hybrid Architecture for Efficient GCN Training with Near-Memory Processing"}, {"paperId": "d70ee0f105fc1691db761fc9c4b29b30f995cbb4", "title": "Two-Face: Combining Collective and One-Sided Communication for Efficient Distributed SpMM"}, {"paperId": "82a38e0726594a302486c7c03e8bbb932ebbb955", "title": "Expediting Distributed GNN Training with Feature-only Partition and Optimized Communication Planning"}, {"paperId": "febbe50d95e27e99c46a0d431dffd371203e57a4", "title": "This paper is included in the Proceedings of the 2023 USENIX Annual Technical Conference."}]}
