{"paperId": "aac83deb068875d2dffb58b2c8fdeffc6ad2a38a", "publicationVenue": {"id": "91d4ecad-d022-4953-901e-c5c57c614f72", "name": "Asia-Pacific Computer Systems Architecture Conference", "type": "conference", "alternate_names": ["ACSAC", "Asia-pacific Comput Syst Archit Conf", "Annu Comput Secur Appl Conf", "Annual Computer Security Applications Conference"], "url": "http://www.wikicfp.com/cfp/program?id=46", "alternate_urls": ["http://www.acsac.org/"]}, "title": "A First Look at Toxicity Injection Attacks on Open-domain Chatbots", "abstract": "Chatbot systems have improved significantly because of the advances made in language modeling. These machine learning systems follow an end-to-end data-driven learning paradigm and are trained on large conversational datasets. Imperfections or harmful biases in the training datasets can cause the models to learn toxic behavior, and thereby expose their users to harmful responses. Prior work has focused on measuring the inherent toxicity of such chatbots, by devising queries that are more likely to produce toxic responses. In this work, we ask the question: How easy or hard is it to inject toxicity into a chatbot after deployment? We study this in a practical scenario known as Dialog-based Learning (DBL), where a chatbot is periodically trained on recent conversations with its users after deployment. A DBL setting can be exploited to poison the training dataset for each training cycle. Our attacks would allow an adversary to manipulate the degree of toxicity in a model and also enable control over what type of queries can trigger a toxic response. Our fully automated attacks only require LLM-based software agents masquerading as (malicious) users to inject high levels of toxicity. We systematically explore the vulnerability of popular chatbot pipelines to this threat. Lastly, we show that several existing toxicity mitigation strategies (designed for chatbots) can be significantly weakened by adaptive attackers.", "venue": "Asia-Pacific Computer Systems Architecture Conference", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2023-12-04", "journal": {"name": "Proceedings of the 39th Annual Computer Security Applications Conference"}, "authors": [{"authorId": "2263670007", "name": "Connor Weeks"}, {"authorId": "2359736", "name": "Aravind Cheruvu"}, {"authorId": "2184572952", "name": "Sifat Muhammad Abdullah"}, {"authorId": "2069475624", "name": "Shravya Kanchi"}, {"authorId": "2264093571", "name": "Daphne Yao"}, {"authorId": "2264455945", "name": "Bimal Viswanath"}], "citations": [{"paperId": "b0ba922e612b43aae4b011c4f754592b70394be3", "title": "Thai-language chatbot security: Detecting instruction attacks with XLM-RoBERTa and Bi-GRU"}]}
