{"paperId": "0d2b32f8aec73f09a2d5d1e2f77b2e941ebf7002", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Conditionally Risk-Averse Contextual Bandits", "abstract": "Contextual bandits with average-case statistical guarantees are inadequate in risk-averse situations because they might trade off degraded worst-case behaviour for better average performance. Designing a risk-averse contextual bandit is challenging because exploration is necessary but risk-aversion is sensitive to the entire distribution of rewards; nonetheless we exhibit the first risk-averse contextual bandit algorithm with an online regret guarantee. We conduct experiments from diverse scenarios where worst-case outcomes should be avoided, from dynamic pricing, inventory management, and self-tuning software; including a production exascale data processing system.", "venue": "arXiv.org", "year": 2022, "fieldsOfStudy": ["Computer Science", "Mathematics"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-10-24", "journal": {"name": "ArXiv", "volume": "abs/2210.13573"}, "authors": [{"authorId": "2051329471", "name": "M'onika Farsang"}, {"authorId": "3040175", "name": "Paul Mineiro"}, {"authorId": "2516526", "name": "Wangda Zhang"}], "citations": [{"paperId": "1736e843974186299242f44a17845dfcf1bb6547", "title": "More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning"}, {"paperId": "8d7154a1714f4a076ff211b9a4e0b95429e7a9c1", "title": "The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning"}]}
