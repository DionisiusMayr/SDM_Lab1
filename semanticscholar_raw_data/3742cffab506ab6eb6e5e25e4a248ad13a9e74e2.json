{"paperId": "3742cffab506ab6eb6e5e25e4a248ad13a9e74e2", "publicationVenue": {"id": "7c9d091e-015e-4e5d-a11f-9bc369fcf414", "name": "IEEE Transactions on Parallel and Distributed Systems", "type": "journal", "alternate_names": ["IEEE Trans Parallel Distrib Syst"], "issn": "1045-9219", "url": "http://www.computer.org/tpds", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=71"]}, "title": "Accelerating Gossip-Based Deep Learning in Heterogeneous Edge Computing Platforms", "abstract": "With the exponential growth of data created at the network edge, decentralized and Gossip-based training of deep learning (DL) models on edge computing (EC) gains tremendous research momentum, owing to its capability to learn from resource-strenuous edge nodes with limited network connectivity. Today\u2019s edge devices are extremely heterogeneous, e.g., hardware and software stacks, and result in high performance variation of training time and inducing extra delay to synchronize and converge. The large body of prior art accelerates DL, being data or model parallelization, via a centralized server, e.g., parameter server scheme, which may easily turn into the system bottleneck or single point of failure. In this artice, we propose EdgeGossip, a framework specifically designed to accelerate the training process of decentralized and Gossip-based DL training for heterogeneous EC platforms. EdgeGossip features on: (i) low performance variation among multiple EC platforms during iterative training, and (ii) accuracy-aware training to fastly obtain best possible model accuracy. We implement EdgeGossip based on popular Gossip algorithms and demonstrate its effectiveness using real-world DL workloads, i.e., considerably reducing model training time by an average of 2.70 times while only incurring accuracy losses of 0.78 percent.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-01", "journal": {"name": "IEEE Transactions on Parallel and Distributed Systems", "pages": "1591-1602", "volume": "32"}, "authors": [{"authorId": "2066468959", "name": "Rui Han"}, {"authorId": "2051533827", "name": "Shilin Li"}, {"authorId": "2051536715", "name": "Xiangwei Wang"}, {"authorId": "144212455", "name": "C. Liu"}, {"authorId": "2051541466", "name": "Gaofeng Xin"}, {"authorId": "14672072", "name": "L. Chen"}], "citations": [{"paperId": "8114de409338a60ad030fa65f7720b7be36513b7", "title": "Integration of Edge\u2013AI Into IoT\u2013Cloud Architecture for Landslide Monitoring and Prediction"}, {"paperId": "6198e238e425b1f7f2317ae2bebc41fc37aa386f", "title": "Accelerating Heterogeneous Tensor Parallelism via Flexible Workload Control"}, {"paperId": "a31fad4ebe1957b4fea92b6fbd0bed5df98b7df6", "title": "Optimizing Aggregation Frequency for Hierarchical Model Training in Heterogeneous Edge Computing"}, {"paperId": "c386df36f579bd08972dfeb3b31c14a327770090", "title": "EdgeMove: Pipelining Device-Edge Model Training for Mobile Intelligence"}, {"paperId": "3a6a28bbfa3f9f6f0daed54fed5d40eec8643a16", "title": "FedEdge: Accelerating Edge-Assisted Federated Learning"}, {"paperId": "e14403acf5e5602bf8d566336d909b31cb717b04", "title": "GossipFL: A Decentralized Federated Learning Framework With Sparsified and Adaptive Communication"}, {"paperId": "a97fbcc18f93e1575a238667a1205a857a28a287", "title": "TFormer: A Transmission-Friendly ViT Model for IoT Devices"}, {"paperId": "34e14fdf011987ecf8abac4dc18c776e1554c51c", "title": "FSP: Towards Flexible Synchronous Parallel Frameworks for Distributed Machine Learning"}, {"paperId": "7901621f428c9ff9a4ca74be11a7fe678b73d4ff", "title": "Local Scheduling in KubeEdge-Based Edge Computing Environment"}, {"paperId": "e95cb3ba92a042b33632761623c29ced692eb28a", "title": "Hierarchical Memory Pool Based Edge Semi-Supervised Continual Learning Method"}, {"paperId": "41323a075ca8171455b664078854d0a70670ab15", "title": "An asynchronous distributed training algorithm based on Gossip communication and Stochastic Gradient Descent"}, {"paperId": "d5b753707e7dbc9dc78fc684761b73ca45e80b62", "title": "PEPPER"}, {"paperId": "a5e537fe6d9d9a15dc7e1e9d723de10a66f1cff8", "title": "Pyramid: Enabling Hierarchical Neural Networks with Edge Computing"}, {"paperId": "c9de8b052562e03ebe8445a14d9cb3cb08e85fd6", "title": "Enabling All In-Edge Deep Learning: A Literature Review"}, {"paperId": "3e3069c01bed925d371f5a746a4089f9765826e7", "title": "An Asynchronous Distributed Training Algorithm Based on Gossip"}, {"paperId": "09628fa44555ac4ef447bcd19e90d8b427747198", "title": "From Deterioration to Acceleration: A Calibration Approach to Rehabilitating Step Asynchronism in Federated Optimization"}, {"paperId": "682bac2ae266709259843a06443d03363f189cc2", "title": "BrainyEdge: An AI-enabled framework for IoT edge computing"}, {"paperId": "9b8aabefe970826aa3c9e5ea21586ebb45748c0e", "title": "Enabling Deep Learning for All-in EDGE paradigm"}, {"paperId": "c325ce80cbcd62c3580e22788188323f8a7feca2", "title": "Lightweight and Accurate DNN-based Anomaly Detection at Edge"}]}
