{"paperId": "cf3961d42d5de56cc00623315fd5b05ed0311a57", "publicationVenue": {"id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e", "name": "Proceedings of the VLDB Endowment", "type": "journal", "alternate_names": ["Proceedings of The Vldb Endowment", "Proc VLDB Endow", "Proc Vldb Endow"], "issn": "2150-8097", "url": "http://dl.acm.org/toc.cfm?id=J1174", "alternate_urls": ["http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"]}, "title": "dmapply: A functional primitive to express distributed machine learning algorithms in R", "abstract": "Due to R's popularity as a data-mining tool, many distributed systems expose an R-based API to users who need to build a distributed application in R. As a result, data scientists have to learn to use different interfaces such as RHadoop, SparkR, Revolution R's ScaleR, and HPE's Distributed R. Unfortunately, these interfaces are custom, non-standard, and difficult to learn. Not surprisingly, R applications written in one framework do not work in another, and each backend infrastructure has spent redundant effort in implementing distributed machine learning algorithms. \n \nWorking with the members of R-core, we have created ddR (Distributed Data structures in R), a unified system that works across different distributed frameworks. In ddR, we introduce a novel programming primitive called dmapply that executes functions on distributed data structures. The dmapply primitive encapsulates different computation patterns: from function and data broadcast to pair-wise communication. We show that dmapply is powerful enough to express algorithms that fit the statistical query model, which includes many popular machine learning algorithms, as well as applications written in MapReduce. We have integrated ddR with many backends, such as R's single-node parallel framework, multi-node SNOW framework, Spark, and HPE Distributed R, with few or no modifications to any of these systems. We have also implemented multiple machine learning algorithms which are not only portable across different distributed systems, but also have performance comparable to the \"native\" implementations on the backends. We believe that ddR will standardize distributed computing in R, just like the SQL interface has standardized how relational data is manipulated.", "venue": "Proceedings of the VLDB Endowment", "year": 2016, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-09-01", "journal": {"name": "Proc. VLDB Endow.", "pages": "1293-1304", "volume": "9"}, "authors": [{"authorId": "66030128", "name": "Edward Ma"}, {"authorId": "20278612", "name": "Vishrut Gupta"}, {"authorId": "49975266", "name": "M. Hsu"}, {"authorId": "48315986", "name": "Indrajit Roy"}], "citations": [{"paperId": "e91a9d1c7707f81b3d1d1af171875ce7c0538d83", "title": "DistStat.jl: Towards Unified Programming for High-Performance Statistical Computing Environments in Julia"}, {"paperId": "4ab38afa44f1da43746cd4a01344c8ec212b9de3", "title": "Data Management in Machine Learning Systems"}, {"paperId": "1bc90350074050137088f8d57590d94bb63ccaa9", "title": "Efficient Data-Parallel Cumulative Aggregates for Large-Scale Machine Learning"}]}
