{"paperId": "5ca0cadf12cef2908c636d64da225e168273ca90", "publicationVenue": null, "title": "Exploiting Network Loss for Distributed Approximate Computing with NetApprox", "abstract": "Many data center applications such as machine learning and big data analytics can complete their analysis without processing the complete set of data. While extensive approximate-aware optimizations have been proposed at hardware, programming language, and application levels; however, to date, the approximate computing optimizations have ignored the network layer. We propose NetApprox, which to the best of our knowledge, is the \ufb01rst approximate-aware network layer comprising transport-layer protocol, network resource allocation schemes, and scheduling/priority-assignment policies. Building on the observation that approximate applications can tolerate loss, NetApprox\u2019s main insights are to aggressively send approximate tra\ufb03c (which improves the performance of approximate applications) and to minimize the network resources allocated to approximate tra\ufb03c (which simultaneously limits the impact of aggressive approximate tra\ufb03c while freeing up resources that, in turn, improve non-approximate applications\u2019 performance). We ported Flink, Kafka, Spark, and PyTorch to NetApprox and evaluated NetApprox with both large-scale simulation and real implementation. Our evaluation results show that NetApprox improves job completion times by up to 80% compared to network-oblivious approximation solutions, and improves the performance of co-running non-approximate workloads by 79%.", "venue": "", "year": 2019, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2019-01-07", "journal": null, "authors": [{"authorId": "144064348", "name": "Ke Liu"}, {"authorId": "2173718976", "name": "Jinmou Li"}, {"authorId": "2984487", "name": "Shin-Yeh Tsai"}, {"authorId": "144992984", "name": "Theophilus A. Benson"}, {"authorId": "2108361757", "name": "Yiying Zhang"}], "citations": []}
