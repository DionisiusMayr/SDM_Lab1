{"paperId": "fd206a301c7cd73cb0c829385c37acff005d9ae8", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Efficient Human-AI Coordination via Preparatory Language-based Convention", "abstract": "Developing intelligent agents capable of seamless coordination with humans is a critical step towards achieving artificial general intelligence. Existing methods for human-AI coordination typically train an agent to coordinate with a diverse set of policies or with human models fitted from real human data. However, the massively diverse styles of human behavior present obstacles for AI systems with constrained capacity, while high quality human data may not be readily available in real-world scenarios. In this study, we observe that prior to coordination, humans engage in communication to establish conventions that specify individual roles and actions, making their coordination proceed in an orderly manner. Building upon this observation, we propose employing the large language model (LLM) to develop an action plan (or equivalently, a convention) that effectively guides both human and AI. By inputting task requirements, human preferences, the number of agents, and other pertinent information into the LLM, it can generate a comprehensive convention that facilitates a clear understanding of tasks and responsibilities for all parties involved. Furthermore, we demonstrate that decomposing the convention formulation problem into sub-problems with multiple new sessions being sequentially employed and human feedback, will yield a more efficient coordination convention. Experimental evaluations conducted in the Overcooked-AI environment, utilizing a human proxy model, highlight the superior performance of our proposed method compared to existing learning-based approaches. When coordinating with real humans, our method achieves better alignment with human preferences and an average performance improvement of 15% compared to the state-of-the-art.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-01", "journal": {"name": "ArXiv", "volume": "abs/2311.00416"}, "authors": [{"authorId": "2174870366", "name": "Cong Guan"}, {"authorId": "2264498032", "name": "Lichao Zhang"}, {"authorId": "2264004041", "name": "Chunpeng Fan"}, {"authorId": "2257096601", "name": "Yi-Chen Li"}, {"authorId": "2262324272", "name": "Feng Chen"}, {"authorId": "2216719801", "name": "Lihe Li"}, {"authorId": "2264087977", "name": "Yunjia Tian"}, {"authorId": "49785134", "name": "Lei Yuan"}, {"authorId": "49785134", "name": "Lei Yuan"}, {"authorId": "2264002365", "name": "Yang Yu"}], "citations": [{"paperId": "0bc72e0c31df7bd821b8becd8f43c77c27af5155", "title": "Large Language Models and Video Games: A Preliminary Scoping Review"}]}
