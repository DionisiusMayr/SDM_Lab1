{"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "A Survey on Knowledge Distillation of Large Language Models", "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary LLMs, such as GPT-4, to their open-source counterparts like LLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a crucial role in both compressing these models, and facilitating their self-improvement by employing themselves as teachers. This paper presents a comprehensive survey of KD's role within the realm of LLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self-improvement. Our survey is meticulously structured around three foundational pillars: \\textit{algorithm}, \\textit{skill}, and \\textit{verticalization} -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in KD and proposing future research directions. Importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2024-02-20", "journal": {"name": "ArXiv", "volume": "abs/2402.13116"}, "authors": [{"authorId": "2279658967", "name": "Xiaohan Xu"}, {"authorId": "2150655891", "name": "Ming Li"}, {"authorId": "8801869", "name": "Chongyang Tao"}, {"authorId": "2279548827", "name": "Tao Shen"}, {"authorId": "2284763029", "name": "Reynold Cheng"}, {"authorId": "2154860526", "name": "Jinyang Li"}, {"authorId": "2284826718", "name": "Can Xu"}, {"authorId": "2284762675", "name": "Dacheng Tao"}, {"authorId": "2258913288", "name": "Tianyi Zhou"}], "citations": [{"paperId": "d06e65f74715e071678bf8ccdcf9d52004a10280", "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences"}, {"paperId": "830dc0848a5fd6f1c35ca6f67a79addf3a8fd0fa", "title": "Towards a theory of model distillation"}, {"paperId": "da20d127744d97df4f9e0d63cbbda74bcb12233b", "title": "Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation"}, {"paperId": "dbf829c977c121c3704d070d7800d29fe5914756", "title": "LLM Inference Unveiled: Survey and Roofline Model Insights"}]}
