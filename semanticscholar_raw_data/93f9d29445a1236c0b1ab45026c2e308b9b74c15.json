{"paperId": "93f9d29445a1236c0b1ab45026c2e308b9b74c15", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Understanding Scaling Laws for Recommendation Models", "abstract": "Scale has been a major driving force in improving machine learning performance, and understanding scaling laws is essential for strategic planning for a sustainable model quality performance growth, long-term resource planning and developing efficient system infrastructures to support large-scale models. In this paper, we study empirical scaling laws for DLRM style recommendation models, in particular Click-Through Rate (CTR). We observe that model quality scales with power law plus constant in model size, data size and amount of compute used for training. We characterize scaling efficiency along three different resource dimensions, namely data, parameters and compute by comparing the different scaling schemes along these axes. We show that parameter scaling is out of steam for the model architecture under study, and until a higher-performing model architecture emerges, data scaling is the path forward. The key research questions addressed by this study include: Does a recommendation model scale sustainably as predicted by the scaling laws? Or are we far off from the scaling law predictions? What are the limits of scaling? What are the implications of the scaling laws on long-term hardware/system development?", "venue": "arXiv.org", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-08-17", "journal": {"name": "ArXiv", "volume": "abs/2208.08489"}, "authors": [{"authorId": "2774880", "name": "Newsha Ardalani"}, {"authorId": "2797270", "name": "Carole-Jean Wu"}, {"authorId": "2111435093", "name": "Zeliang Chen"}, {"authorId": "71887641", "name": "Bhargav Bhushanam"}, {"authorId": "2181861797", "name": "Adnan Aziz"}], "citations": [{"paperId": "38cd208cdbc3631fcac4148bd4a421dd87c8d110", "title": "Scaling Laws For Dense Retrieval"}, {"paperId": "13965d8d68217308ff8c7e738ced637739c6a1b8", "title": "Bridging Language and Items for Retrieval and Recommendation"}, {"paperId": "74dbc069841d8fa55cd2721e53b9f4532899c146", "title": "Ad Recommendation in a Collapsed and Entangled World"}, {"paperId": "a41ba3b5fec34ac5c21b4c411cd596f76ad7bf00", "title": "Proxy-based Item Representation for Attribute and Context-aware Recommendation"}, {"paperId": "cc2099784e9b36cec19a1f43d8531d098288f4bf", "title": "Increased Compute Efficiency and the Diffusion of AI Capabilities"}, {"paperId": "32f65dd751e5c491d83c2f46e41f50afa1c6d2bb", "title": "Scaling Law of Large Sequential Recommendation Models"}, {"paperId": "3493104b05ea7721d862b3dc99b97f8678b80596", "title": "Ad-Rec: Advanced Feature Interactions to Address Covariate-Shifts in Recommendation Networks"}, {"paperId": "9e4980cb927b803d375c5796f4a2eb3a7fe0555d", "title": "The Quantization Model of Neural Scaling"}, {"paperId": "f087453fa1f2afddb4235f782281492d6c8462b3", "title": "Improving Training Stability for Multitask Ranking Models in Recommender Systems"}, {"paperId": "af34181ae916e01c72513f915f984a1d2c7febab", "title": "Augmented Behavioral Annotation Tools, with Application to Multimodal Datasets and Models: A Systematic Review"}, {"paperId": "7557105c9aa6a26db4f8e73fabb25e8134013fb5", "title": "Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning"}, {"paperId": "3437212c42cc17e1bd0904601c79e3240cc44331", "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure"}, {"paperId": "557a5147d88ad361b807d4c93decac6d57d2d5e9", "title": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans"}, {"paperId": "c5cf54b2b6abf658697d272c1377812fd9e24e11", "title": "Scaling Generative Pre-training for User Ad Activity Sequences"}]}
