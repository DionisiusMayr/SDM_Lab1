{"paperId": "ce573c7fa7f01d16c873f494e5c1140518b9b4f2", "publicationVenue": {"id": "7fca7065-fa6f-490b-923e-3ffc07857f1c", "name": "IEEE Transactions on Cloud Computing", "type": "journal", "alternate_names": ["IEEE Trans Cloud Comput"], "issn": "2168-7161", "url": "https://www.computer.org/web/tcc", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=6245519"]}, "title": "CLUE: Systems Support for Knowledge Transfer in Collaborative Learning With Neural Nets", "abstract": "For highly distributed environments such as edge computing, collaborative learning approaches eschew the dependence on a global, shared model, in favor of models tailored for each location. Creating tailored models for individual learning contexts reduces the amount of data transfer, while collaboration among peers provides acceptable model performance. Collaboration assumes, however, the availability of knowledge transfer mechanisms, which are not trivial for deep learning models where knowledge isn't easily attributed to precise model slices. We present CLUE \u2013 a framework that facilitates knowledge transfer for neural networks. CLUE provides new system support for dynamically extracting significant parameters from a helper node's neural network, and uses this with a multi-model boosting-based approach to improve the predictive performance of the target node. The evaluation of CLUE with different PyTorch and TensorFlow neural network models demonstrates that its knowledge transfer mechanism improves by up to <inline-formula><tex-math notation=\"LaTeX\">$3.5\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"daga-ieq1-3294490.gif\"/></alternatives></inline-formula> how quickly a model adapts to changes, compared to learning in isolation, while affording up to several magnitudes reduction in data movement costs compared to federated learning.", "venue": "IEEE Transactions on Cloud Computing", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-01", "journal": {"name": "IEEE Transactions on Cloud Computing", "pages": "3541-3554", "volume": "11"}, "authors": [{"authorId": "1392944818", "name": "Harshit Daga"}, {"authorId": "2221821365", "name": "Yiwen Chen"}, {"authorId": "2075308569", "name": "Aastha Agrawal"}, {"authorId": "1691551", "name": "Ada Gavrilovska"}], "citations": [{"paperId": "796f31382a05dd40c715eb81acaaf45ba25571e2", "title": "Flame: Simplifying Topology Extension in Federated Learning"}]}
