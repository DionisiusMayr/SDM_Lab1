{"paperId": "5db3257a61d86f302767ae1f21d6fd30567f12e5", "publicationVenue": {"id": "0d6f7fba-7092-46b3-8039-93458dba736b", "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "type": "conference", "alternate_names": ["Int Conf Acoust Speech Signal Process", "IEEE Int Conf Acoust Speech Signal Process", "ICASSP", "International Conference on Acoustics, Speech, and Signal Processing"], "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"}, "title": "Towards Building the Federated GPT: Federated Instruction Tuning", "abstract": "While\"instruction-tuned\"generative large language models (LLMs) have demonstrated an impressive ability to generalize to new tasks, the training phases heavily rely on large amounts of diverse and high-quality instruction data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data, especially when it comes to human-written data, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to such data, making the process of obtaining it a complex and nuanced undertaking. Consequently, this hinders the generality of the tuned models and may restrict their effectiveness in certain contexts. To tackle this issue, our study introduces a new approach called Federated Instruction Tuning (FedIT), which leverages federated learning (FL) as the learning framework for the instruction tuning of LLMs. This marks the first exploration of FL-based instruction tuning for LLMs. This is especially important since text data is predominantly generated by end users. Therefore, it is imperative to design and adapt FL approaches to effectively leverage these users' diverse instructions stored on local devices, while preserving privacy and ensuring data security. In the current paper, by conducting widely used GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous and diverse sets of instructions on the client's end with the proposed framework FedIT, we improved the performance of LLMs compared to centralized training with only limited local instructions. Further, in this paper, we developed a Github repository named Shepherd. This repository offers a foundational framework for exploring federated fine-tuning of LLMs using heterogeneous instructions across diverse categories.", "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "year": 2023, "fieldsOfStudy": ["Computer Science", "Engineering"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-05-09", "journal": {"name": "ArXiv", "volume": "abs/2305.05644"}, "authors": [{"authorId": "2047971184", "name": "Jianyi Zhang"}, {"authorId": "3005436", "name": "Saeed Vahidian"}, {"authorId": "2211526996", "name": "Martin Kuo"}, {"authorId": "2109737569", "name": "Chunyuan Li"}, {"authorId": "1940556", "name": "Ruiyi Zhang"}, {"authorId": "2107926840", "name": "Guoyin Wang"}, {"authorId": "2213153798", "name": "Yiran Chen"}], "citations": [{"paperId": "96562e6878390ce8ce4a4c151f654b1135305e3a", "title": "Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised Video Anomaly Detection: A New Baseline"}, {"paperId": "60071051e176b6e2fadbededadbb08b05e125c13", "title": "Dual-Personalizing Adapter for Federated Foundation Models"}, {"paperId": "d48eb2400161a2ceb1d787be580dc20f61943d73", "title": "FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning"}, {"paperId": "1359b818543bfde35f0966b74612d18ae28ca41f", "title": "On Protecting the Data Privacy of Large Language Models (LLMs): A Survey"}, {"paperId": "7ae48b24cbf955bf9b9498fb287bf4c5cd3b73d4", "title": "OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning"}, {"paperId": "0255001544a130e64802b03ba03d4fdd0cd34dbb", "title": "On the Convergence of Zeroth-Order Federated Tuning for Large Language Models"}, {"paperId": "f39d1a38b1b9513d6d50ceb8c4663a97deafd632", "title": "A Fast, Performant, Secure Distributed Training Framework For Large Language Model"}, {"paperId": "5423067581ac9d51d19253c9085b871ef0373dfe", "title": "FedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models"}, {"paperId": "06eae596ea3e996453039f6a2cc68732cbba884b", "title": "Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes"}, {"paperId": "f416c5574c9741e14f0fd728112178bcda382c21", "title": "Grounding Foundation Models through Federated Transfer Learning: A General Framework"}, {"paperId": "f7ea50dea238913894dde804d696f22b13c7cbdf", "title": "Multi-Objective Intrinsic Reward Learning for Conversational Recommender Systems"}, {"paperId": "8a75ffe04999efeff039085c8b160b1b4ec6a897", "title": "FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing"}, {"paperId": "61f46dbe000930877c5da4d8628c63ce1ce2df82", "title": "Profit: Benchmarking Personalization and Robustness Trade-off in Federated Prompt Tuning"}, {"paperId": "004699d8ed6d1929e484aba461391fadb1f5b0a7", "title": "Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly"}, {"paperId": "ab90da70bf4671bb95d8e7ff97b2cce19768c579", "title": "Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models"}, {"paperId": "4d08d652a80050d3682a626ca0fa388534a160b4", "title": "Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models (Vision Paper)"}, {"paperId": "e7987503524dfbcf118c4ccdd303cc798bb90b47", "title": "A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications"}, {"paperId": "383530e4661a1b3707c11d2f285e783470c07bf2", "title": "CEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction"}, {"paperId": "3cb399d96cd70f5e9aa4ffecf7d329d1b0910745", "title": "FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning"}, {"paperId": "f5e670c22d1125de557aaa79f721fcfb557fcb36", "title": "Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning"}, {"paperId": "105c11cd7920a606bee6048f129e3440cfba58fd", "title": "NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services"}, {"paperId": "256d20b96fa0ec65a373bfe64f128eb56b4ea508", "title": "Instruction Tuned Models are Quick Learners"}, {"paperId": "4a0ec81169d62cc97abdc0a5dc3875149c417ab9", "title": "When Do Curricula Work in Federated Learning?"}, {"paperId": "b1593fe972b5a734a9ad692f32a60626f27d86dc", "title": "Decentralized digital twins of complex dynamical systems"}, {"paperId": "8094f5b2b124b150167175d871015b5f583b75ff", "title": "NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services"}]}
