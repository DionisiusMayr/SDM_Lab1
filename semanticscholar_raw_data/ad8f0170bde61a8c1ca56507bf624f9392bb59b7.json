{"paperId": "ad8f0170bde61a8c1ca56507bf624f9392bb59b7", "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "title": "Ruddit: Norms of Offensiveness for English Reddit Comments", "abstract": "On social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds. Automatic methods to detect offensive language have largely relied on datasets with categorical labels. However, comments can vary in their degree of offensiveness. We create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). The dataset was annotated using Best\u2013Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales. We show that the method produces highly reliable offensiveness scores. Finally, we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-10", "journal": {"pages": "2700-2717"}, "authors": [{"authorId": "35831406", "name": "Rishav Hada"}, {"authorId": "33005304", "name": "S. Sudhir"}, {"authorId": "3047561", "name": "Pushkar Mishra"}, {"authorId": "2169553", "name": "H. Yannakoudakis"}, {"authorId": "143880621", "name": "Saif M. Mohammad"}, {"authorId": "2362276", "name": "Ekaterina Shutova"}], "citations": [{"paperId": "83b40da37bcca822ff2daff18e460e07067bfad3", "title": "\"You are an expert annotator\": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling"}, {"paperId": "54e7d7c185b01866bdf5a45b00bbda33f0cda83c", "title": "Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection"}, {"paperId": "0b5a05dddfd06b27a8cdb7d65648efbcef78b017", "title": "GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis"}, {"paperId": "e563867ff177218d9652d3cb70e3069827bea880", "title": "Enhancing the fairness of offensive memes detection models by mitigating unintended political bias"}, {"paperId": "8b28792f8405b737229afb92c99c579b86d8aa98", "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations"}, {"paperId": "e8bd3bafa4b7a43ffa5d94e41b46cc672fc49f19", "title": "Style Locality for Controllable Generation with kNN Language Models"}, {"paperId": "aa988cf6414025df4ea411ed885f2ea42b25a111", "title": "\"Fifty Shades of Bias\": Normative Ratings of Gender Bias in GPT Generated English Text"}, {"paperId": "5664925fd86e47710fe8b807b20e6276575ebd83", "title": "Perceived masculinity from Facebook photographs of candidates predicts electoral success"}, {"paperId": "8e1d9e6e81d41ab3b598298835a233474892504e", "title": "A Benchmark for Understanding Dialogue Safety in Mental Health Support"}, {"paperId": "b0ed2c0222d5f69d3af59378d8614d8bf7c4c55f", "title": "When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset"}, {"paperId": "f300e98aea22d684f7ad7e11f03c618fceba3421", "title": "Praise or Insult? Identifying Cyberbullying Using Natural Language Processing"}, {"paperId": "81f72b257470b659cffe5e08117d4c37169525b9", "title": "On the rise of fear speech in online social media"}, {"paperId": "6dad3e05bd7cfdb97909bcf0017009a13b0a7f9f", "title": "Punctuating the other: Graphic cues, voice, and positioning in digital discourse"}, {"paperId": "babecfb5f1e034d1d9787c8efab2a954d346379c", "title": "Reddit Comment Toxicity Score Prediction through BERT via Transformer Based Architecture"}, {"paperId": "47565b408e5d08a97923e4e901f586877c1e3a6a", "title": "Which One Is More Toxic? Findings from Jigsaw Rate Severity of Toxic Comments"}, {"paperId": "693be8fc0fe567c67abc507fd0aebe3009ce2f29", "title": "The Reddit Politosphere: A Large-Scale Text and Network Resource of Online Political Discourse"}, {"paperId": "17c310a398c7d106b8b953ac3feef9e9e5c24f00", "title": "Analyzing the Intensity of Complaints on Social Media"}, {"paperId": "2246ddfc86c4bcb05755b5eb56111062b09a2812", "title": "CRUSH: Contextually Regularized and User anchored Self-supervised Hate speech Detection"}, {"paperId": "f56cda7ee6b3cfa427d045b6cc754ec68349c511", "title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts"}, {"paperId": "9bc2ac6702a941b2ab72fe981404b6e011ef24b6", "title": "WLV-RIT at GermEval 2021: Multitask Learning with Transformers to Detect Toxic, Engaging, and Fact-Claiming Comments"}, {"paperId": "0ac0ff4b6909d5ce7f8a069728eea51ca662c2ba", "title": "BERT-based Approach to Arabic Hate Speech and Offensive Language Detection in Twitter: Exploiting Emojis and Sentiment Analysis"}, {"paperId": "db16c168f8a749c6e32970ac0746e5175dc2efda", "title": "GOF at Arabic Hate Speech 2022: Breaking The Loss Function Convention For Data-Imbalanced Arabic Offensive Text Detection"}, {"paperId": "7b5819fe15ef5033c457c18e2e5a77025ed1e876", "title": "The lack of theory is painful: Modeling Harshness in Peer Review Comments"}, {"paperId": "8b35f691ca2204a7ec79b901e0fafda7069bcf18", "title": "Multilingual Abusive Comment Detection at Scale for Indic Languages"}, {"paperId": "bc7688e229b15e4ce585cf5f71cc00bc38618284", "title": "Leveraging time-dependent lexical features for offensive language detection"}]}
