{"paperId": "db4111f3e2569279d8c4cf1d6a64419aab539482", "publicationVenue": {"id": "d13e941e-4cac-4f1d-bdca-77d927e31f1b", "name": "ACM Symposium on Cloud Computing", "type": "conference", "alternate_names": ["System-on-Chip Conference", "ACM Symp Cloud Comput", "Syst Conf", "Symp Cloud Comput", "Annual IEEE International System-on-Chip Conference", "Symposium on Cloud Computing", "Annu IEEE Int Syst Conf", "SoCC"], "url": "http://www.ieee-socc.org/"}, "title": "Libra and the Art of Task Sizing in Big-Data Analytic Systems", "abstract": "Despite extensive investigation of job scheduling in data-intensive computation frameworks, less consideration has been given to optimizing job partitioning for resource utilization and efficient processing. Instead, partitioning and job sizing are a form of dark art, typically left to developer intuition and trial-and-error style experimentation. In this work, we propose that just as job scheduling and resource allocation are out-sourced to a trusted mechanism external to the workload, so too should be the responsibility for partitioning data as a determinant for task size. Job partitioning essentially involves determining the partition sizes to match the resource allocation at the finest granularity. This is a complex, multi-dimensional problem that is highly application specific: resource allocation, computational runtime, shuffle and reduce communication requirements, and task startup overheads all have strong influence on the most effective task size for efficient processing. Depending on the partition size, the job completion time can differ by as much as 10 times! Fortunately, we observe a general trend underlying the tradeoff between full resource utilization and system overhead across different settings. The optimal job partition size balances these two conflicting forces. Given this trend, we design Libra to automate job partitioning as a framework extension. We integrate Libra with Spark and evaluate its performance on EC2. Compared to state-of-the-art techniques, Libra can reduce the individual job execution time by 25% to 70%.", "venue": "ACM Symposium on Cloud Computing", "year": 2019, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2019-11-20", "journal": {"name": "Proceedings of the ACM Symposium on Cloud Computing"}, "authors": [{"authorId": "2150928125", "name": "Ruikang Li"}, {"authorId": "3073889", "name": "Peizhen Guo"}, {"authorId": "2118095167", "name": "Bo Hu"}, {"authorId": "7848960", "name": "Wenjun Hu"}], "citations": [{"paperId": "ff22bc3e6cb0d654e81bde2b8a7ae94cf90b4ded", "title": "Learning-Based Decentralized Offloading Decision Making in an Adversarial Environment"}, {"paperId": "fecd4289d2a3c3c1bb85cf9652a24ff1f56b4c4a", "title": "SplitServe: Efficiently Splitting Apache Spark Jobs Across FaaS and IaaS"}, {"paperId": "47cd9067b3f446bb9d61181909505e2715f1b205", "title": "Heterogeneous MacroTasking (HeMT) for Parallel Processing in the Cloud"}]}
