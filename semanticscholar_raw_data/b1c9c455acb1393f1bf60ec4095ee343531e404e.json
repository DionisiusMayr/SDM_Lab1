{"paperId": "b1c9c455acb1393f1bf60ec4095ee343531e404e", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use", "abstract": "Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers -- we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to. Our code is in https://github.com/HowieHwong/MetaTool.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-04", "journal": {"name": "ArXiv", "volume": "abs/2310.03128"}, "authors": [{"authorId": "2257084278", "name": "Yue Huang"}, {"authorId": "2256650213", "name": "Jiawen Shi"}, {"authorId": "2257259410", "name": "Yuan Li"}, {"authorId": "145699459", "name": "Chenrui Fan"}, {"authorId": "2254867423", "name": "Siyuan Wu"}, {"authorId": "2254328621", "name": "Qihui Zhang"}, {"authorId": "2254346817", "name": "Yixin Liu"}, {"authorId": "2221116622", "name": "Pan Zhou"}, {"authorId": "2254266993", "name": "Yao Wan"}, {"authorId": "2249536787", "name": "Neil Zhenqiang Gong"}, {"authorId": "2243348413", "name": "Lichao Sun"}], "citations": [{"paperId": "1819a53eddb4a5334937561bb57542d7f11c8308", "title": "What Are Tools Anyway? A Survey from the Language Model Perspective"}, {"paperId": "ab12d8c5e607d702f2fc02ea711960509f9ec1a9", "title": "m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks"}, {"paperId": "66eae99f971b8dc3b3fd83e335848d9c95594bb5", "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models"}, {"paperId": "f9c353dc639cced8cd098fe6d9c92bdef53ebc19", "title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents"}, {"paperId": "966ba2acfe0700c2410efe15ed1b6c25340b7a95", "title": "Learning to Use Tools via Cooperative and Interactive Agents"}, {"paperId": "a6f7485dfdf45320e82d84bcfdc51bcd52dff18b", "title": "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"}, {"paperId": "45ea1b47195d3d381e4b08f8cc0be3568c780ea9", "title": "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs"}, {"paperId": "a3c05430f5af431d21c3707905ae4deec84c0041", "title": "I Think, Therefore I am: Benchmarking Awareness of Large Language Models Using AwareBench"}, {"paperId": "5a9f79660472e894e482bae011c122f35f9a5095", "title": "Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios"}, {"paperId": "8fb92f51434543c4a8cd4980f84cf04552c712cc", "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning"}, {"paperId": "4f2a56102bcbf0fe79379c4c27daecbccfb35a26", "title": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"}, {"paperId": "d6beb9cc394f1e2046371678737346f05270ca91", "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning"}, {"paperId": "29d02b653ea90f3f0b8793aefcca0c0b39882946", "title": "Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game"}, {"paperId": "72273f7a050529fc71c7d45c0256d2b9754f56bb", "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration"}, {"paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2", "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"}, {"paperId": "b62a2919aa398403e42e5550997e5839c11910c7", "title": "Search-Adaptor: Embedding Customization for Information Retrieval"}, {"paperId": "7f56b3005b2e66ecc6598d7376275b3bf8aea543", "title": "I Think, Therefore I am: Awareness in Large Language Models"}]}
