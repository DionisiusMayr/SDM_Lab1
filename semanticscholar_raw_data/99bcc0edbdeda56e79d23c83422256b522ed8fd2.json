{"paperId": "99bcc0edbdeda56e79d23c83422256b522ed8fd2", "publicationVenue": {"id": "d135477d-8b21-429b-8564-b80f938a0147", "name": "International Conference on High Performance Computing", "type": "conference", "alternate_names": ["HiPC", "High Performance Computing Symposium", "IEEE International Conference on High Performance Computing, Data, and Analytics", "HPC", "Int Adv Res Workshop High Perform Comput", "Int Conf High Perform Comput", "International Advanced Research Workshop on High Performance Computing", "High Perform Comput Symp", "IEEE Int Conf High Perform Comput Data Anal"], "url": "http://www.wikicfp.com/cfp/program?id=1192"}, "title": "Optimizing the Training of Co-Located Deep Learning Models Using Cache-Aware Staggering", "abstract": "Despite significant advances, training deep learning models remains a time-consuming and resource-intensive task. One of the key challenges in this context is the ingestion of the training data, which involves non-trivial overheads: read the training data from a remote repository, apply augmentations and transformations, shuffle the training samples, and assemble them into mini-batches. Despite the introduction of abstractions such as data pipelines that aim to hide such overheads asynchronously, it is often the case that the data ingestion is slower than the training, causing a delay at each training iteration. This problem is further augmented when training multiple deep learning models simultaneously on powerful compute nodes that feature multiple GPUs. In this case, the training data is often reused across different training instances (e.g., in the case of multi-model or ensemble training) or even within the same training instance (e.g., data-parallel training). However, transparent caching solutions (e.g., OS-level POSIX caching) are not suitable to directly mitigate the competition between training instances that reuse the same training data. In this paper, we study the problem of how to minimize the makespan of running two training instances that reuse the same training data. The makespan is subject to a trade-off: if the training instances start at the same time, competition for I/O bandwidth slows down the data pipelines and increases the makespan. If one training instance is staggered, competition is reduced but the makespan increases. We aim to optimize this trade-off by proposing a performance model capable of predicting the makespan based on the staggering between the training instances, which can be used to find the optimal staggering that triggers just enough competition to make optimal use of transparent caching in order to minimize the makespan. Experiments with different combinations of learning models using the same training data demonstrate that (1) staggering is important to minimize the makespan; (2) our performance model is accurate and can predict the optimal staggering in advance based on calibration overhead.", "venue": "International Conference on High Performance Computing", "year": 2023, "fieldsOfStudy": null, "publicationTypes": ["Conference"], "publicationDate": "2023-12-18", "journal": {"name": "2023 IEEE 30th International Conference on High Performance Computing, Data, and Analytics (HiPC)", "pages": "246-255"}, "authors": [{"authorId": "2247675174", "name": "Kevin Assogba"}, {"authorId": "2247675273", "name": "Bogdan Nicolae"}, {"authorId": "2248443582", "name": "M. Rafique"}], "citations": []}
