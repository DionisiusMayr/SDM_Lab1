{"paperId": "203a297db586ffb4cd858fe5f219a9a1571c87b2", "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods in Natural Language Processing", "type": "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "title": "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing", "abstract": "We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-10-12", "journal": {"name": "ArXiv", "volume": "abs/2310.08433"}, "authors": [{"authorId": "2257344979", "name": "Carlos G'omez-Rodr'iguez"}, {"authorId": "2257350230", "name": "Paul Williams"}], "citations": [{"paperId": "f2ccc7a70743eb28a2eb8791f754ac96fbf8ff4e", "title": "Train&Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases"}, {"paperId": "573dbc7d2d4f63e8c045225c03d606284290f4f8", "title": "Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications"}, {"paperId": "b28ad67a90bea98eafefe6259b888c1d75b2ccbb", "title": "T-RAG: Lessons from the LLM Trenches"}, {"paperId": "de534c7a644ea6017ddf03e22cdabc0724758f7d", "title": "Personalized Text Generation with Fine-Grained Linguistic Control"}, {"paperId": "b03460fbae0ea0356ea7d245a7804cba50ebdde7", "title": "Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks"}, {"paperId": "8396c663cae6727bfa7d81b6a16e055bdf47e1b0", "title": "From Prompt Engineering to Prompt Science With Human in the Loop"}]}
