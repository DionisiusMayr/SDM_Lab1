{"paperId": "53e9260a6e4cd6c9aa8c8a3607c9de5ef2207593", "publicationVenue": null, "title": "Eliminating Communication Bottlenecks in Cross-Device Federated Learning with In-Network Processing at the Edge", "abstract": "Nowadays, cross-device federated learning (FL) is the key to achieving personalization services for mobile users and has been widely employed by companies like Google, Microsoft, and Alibaba in production. With the explosive increase of participants, the central FL server, which acts as the manager and aggregator of cross-device model training, would get overloaded, becoming the system bottlenecks. Inspired by the emerging wave of edge computing, an interesting question is: could edge clouds help cross-device FL systems overcome the bottleneck?This article provides a cautiously optimistic answer by proposing INP, an FL-specific In-Network Processing framework, along with the novel Model Download Protocol of MDP and Model Upload Protocol of MUP. With MDP and MUP, edge cloud nodes along the paths in INP can easily eliminate duplicated model downloads and pre-aggregate associated gradient uploads for the central FL server, thus alleviating its bottleneck effect, and further accelerating the entire training progress significantly.", "venue": "ICC 2022 - IEEE International Conference on Communications", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-05-16", "journal": {"name": "ICC 2022 - IEEE International Conference on Communications", "pages": "4601-4606"}, "authors": [{"authorId": "2962874", "name": "Shouxi Luo"}, {"authorId": "145563833", "name": "P. Fan"}, {"authorId": "38092176", "name": "Huanlai Xing"}, {"authorId": "2064538", "name": "Long Luo"}, {"authorId": "49514984", "name": "Hongfang Yu"}], "citations": [{"paperId": "ba6261f420bff7ae659ac56d797f014fc2061669", "title": "FedGSync: Jointly Optimized Weak Synchronization and Gradient Transmission for Fast Distributed Machine Learning in Heterogeneous WAN"}, {"paperId": "4cc6008b3615804bd02f55ec9848028d0c726968", "title": "Maximizing Aggregation Throughput for Distributed Training with Constrained In-Network Computing"}, {"paperId": "74cb59f7980fd298da53e46d95a4c2319be3bb07", "title": "Fast Parameter Synchronization for Distributed Learning with Selective Multicast"}]}
