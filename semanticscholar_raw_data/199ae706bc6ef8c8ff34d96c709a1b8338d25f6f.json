{"paperId": "199ae706bc6ef8c8ff34d96c709a1b8338d25f6f", "publicationVenue": null, "title": "Tech Report of Distributed Deep Learning on Data Systems: A Comparative Analysis of Approaches", "abstract": "Deep learning (DL) is growing in popularity for many data analytics applications, including among enterprises. Large business-critical datasets in such settings typically reside in RDBMSs or other data systems. The DB community has long aimed to bring machine learning (ML) to DBMS-resident data. Given past lessons from in-DBMS ML and recent advances in scalable DL systems, DBMS and cloud vendors are increasingly interested in adding more DL support for DB-resident data. Recently, a new parallel DL model selection execution approach called Model Hopper Parallelism (MOP) was proposed. In this paper, we characterize the particular suitability of MOP for DL on data systems, but to bring MOP-based DL to DB-resident data, we show that there is no single \u201cbest\u201d approach, and an interesting tradeoff space of approaches exists. We explain four canonical approaches and build prototypes upon Greenplum Data-base, compare them analytically on multiple criteria (e.g., runtime efficiency and ease of governance) and compare them empirically with large-scale DL workloads. Our experiments and analyses show that it is non-trivial to meet all practical desiderata well and there is a Pareto frontier; for instance, some approaches are 3x-6x faster but fare worse on governance and portability. Our results and insights can help DBMS and cloud vendors design better DL support for DB users. All of our source code, data, and other artifacts are available at https://github.com/makemebitter/cerebro-ds.", "venue": "", "year": 2020, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2108545581", "name": "Yuhao Zhang"}, {"authorId": "102661793", "name": "F. Mcquillan"}, {"authorId": "34439326", "name": "Nandish Jayaram"}, {"authorId": "2089072910", "name": "Nikhil Kak"}, {"authorId": "2082929827", "name": "Ekta Khanna"}, {"authorId": "2119303314", "name": "Arun Kumar"}], "citations": [{"paperId": "43fe19e89b41a8b89d9aa2b482d18cb717997f9c", "title": "Streaming traffic classification: a hybrid deep learning and big data approach"}, {"paperId": "543aaa8400c2db06e17476bac72669d9bb8e24a4", "title": "The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning"}, {"paperId": "3f88b30c63cb50478fa45c25629f60314f69b89c", "title": "Enabling Efficient Random Access to Hierarchically Compressed Text Data on Diverse GPU Platforms"}, {"paperId": "e980ae1d33a182dc0724348a08c33f0f8a028626", "title": "Lotan: Bridging the Gap between GNNs and Scalable Graph Analytics Engines"}, {"paperId": "5a594238ddeea258b00fa5c4d85051d42f2554b9", "title": "Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning"}, {"paperId": "e18093d0e0c21580426d2fc2024547ab1af77af4", "title": "EdgeNN: Efficient Neural Network Inference for CPU-GPU Integrated Edge Devices"}, {"paperId": "b21331583813ec3b29adf8d3f632e701af1e473e", "title": "The Art of Losing to Win: Using Lossy Image Compression to Improve Data Loading in Deep Learning Pipelines"}, {"paperId": "9ca437cdfad4de05f46b8bc3641c1c635050b312", "title": "Modeling the Training Iteration Time for Heterogeneous Distributed Deep Learning Systems"}, {"paperId": "5205f96c6894b1b1fed6683d0c67f4b1f3b25b45", "title": "Scalable Graph Convolutional Network Training on Distributed-Memory Systems"}, {"paperId": "127b7ef160f82226cc1020cc0f2e34080bc33ed4", "title": "The Tensor Data Platform: Towards an AI-centric Database System"}, {"paperId": "e04ad0b1f7b8e740b51db4102845a8e2c8de0e73", "title": "SOLAR: A Highly Optimized Data Loading Framework for Distributed Training of CNN-based Scientific Surrogates"}, {"paperId": "b5bf66680bec48be9f73df28a1f6dc9cf106931f", "title": "Stochastic Gradient Descent without Full Data Shuffle"}, {"paperId": "4a58f24b0bf620729f43b880b29f801f1917848e", "title": "In-Database Machine Learning with CorgiPile: Stochastic Gradient Descent without Full Data Shuffle"}, {"paperId": "e65a8c5151bb30a034248e03f12f5b4cc6115d25", "title": "Nautilus: An Optimized System for Deep Transfer Learning over Evolving Training Datasets"}, {"paperId": "938ff51b43f95921c6071bf7f85ccb358da23951", "title": "End-to-end Optimization of Machine Learning Prediction Queries"}, {"paperId": "7270ae890660825b4d157fba348cbe1b60c38753", "title": "User-Defined Operators: Efficiently Integrating Custom Algorithms into Modern Databases"}, {"paperId": "f99e9d5eff7c7be9bdea26338f027c77ed7a9792", "title": "File System Support for Privacy-Preserving Analysis and Forensics in Low-Bandwidth Edge Environments"}, {"paperId": "691322a452a4e9e97870b8f330696a232fff25c2", "title": "Database Native Model Selection: Harnessing Deep Neural Networks in Database Systems"}, {"paperId": "8e51f65e05dca6a74576ae896ca39eeea2264bad", "title": "Exploration of Approaches for In-Database ML"}, {"paperId": "e49dd78b6731b1c5168694434a15b915e55e0f9b", "title": "SQLFlow: An Extensible Toolkit Integrating DB and AI"}, {"paperId": "4c55a58372f12aed456f90991fa93df9c53e413e", "title": "Cerebro: A Layered Data Platform for Scalable Deep Learning"}, {"paperId": "4a52a1b8e8b0cf25b4fed2fe15374148364645c1", "title": "Some Damaging Delusions of Deep Learning Practice (and How to Avoid Them)"}, {"paperId": "d3e4222af9356ca643b268239cd33d9607abd3e1", "title": "Integrating Cerebro with Dask"}]}
