{"paperId": "daaa7d4ffb9265226e4baadd2db9a01aa7b2f6fb", "publicationVenue": {"id": "d92b00cb-4036-4669-82e9-a8f04895dcf5", "name": "Research Square", "alternate_names": ["Res Sq"], "issn": "2693-5015"}, "title": "Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts", "abstract": "Sifting through vast textual data and summarizing key information from electronic health records (EHR) imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy on a diverse range of clinical summarization tasks has not yet been rigorously demonstrated. In this work, we apply domain adaptation methods to eight LLMs, spanning six datasets and four distinct clinical summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not improve results. Further, in a clinical reader study with ten physicians, we show that summaries from our best-adapted LLMs are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis highlights challenges faced by both LLMs and human experts. Lastly, we correlate traditional quantitative NLP metrics with reader study scores to enhance our understanding of how these metrics align with physician preferences. Our research marks the first evidence of LLMs outperforming human experts in clinical text summarization across multiple tasks. This implies that integrating LLMs into clinical workflows could alleviate documentation burden, empowering clinicians to focus more on personalized patient care and the inherently human aspects of medicine.", "venue": "Research Square", "year": 2023, "fieldsOfStudy": ["Computer Science", "Medicine"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-30", "journal": {"name": "Research Square"}, "authors": [{"authorId": "1741931192", "name": "Dave Van Veen"}, {"authorId": "150066767", "name": "Cara Van Uden"}, {"authorId": "13794901", "name": "Louis Blankemeier"}, {"authorId": "35935570", "name": "Jean-Benoit Delbrouck"}, {"authorId": "2240531495", "name": "Asad Aali"}, {"authorId": "2253667766", "name": "Christian Bluethgen"}, {"authorId": "2284606367", "name": "Anuj Pareek"}, {"authorId": "2240532529", "name": "Malgorzata Polacin"}, {"authorId": "2240533025", "name": "William Collins"}, {"authorId": "2291413200", "name": "Neera Ahuja"}, {"authorId": "2262340449", "name": "Curtis P. Langlotz"}, {"authorId": "2240527904", "name": "Jason Hom"}, {"authorId": "2250464836", "name": "S. Gatidis"}, {"authorId": "2240532544", "name": "John M. Pauly"}, {"authorId": "2240520791", "name": "Akshay S. Chaudhari"}], "citations": [{"paperId": "1eb62b01d329b3d25f2176f1f763d5f473f13eef", "title": "Towards a Robust Retrieval-Based Summarization System"}, {"paperId": "f44c159e747599e5eadb82939a332a36103fd3bb", "title": "Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization"}, {"paperId": "0c6c9e8e0f4e82013b283e1e98861a4b60098668", "title": "Do Large Language Models understand Medical Codes?"}, {"paperId": "006134d7d87ad8ad5836c20cac57b326da632672", "title": "S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document"}, {"paperId": "241c2af99e1c6755f65ab1189d1fcad33dc77e07", "title": "A Continued Pretrained LLM Approach for Automatic Medical Note Generation"}, {"paperId": "cbf8b22de75aa61a00e0100b9b4e8abb18bc11c3", "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models"}, {"paperId": "e866687bc3053c0a38aa2847122f50c573df29dc", "title": "A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries"}, {"paperId": "c0082580c4b9e5c6c96cf06f1be67c0cbbafb753", "title": "A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models"}, {"paperId": "ea89b058ce619ed16d4de633126b02a8179457c8", "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)"}, {"paperId": "4a3e0c90baa440d7e327f4b156bca3a40cefbd40", "title": "SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs"}, {"paperId": "91664510519b562d17ad5e3cde3a282b92a77fac", "title": "DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation"}, {"paperId": "bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3", "title": "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"}]}
