{"paperId": "b6476f998c29577358942be9b25eb904632075ea", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "ATOM: Asynchronous Training of Massive Models for Deep Learning in a Decentralized Environment", "abstract": "The advent of the Transformer architecture has propelled the growth of natural language processing (NLP) models, leading to remarkable achievements in numerous NLP tasks. Yet, the absence of specialized hardware like expansive GPU memory and high-speed interconnects poses challenges for training large-scale models. This makes it daunting for many users to experiment with pre-training and fine-tuning large language models (LLMs). In this study, we introduce \\atom, a resilient distributed training framework designed for asynchronous training of vast models in a decentralized setting using cost-effective hardware, including consumer-grade GPUs and Ethernet. Unlike conventional model partitioning methods that distribute sub-models across GPUs, \\atom aims to accommodate a complete LLM on one host (peer) through seamlessly model swapping and concurrently trains multiple copies across various peers to optimize training throughput. Through static analysis, \\atom identifies the best model partitioning strategy and flawlessly merges model execution with swapping. Key benefits of \\atom include: Avoiding the central point of failure found in pipeline parallelism methods. Demonstrating superior performance and scalability compared to closely-integrated pipeline parallelism in slower networks. Our experiments using different GPT-3 model configurations reveal that, in scenarios with suboptimal network connections, \\atom can enhance training efficiency up to $20 \\times$ when juxtaposed with the state-of-the-art decentralized pipeline parallelism approaches.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-03-15", "journal": {"name": "ArXiv", "volume": "abs/2403.10504"}, "authors": [{"authorId": "2108015474", "name": "Xiaofeng Wu"}, {"authorId": "2292104164", "name": "Jia Rao"}, {"authorId": "2292171011", "name": "Wei Chen"}], "citations": []}
