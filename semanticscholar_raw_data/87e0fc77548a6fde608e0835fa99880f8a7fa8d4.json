{"paperId": "87e0fc77548a6fde608e0835fa99880f8a7fa8d4", "publicationVenue": {"id": "9448f839-459b-45f3-8573-5eff3f032334", "name": "USENIX Annual Technical Conference", "type": "conference", "alternate_names": ["USENIX Annu Tech Conf", "USENIX", "USENIX ATC"], "url": "https://www.usenix.org/conferences/byname/131"}, "title": "NeuGraph: Parallel Deep Neural Network Computation on Large Graphs", "abstract": "Recent deep learning models have moved beyond low dimensional regular grids such as image, video, and speech, to high-dimensional graph-structured data, such as social networks, e-commerce user-item graphs, and knowledge graphs. This evolution has led to large graph-based neural network models that go beyond what existing deep learning frameworks or graph computing systems are designed for. We present NeuGraph, a new framework that bridges the graph and dataflow models to support efficient and scalable parallel neural network computation on graphs. NeuGraph introduces graph computation optimizations into the management of data partitioning, scheduling, and parallelism in dataflow-based deep learning frameworks. Our evaluation shows that, on small graphs that can fit in a single GPU, NeuGraph outperforms state-of-the-art implementations by a significant margin, while scaling to large real-world graphs that none of the existing frameworks can handle directly with GPUs. (Please stay tuned for further updates.)", "venue": "USENIX Annual Technical Conference", "year": 2019, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-07-02", "journal": {"pages": "443-458"}, "authors": [{"authorId": "2492241", "name": "Lingxiao Ma"}, {"authorId": "98256743", "name": "Zhi Yang"}, {"authorId": "11009920", "name": "Youshan Miao"}, {"authorId": "2870618", "name": "Jilong Xue"}, {"authorId": "145217421", "name": "Ming Wu"}, {"authorId": "93317102", "name": "Lidong Zhou"}, {"authorId": "34889832", "name": "Yafei Dai"}], "citations": [{"paperId": "5d8d16b2518286351136e01da84bdb773726246c", "title": "GeoT: Tensor Centric Library for Graph Neural Network via Efficient Segment Reduction on GPU"}, {"paperId": "b7cdce5df8ced4861984dcee1366ef4e79666df0", "title": "CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks"}, {"paperId": "c3364b1fb446763cf56d5532f2ded29076e6d01f", "title": "Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling"}, {"paperId": "0a980180974561d3861ee298503426ddf1c3aa65", "title": "PruneGNN: Algorithm-Architecture Pruning Framework for Graph Neural Network Acceleration"}, {"paperId": "48c35e8cfa7ac7e24c448fb21154151097e4f184", "title": "Celeritas: Out-of-Core Based Unsupervised Graph Neural Network via Cross-Layer Computing 2024"}, {"paperId": "9e98cd878653b68710d89248881433f95da35b4b", "title": "Accelerating Graph Neural Networks on Real Processing-In-Memory Systems"}, {"paperId": "087a11fd23bbbd9e1b90b493be7af6245837f391", "title": "PARAG: PIM Architecture for Real-Time Acceleration of GCNs"}, {"paperId": "549d440dd1a8f54fb9022b47118685c1c4885f17", "title": "ADGNN: Towards Scalable GNN Training with Aggregation-Difference Aware Sampling"}, {"paperId": "314934f8d952aa32f6368a0fe5b7595b38cf06f1", "title": "Synergistic Signals: Exploiting Co-Engagement and Semantic Links via Graph Neural Networks"}, {"paperId": "de2057a4615c6f2d04a5b179bffd0287670aebd3", "title": "Accelerating GNN Training by Adapting Large Graphs to Distributed Heterogeneous Architectures"}, {"paperId": "a41245be49f9be605b20b97a7272c7f986d26b8b", "title": "HOT: Higher-Order Dynamic Graph Representation Learning with Efficient Transformers"}, {"paperId": "1c0ae6631bf9918adb80e8c2832e2db5545a95e5", "title": "FastSample: Accelerating Distributed Graph Neural Network Training for Billion-Scale Graphs"}, {"paperId": "6e419d87084f8cc153f79f6c399e48092ba237b0", "title": "HongTu: Scalable Full-Graph GNN Training on Multiple GPUs"}, {"paperId": "254c54a44391018ce432e3aac0b34895cc10c287", "title": "GraNNDis: Efficient Unified Distributed Training Framework for Deep GNNs on Large Clusters"}, {"paperId": "02857e8813d7375a377f5f1ac13e9abf9e9e2385", "title": "Adaptive Load Balanced Scheduling and Operator Overlap Pipeline for Accelerating the Dynamic GNN Training"}, {"paperId": "2f3049d3dfa761c94a54168388d9364f6e1a70ab", "title": "Distributed Matrix-Based Sampling for Graph Neural Network Training"}, {"paperId": "2a72d9a7f8a3c4740fc75b8e62a6648189fb8f19", "title": "Barad-dur: Near-Storage Accelerator for Training Large Graph Neural Networks"}, {"paperId": "b81fbd71db8133f2b6f369b0ea85064eacd60022", "title": "Architectural Implications of GNN Aggregation Programming Abstractions"}, {"paperId": "8ff6afb036dbf6f587c90e0d0603f0fc5bfaa555", "title": "A Survey of Graph Pre-processing Methods: From Algorithmic to Hardware Perspectives"}, {"paperId": "d43d6e4bd9dca7ebfd0978ebfa29b40d1c4b08c0", "title": "DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks"}, {"paperId": "f0f84ea6f9d0c417614d205827fecbc238f82a58", "title": "TurboGNN: Improving the End-to-End Performance for Sampling-Based GNN Training on GPUs"}, {"paperId": "e5859eca194340a1295057e049c204f60a0abd82", "title": "Accelerating Graph Convolutional Networks Through a PIM-Accelerated Approach"}, {"paperId": "38ae2071e0bbaa16d61ea007d438cc24a8a11160", "title": "SPEED: Streaming Partition and Parallel Acceleration for Temporal Interaction Graph Embedding"}, {"paperId": "e5e22ece6c570879f155a5204a4ec97d8d8d901c", "title": "Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction"}, {"paperId": "7bc76d815ae3bf267a970d2244c3b3730336977c", "title": "GNNPipe: Scaling Deep GNN Training with Pipelined Model Parallelism"}, {"paperId": "56ca2759683fe4215fbdfe3b7f362767fedb807d", "title": "Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design"}, {"paperId": "6988501587a310dca1fe0a6b5d3e188b26ff8f12", "title": "Redundancy-Free High-Performance Dynamic GNN Training with Hierarchical Pipeline Parallelism"}, {"paperId": "a7f4d825e3088da30a1908c02db001417b1faa97", "title": "A Multicore GNN Training Accelerator"}, {"paperId": "5fdac410ae76efa58658c00ba46d0de60260cade", "title": "Communication-Free Distributed GNN Training with Vertex Cut"}, {"paperId": "3f57f297eb80171f9c2a900d087cfcac943c4c1e", "title": "DGI: An Easy and Efficient Framework for GNN Model Evaluation"}, {"paperId": "bbe3548b34cbe0a45f331cd81b912f4b9703a657", "title": "PGLBox: Multi-GPU Graph Learning Framework for Web-Scale Recommendation"}, {"paperId": "c1928f42ad6293562df6078b83adef1e78bd224e", "title": "Tango: Rethinking Quantization for Graph Neural Network Training on GPUs"}, {"paperId": "54a863e3155637681e9330c04c7c1928c2d7455e", "title": "Serving Graph Neural Networks With Distributed Fog Servers for Smart IoT Services"}, {"paperId": "77cbb7965bbb55619ddf569a0aff4537ad0e9683", "title": "GHOST: A Graph Neural Network Accelerator using Silicon Photonics"}, {"paperId": "e980ae1d33a182dc0724348a08c33f0f8a028626", "title": "Lotan: Bridging the Gap between GNNs and Scalable Graph Analytics Engines"}, {"paperId": "3603f868b94920c329d6bbee602348c9dc5ef558", "title": "GraphNAS++: Distributed Architecture Search for Graph Neural Networks"}, {"paperId": "b07deb1cadf89d3b578f4ff78d718bb01bf2d315", "title": "Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses"}, {"paperId": "bd14e6ce4eeeed6206f291d8f763719abda3e5b7", "title": "SENSEi: Input-Sensitive Compilation for Accelerating GNNs"}, {"paperId": "894d61c709ec6f61899703458d90b09c663d7b11", "title": "A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware"}, {"paperId": "fcfbad2a3df0bea145cede61299ca361e1b2140c", "title": "BatchGNN: Efficient CPU-Based Distributed GNN Training on Very Large Graphs"}, {"paperId": "e89492751fa4ccd016ea2421f8b6b412f8f72b98", "title": "SPADE: A Flexible and Scalable Accelerator for SpMM and SDDMM"}, {"paperId": "902906c2f99f6ac09ff5476a40f64080a7bb24eb", "title": "MetaNMP: Leveraging Cartesian-Like Product to Accelerate HGNNs with Near-Memory Processing"}, {"paperId": "3229291b9b7e9a7200b543089e05a4442cbe6a9b", "title": "Scalable and Efficient Full-Graph GNN Training for Large Graphs"}, {"paperId": "5aac333b25158c4f330474608c93f461f7ae6689", "title": "Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training"}, {"paperId": "258941db02068ab72d40cb7ef3118a5e3f95e9f4", "title": "Attribute-driven streaming edge partitioning with reconciliations for distributed graph neural network training"}, {"paperId": "1268bf8bbba86570d7ed9ad882d780dedbb1710c", "title": "Graph Neural Network for spatiotemporal data: methods and applications"}, {"paperId": "862be95b98b11124a3420c395cc28729084d3d59", "title": "I/O-Efficient Butterfly Counting at Scale"}, {"paperId": "fa1e8a10ec22c5177a0290d4877d150a9b36dafe", "title": "The Evolution of Distributed Systems for Graph Neural Networks and Their Origin in Graph Processing and Deep Learning: A Survey"}, {"paperId": "548449b18b248f08f851d1ac7123b9d793c526c9", "title": "AdaptGear: Accelerating GNN Training via Adaptive Subgraph-Level Kernels on GPUs"}, {"paperId": "2b8c884e3e7a60bc058775445e8e24d25ef91d4a", "title": "Communication-Efficient Graph Neural Networks with Probabilistic Neighborhood Expansion Analysis and Caching"}, {"paperId": "1f63e99817aa069522d9bc1e2c86b1ded0d36cbe", "title": "GraphTensor: Comprehensive GNN-Acceleration Framework for Efficient Parallel Processing of Massive Datasets"}, {"paperId": "23366f13155e836eb155fe6eeba589c1bf2eac93", "title": "GraphMetaP: Efficient MetaPath Generation for Dynamic Heterogeneous Graph Models"}, {"paperId": "85cec0991826ffb4f44fcd11d56c4c2b1356e64c", "title": "Communication Optimization for Distributed Execution of Graph Neural Networks"}, {"paperId": "4aef75d2e29c3ea0536d1aede5370954ea2ecaa2", "title": "Graph Mining for Cybersecurity: A Survey"}, {"paperId": "fec39f9582a047a6b483c19f75d8ed8f91058d4a", "title": "Dynamic Activation of Clients and Parameters for Federated Learning over Heterogeneous Graphs"}, {"paperId": "87d7c926ed572d9cb83c5e0a8153ef0799a5b501", "title": "MergePath-SpMM: Parallel Sparse Matrix-Matrix Algorithm for Graph Neural Network Acceleration"}, {"paperId": "8bc40c2ea8a526b296041450c73fd6784108bf76", "title": "InferTurbo: A Scalable System for Boosting Full-graph Inference of Graph Neural Network over Huge Graphs"}, {"paperId": "81d9cd7fd934f218d3428fc70fc5b8940c0a1107", "title": "GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism"}, {"paperId": "287bcd4eaecba99c5879adc357dd6ec59a60d983", "title": "A Distributed-GPU Deep Reinforcement Learning System for Solving Large Graph Optimization Problems"}, {"paperId": "656e8c5f8bced540425c12d854b2911dddefff14", "title": "Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review"}, {"paperId": "31880b4e0f5203de0a752072103baf0ae68c0af9", "title": "Boosting Distributed Full-graph GNN Training with Asynchronous One-bit Communication"}, {"paperId": "f0567d091f7e8cd5e6e72d0399f2527113f8021c", "title": "TGOpt: Redundancy-Aware Optimizations for Temporal Graph Attention Networks"}, {"paperId": "656a69cc79dfe67dcee2441e19ec44b43df938b4", "title": "Scalable Neural Network Training over Distributed Graphs"}, {"paperId": "2e24c192aa130183188b67f4ba4450605b6dbc6d", "title": "Software-hardware co-design for accelerating large-scale graph convolutional network inference on FPGA"}, {"paperId": "b506b880d2a818ea34bd65734cf79a03862104b8", "title": "Betty: Enabling Large-Scale GNN Training with Batch-Level Graph Partitioning"}, {"paperId": "9dadfd97013f649fbfa5641ef829cc1d575040e4", "title": "uGrapher: High-Performance Graph Operator Computation via Unified Abstraction for Graph Neural Networks"}, {"paperId": "46210e170045df3c0c50a17bb63e6de480d62f9d", "title": "FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training"}, {"paperId": "ebe8a3c68b88e7c2747f92414a59ba6e708f0670", "title": "PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs"}, {"paperId": "5205f96c6894b1b1fed6683d0c67f4b1f3b25b45", "title": "Scalable Graph Convolutional Network Training on Distributed-Memory Systems"}, {"paperId": "2431364881fa81c7d5215307d495932a85a3735c", "title": "SCGraph: Accelerating Sample-based GNN Training by Staged Caching of Features on GPUs"}, {"paperId": "d4cdadb0355e7c8bad2a6040687ae4f9a9233937", "title": "DGI: Easy and Efficient Inference for GNNs"}, {"paperId": "823a52b2b770d4e198d4693b27e4cebb4822e5ad", "title": "Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry"}, {"paperId": "141c3de5103c8cb11b3119470f1aad438445399a", "title": "A Comprehensive Survey on Distributed Training of Graph Neural Networks"}, {"paperId": "1e79e33c77b2d8eaf643af0e1f5003057d7356b2", "title": "Distributed Graph Neural Network Training: A Survey"}, {"paperId": "e2b4fe32e257d859e02abbf1f2677b7662cb82f3", "title": "WholeGraph: A Fast Graph Neural Network Training Framework with Multi-GPU Distributed Shared Memory Architecture"}, {"paperId": "4d468c7b4345fcd25d40ca67313a743e6e164444", "title": "HGL: Accelerating Heterogeneous GNN Training with Holistic Representation and Optimization"}, {"paperId": "1b2576c824a8c71e559eea72486092575e5dbbde", "title": "GNN at the Edge: Cost-Efficient Graph Neural Network Processing Over Distributed Edge Servers"}, {"paperId": "28173e3a23927c420e621934f01230aa158411fa", "title": "DGS: Communication-Efficient Graph Sampling for Distributed GNN Training"}, {"paperId": "17f3d9b84c21cfc313d5c11f9efba37605cade94", "title": "Software Systems Implementation and Domain-Specific Architectures towards Graph Analytics"}, {"paperId": "a1162fa7e6e09872341001175c39a8662d41162b", "title": "T-GCN: A Sampling Based Streaming Graph Neural Network System with Hybrid Architecture"}, {"paperId": "2f81ccbe5b697ba53cf0fa00dcb6a7fc4b10b092", "title": "Optimizing Aggregate Computation of Graph Neural Networks with on-GPU Interpreter-Style Programming"}, {"paperId": "1cbbe1a129c0d1f53d8e16ca74238334372507dc", "title": "Neural Graph Databases"}, {"paperId": "00304f852a35dc32fd1a0e6b6c6226c7153e3ca9", "title": "MGG: Accelerating Graph Neural Networks with Fine-Grained Intra-Kernel Communication-Computation Pipelining on Multi-GPU Platforms"}, {"paperId": "e57c6fad3da2bcd017eb2742cd49e3f7faded5d0", "title": "Sage: A System for Uncertain Network Analysis"}, {"paperId": "0a942d68964ba104cfd49b675c976c1a1dbb65d1", "title": "MG-GCN: A Scalable multi-GPU GCN Training Framework"}, {"paperId": "6c39755e82d94e00922699ad3f469717b1a16db8", "title": "Classifying tumor brain images using parallel deep learning algorithms"}, {"paperId": "af2bb3a28f95bb4ff5c6f26321a45c39be2b94ed", "title": "Ginex: SSD-enabled Billion-scale Graph Neural Network Training on a Single Machine via Provably Optimal In-memory Caching"}, {"paperId": "ef12b71e7c7cc7483e3c2147ebf9753a31554266", "title": "TLPGNN: A Lightweight Two-Level Parallelism Paradigm for Graph Neural Network Computation on GPU"}, {"paperId": "407422d0dfc5cea343328e8a9562319a95f29e8a", "title": "Research on complex data analysis model based on distribution automation data"}, {"paperId": "5e302753eee290fed68e9f22a8764879639ada1b", "title": "Graphite: optimizing graph neural networks on CPUs through cooperative software-hardware techniques"}, {"paperId": "5be7d06ee1647b4b257036e8d1f3b4a0ecb0b05c", "title": "Hyperscale FPGA-as-a-service architecture for large-scale distributed graph neural network"}, {"paperId": "e08e552c62b40c7ee2ec660b365d11956ef1d40d", "title": "NeutronStar: Distributed GNN Training with Hybrid Dependency Management"}, {"paperId": "8687545d164e7d3fcd7e1642659837e8ad185548", "title": "Federated Graph Learning with Periodic Neighbour Sampling"}, {"paperId": "5f1776b961a8f3080a66a76daf3c38879ad53242", "title": "QEGCN: An FPGA-based accelerator for quantized GCNs with edge-level parallelism"}, {"paperId": "bda02351d387bc63265f9e5671a69be8d386272c", "title": "Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization"}, {"paperId": "8018a561d2ed821cfb4be7ce04993cabf9932c2f", "title": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis"}, {"paperId": "39271db2458f6fb635e40372049cb1e203c7e05c", "title": "SANCUS: Staleness-Aware Communication-Avoiding Full-Graph Decentralized Training in Large-Scale Graph Neural Networks"}, {"paperId": "bcfd5c42af568bd4e15ee3b6be518490a6d7e371", "title": "EC-Graph: A Distributed Graph Neural Network System with Error-Compensated Compression"}, {"paperId": "7a5465cfed15dc98852ff2d0b340320a5f8f06cd", "title": "Exploiting Hierarchical Parallelism and Reusability in Tensor Kernel Processing on Heterogeneous HPC Systems"}, {"paperId": "e4b81c6d782d4da6f4168298f4861f410a7a4f9f", "title": "Fograph: Enabling Real-Time Deep Graph Inference with Fog Computing"}, {"paperId": "e7c462bf521add5615d5716069508648b283afe8", "title": "Optimizing Task Placement and Online Scheduling for Distributed GNN Training Acceleration"}, {"paperId": "3512b4dac371a92a82b13c30758e389dab2c9b67", "title": "Accelerating Backward Aggregation in GCN Training With Execution Path Preparing on GPUs"}, {"paperId": "740b9d4414a955a74c16f5f3617d2b5dde2e9adf", "title": "GNNLab: a factored system for sample-based GNN training over GPUs"}, {"paperId": "20dc2612c0296952a363f2aa5d78d2178503553f", "title": "Rethinking graph data placement for graph neural network training on multiple GPUs"}, {"paperId": "d43acd18e8ddabba5c35e88788723ab8d18345da", "title": "BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks with Partition-Parallelism and Random Boundary Node Sampling"}, {"paperId": "286d371febea99ec19044e69e163e8bd53137a7f", "title": "PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication"}, {"paperId": "6a62ca8c617656caef26a31b13e1cadab6e996e3", "title": "Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations"}, {"paperId": "34f5aa7d37210b2895f40903e05f67d36813adcd", "title": "Design of Graph Neural Network Accelerator Based on Heterogeneous Architecture"}, {"paperId": "1df8b8cc125f667c6495b76e347da621109b0f73", "title": "ByteGNN: Efficient Graph Neural Network Training at Large Scale"}, {"paperId": "b3cbbc1f34a20c22853f3dd347fd635b2e414fd5", "title": "Scaling knowledge graph embedding models for link prediction"}, {"paperId": "4aaff01365a68968e7f17a820fe579177a90899b", "title": "Distributed Hybrid CPU and GPU training for Graph Neural Networks on Billion-Scale Heterogeneous Graphs"}, {"paperId": "94f0823f8db5360972a7a68b453e28ddf9c4e992", "title": "BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing"}, {"paperId": "8a7dd51a7dd86405781f3a35e196a77a502f4b0b", "title": "CM-GCN: A Distributed Framework for Graph Convolutional Networks using Cohesive Mini-batches"}, {"paperId": "1b760a77fcc6c577bc8021fd344a34e6a653e320", "title": "TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs"}, {"paperId": "14358d9111109aa0f1cb2084d5c74b941f11ac7e", "title": "A class-specific metric learning approach for graph embedding by information granulation"}, {"paperId": "b6bf9dc6144dfea4ec1338ee7eb83d84c65588d1", "title": "Efficient Asynchronous GCN Training on a GPU Cluster"}, {"paperId": "5ba725bfd56c24c19b1eca64a3f19c84931fa35a", "title": "A Gather Accelerator for GNNs on FPGA Platform"}, {"paperId": "8ee0b26576492c0ca031143bca5432896f35caf4", "title": "QGTC: accelerating quantized graph neural networks via GPU tensor core"}, {"paperId": "aa166f2a175aa8771a0a196e9ae900ed99a20de9", "title": "Sequential Aggregation and Rematerialization: Distributed Full-batch Training of Graph Neural Networks on Large Graphs"}, {"paperId": "5b91a2384e08433da2f2bafd2e4574ba8ecaecc4", "title": "Graph Neural Network Training and Data Tiering"}, {"paperId": "6cdc6eebdec0e5c1ba47db4a0d0c4381cc82d0ed", "title": "FedGraph: Federated Graph Learning With Intelligent Sampling"}, {"paperId": "0ee96b161c5ce7e377ba93e7f74a7182ecb2a6ac", "title": "GNNear: Accelerating Full-Batch Training of Graph Neural Networks with near-Memory Processing"}, {"paperId": "52a1f6849fab49681dc4005f176d7443c40e468a", "title": "A Deep Dive Into Understanding The Random Walk-Based Temporal Graph Learning"}, {"paperId": "20915cba97913945c01ee1ab57a03080caa1776b", "title": "SketchNE: Embedding Billion-Scale Networks Accurately in One Hour"}, {"paperId": "bfc1fbd304707adf8e55f18b0ac71c70549a52e7", "title": "Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective"}, {"paperId": "8cc8bf086f840de138e11da9ae93a3ce00a5d22b", "title": "MG-GCN: Scalable Multi-GPU GCN Training Framework"}, {"paperId": "8d93f1738790954941105d5631ad777c85fd08c1", "title": "Accelerating Training and Inference of Graph Neural Networks with Fast Sampling and Pipelining"}, {"paperId": "b588110ee9e30a1b1ee130c3323f8eef308a59dc", "title": "Efficient Data Loader for Fast Sampling-Based GNN Training on Large Graphs"}, {"paperId": "2a87e8dd28e6faba45471f2006d646c45449194f", "title": "Efficient Scaling of Dynamic Graph Neural Networks"}, {"paperId": "91129013e17d8355bc1a87603789005aa2cf42f3", "title": "IGNNITION: Bridging the Gap between Graph Neural Networks and Networking Systems"}, {"paperId": "9ae5cc2b23df6247d3a16f60bc1a6fd377198885", "title": "Understanding the Design Space of Sparse/Dense Multiphase Dataflows for Mapping Graph Neural Networks on Spatial Accelerators."}, {"paperId": "b40352e71bae7ecb1dfe93cdad044749202eb6cb", "title": "PCGraph: Accelerating GNN Inference on Large Graphs via Partition Caching"}, {"paperId": "b65556aebe4eb38bc88f9808ab9e5dd7fda4354b", "title": "GNNSampler: Bridging the Gap between Sampling Algorithms of GNN and Hardware"}, {"paperId": "5635ddb417c964f4e8b0334fb07fbe1b0e93a7fa", "title": "Accelerating GNN training with locality-aware partial execution"}, {"paperId": "6b76654f74df814bb778fe5d03e088c69489e347", "title": "Automatic Generation of High-Performance Inference Kernels for Graph Neural Networks on Multi-Core Systems"}, {"paperId": "e29466b2b03949ba65c5106b426fc5586b8d2caa", "title": "ZIPPER: Exploiting Tile- and Operator-level Parallelism for General and Scalable Graph Neural Network Acceleration"}, {"paperId": "e3c1bb88b4b8299d331d83e4dce7837caa6db93d", "title": "GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings"}, {"paperId": "0aa0e47b15b59e5508bea390b196f8cd3758b6dc", "title": "Vertex-Centric Visual Programming for Graph Neural Networks"}, {"paperId": "fb1a90bd9179e6d3f754b565847523a3dc775671", "title": "Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads"}, {"paperId": "8f3c5bd1bba7ec2b79ca38998777a525153ecd6f", "title": "OpenGraphGym-MG: Using Reinforcement Learning to Solve Large Graph Optimization Problems on MultiGPU Systems"}, {"paperId": "674819a81b1f27e65a4ea3de86bfdafb52612223", "title": "Scalable Graph Neural Network Training"}, {"paperId": "016e5d45c8f932e59d8d048bae357469cccf1064", "title": "FlexGraph: a flexible and efficient distributed framework for GNN training"}, {"paperId": "3308eeac303fd042f8d7659d9c43faa35173c39b", "title": "Seastar: vertex-centric programming for graph neural networks"}, {"paperId": "0d6c6e8d01945b9664c8cd9ac704888149dec5e2", "title": "DGCL: an efficient communication library for distributed GNN training"}, {"paperId": "3e190752057e4d54f655a90de45a28fb876cacd3", "title": "GraphTheta: A Distributed Graph Neural Network Learning System With Flexible Training Strategy"}, {"paperId": "933a839b7f1bcaac7be5e7c100f7a5501e268803", "title": "GMLP: Building Scalable and Flexible Graph Neural Networks with Feature-Message Passing"}, {"paperId": "b0d62f38592dbae23628d9700490cb11ac873182", "title": "DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks"}, {"paperId": "96c953826918394468ba4791749274ac966faf5f", "title": "ADGraph: Accurate, Distributed Training on Large Graphs"}, {"paperId": "3eabdfc1e8ebc3c3a5b40676ad6cc55dad2469ef", "title": "Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture"}, {"paperId": "311171c54d9c8e2c412e4ac79773166b19dc706d", "title": "GNNMark: A Benchmark Suite to Characterize Graph Neural Network Training on GPUs"}, {"paperId": "99016bc147399e86cdd0775bf7d7031de223570b", "title": "A Taxonomy for Classification and Comparison of Dataflows for GNN Accelerators"}, {"paperId": "edb7a4e369d9d73d876d66b8a4a5c6b3497fbda8", "title": "Understanding and bridging the gaps in current GNN performance optimizations"}, {"paperId": "f7b5b770914b250ec1114151c44337ef5f612b1f", "title": "GraphGallery: A Platform for Fast Benchmarking and Easy Development of Graph Neural Networks Based Intelligent Software"}, {"paperId": "cbb5d6214322ce32f33f68dbf0c20341c22d7c7b", "title": "Analyzing the Performance of Graph Neural Networks with Pipe Parallelism"}, {"paperId": "49535398cd1e13e01c1b71576d6c4717f1b29c3f", "title": "Distributed Training of Graph Convolutional Networks using Subgraph Approximation"}, {"paperId": "76e5889bda726945a9d49fd87a8726daff00d1ff", "title": "fuseGNN: Accelerating Graph Convolutional Neural Network Training on GPGPU"}, {"paperId": "c72f3f0ab953ed4e42b66b071471ce26dc0e4675", "title": "PaGraph: Scaling GNN training on large graphs via computation-aware caching"}, {"paperId": "037df1500b9b8d4a57455b7ad205f86cc94a0b13", "title": "DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs"}, {"paperId": "2e92728e393538304555e48a8d7532daf5ebabd4", "title": "Computing Graph Neural Networks: A Survey from Algorithms to Accelerators"}, {"paperId": "528ff02f6d6505bd121107edd69ab9c777a53d7e", "title": "Accelerating graph sampling for graph machine learning using GPUs"}, {"paperId": "248031105cc00737877ded134852d0edeb43b73c", "title": "FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems"}, {"paperId": "3bbb1f931400b86e0b81e1e64a7834aeebdb49c6", "title": "G3"}, {"paperId": "c50e20fd223807a6a692103bcd5530f0388c15e7", "title": "GRIP: A Graph Neural Network Accelerator Architecture"}, {"paperId": "f9d34299bab1ac9063f361514b75d626a65e08ba", "title": "GE-SpMM: General-Purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks"}, {"paperId": "a72bbf818135b30ab24835e663bc8dcb7b8274ff", "title": "GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs"}, {"paperId": "7ccbe9525f69449b2eaaa86a7b3947b1d965e9c2", "title": "GNNAdvisor: An Efficient Runtime System for GNN Acceleration on GPUs"}, {"paperId": "6dcf5882576153d3d80945d7013acd1b9ef140c7", "title": "Reducing Communication in Graph Neural Network Training"}, {"paperId": "f39c4f757f0378914052f8856a85fc90956e9329", "title": "PCGCN: Partition-Centric Processing for Accelerating Graph Convolutional Network"}, {"paperId": "8e5a30011f16715d3ca58a81cf0b2f96a2f26534", "title": "Architectural Implications of Graph Neural Networks"}, {"paperId": "381411d740562de1e766dc8cc833844eb99dde01", "title": "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks."}, {"paperId": "1a3cdea3fe527ce1165c8758ab24aee46c08bb19", "title": "EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks"}, {"paperId": "5d137be96e0358fc8ee572234b718478a11c5bec", "title": "A Distributed Multi-GPU System for Fast Graph Processing"}, {"paperId": "1c63814a9ca53984330e23050b73c9abd0ebdc30", "title": "GNNPipe: Accelerating Distributed Full-Graph GNN Training with Pipelined Model Parallelism"}, {"paperId": "e0c94287ec33ca5caf94149c84ae58c6dc8eb76d", "title": "Empowering GNNs with Fine-grained Communication-Computation Pipelining on Multi-GPU Platforms"}, {"paperId": "fe3d6c9a7e76a175ca90f7ce05beb9965f148f26", "title": "the Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation."}, {"paperId": "d82362e1cb341e376d9e60784d4961b5ee090f81", "title": "P IPE GCN: E FFICIENT F ULL -G RAPH T RAINING OF G RAPH C ONVOLUTIONAL N ETWORKS WITH P IPELINED F EATURE C OMMUNICATION"}, {"paperId": "984b9c3dd2b1c3320b387dd7e1eccc50d7b44226", "title": "Distributed Graph Neural Network Training with Periodic Historical Embedding Synchronization"}, {"paperId": "a83037d8b73c4021c543209293f8294ca2f3f56f", "title": "GDLL: A Scalable and Share Nothing Architecture based Distributed Graph Neural Networks Framework"}, {"paperId": "60b767ff6474a7861616fac3918cc609dd4debc7", "title": "TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs"}, {"paperId": "40f2b55a2fc836bb2b05d1f6f86e0300ec213e12", "title": "Graphiler: Optimizing Graph Neural Networks with Message Passing Data Flow Graph"}, {"paperId": "af0d2f8b21334ea9d6dd05254923707f605635d6", "title": "A Review on Community Detection in Large Complex Networks from Conventional to Deep Learning Methods: A Call for the Use of Parallel Meta-Heuristic Algorithms"}, {"paperId": "65582abd2ea66caee431bcfe29b99cb811b40a9c", "title": "GCNear: A Hybrid Architecture for Efficient GCN Training with Near-Memory Processing"}, {"paperId": "24ebe55d7688c53a978c1519da5b766093eedd5f", "title": "QGTC: Accelerating Quantized GNN via GPU Tensor Core"}, {"paperId": "a7b7e1d384a6fc03ccc96cf3d76a9c2866a201f9", "title": "TC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs"}, {"paperId": "9de73bbcf64e800d300d89bcc8cbb1100616e8c6", "title": "Shugra: A Shuf\ufb02ing-Driven Graph Convolutional Network Accelerator"}, {"paperId": "8ea833e0c4c0b77165b7f7dcd81a210d63b75adf", "title": "F EATURE C OMMUNICATION"}, {"paperId": "3cdcd5d33df0eeb0abd1fac29049c4bb3f1a80da", "title": "Distributed Hybrid CPU and GPU training for Graph Neural Networks on Billion-Scale Graphs"}, {"paperId": "ffc9aff36a100350736edec9f9c1747a8fe068cc", "title": "Advanced Computer Architecture: 13th Conference, ACA 2020, Kunming, China, August 13\u201315, 2020, Proceedings"}, {"paperId": "1d03e4bebc9cf3c3fdd9204504d92b20d97d1fdf", "title": "GNN-PIM: A Processing-in-Memory Architecture for Graph Neural Networks"}, {"paperId": "3b31000fbcb11707c456d659884a474a3e2af0ab", "title": "Distributed Graph Processing: Techniques and Systems"}, {"paperId": "426f449a52d0ecf0cdd87ca99450a6a9ab8cfb58", "title": "GIN : High-Performance, Scalable Inference for Graph Neural Networks"}, {"paperId": "833e39087f70e91ad02e9d1de197912efc7154b5", "title": "Flexible and Efficient Systems for Training Emerging Deep Neural Networks"}, {"paperId": "ec9b3a3f562cfdb6e95d72eb5dc4d4d77f3d5f08", "title": "This paper is included in the Proceedings of the 15th USENIX Symposium on Operating Systems Design and Implementation"}, {"paperId": "72d8a80ddff14bbd9dccca08d006ed248a089d3f", "title": "Sparsity-aware communication for distributed graph neural network training"}, {"paperId": "febbe50d95e27e99c46a0d431dffd371203e57a4", "title": "This paper is included in the Proceedings of the 2023 USENIX Annual Technical Conference."}]}
