{"paperId": "bf5b14ee755f2926122cf394b7bf95519a29b56b", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Bridging the Information Gap Between Domain-Specific Model and General LLM for Personalized Recommendation", "abstract": "Generative large language models(LLMs) are proficient in solving general problems but often struggle to handle domain-specific tasks. This is because most of domain-specific tasks, such as personalized recommendation, rely on task-related information for optimal performance. Current methods attempt to supplement task-related information to LLMs by designing appropriate prompts or employing supervised fine-tuning techniques. Nevertheless, these methods encounter the certain issue that information such as community behavior pattern in RS domain is challenging to express in natural language, which limits the capability of LLMs to surpass state-of-the-art domain-specific models. On the other hand, domain-specific models for personalized recommendation which mainly rely on user interactions are susceptible to data sparsity due to their limited common knowledge capabilities. To address these issues, we proposes a method to bridge the information gap between the domain-specific models and the general large language models. Specifically, we propose an information sharing module which serves as an information storage mechanism and also acts as a bridge for collaborative training between the LLMs and domain-specific models. By doing so, we can improve the performance of LLM-based recommendation with the help of user behavior pattern information mined by domain-specific models. On the other hand, the recommendation performance of domain-specific models can also be improved with the help of common knowledge learned by LLMs. Experimental results on three real-world datasets have demonstrated the effectiveness of the proposed method.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-07", "journal": {"name": "ArXiv", "volume": "abs/2311.03778"}, "authors": [{"authorId": "2265620642", "name": "Wenxuan Zhang"}, {"authorId": "2109503105", "name": "Hongzhi Liu"}, {"authorId": "51123128", "name": "Yingpeng Du"}, {"authorId": "144469723", "name": "Chen Zhu"}, {"authorId": "2265597522", "name": "Yang Song"}, {"authorId": "2283086825", "name": "Hengshu Zhu"}, {"authorId": "2260608759", "name": "Zhonghai Wu"}], "citations": [{"paperId": "6ced16d331e20b7d1b4723501835dbf647db53cf", "title": "Knowledge sharing in manufacturing using LLM-powered tools: user study and model benchmarking"}, {"paperId": "c450bce3f08f2db4f2e7fabbe1260ff119ca43d3", "title": "CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation"}, {"paperId": "14c7747267c875713be896cc6a3d5bbc16ccb015", "title": "Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review"}, {"paperId": "156af81de93f840df39c0973ef1343629427a7db", "title": "Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis"}, {"paperId": "9426ed538470ad98b881a20fd9725bf8536a674f", "title": "FLIP: Towards Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction"}, {"paperId": "bac54736112098616f0e1c90435888ef3e119d32", "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey"}]}
