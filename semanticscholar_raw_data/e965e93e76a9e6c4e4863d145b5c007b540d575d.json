{"paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization", "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.", "venue": "arXiv.org", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-12-22", "journal": {"name": "ArXiv", "volume": "abs/2212.12017"}, "authors": [{"authorId": "2053620465", "name": "S. Iyer"}, {"authorId": "143724481", "name": "Xi Victoria Lin"}, {"authorId": "10721120", "name": "Ramakanth Pasunuru"}, {"authorId": "39980906", "name": "Todor Mihaylov"}, {"authorId": "2082239112", "name": "Daniel Simig"}, {"authorId": "2162840444", "name": "Ping Yu"}, {"authorId": "35752280", "name": "Kurt Shuster"}, {"authorId": "2118914337", "name": "Tianlu Wang"}, {"authorId": "2154975456", "name": "Qing Liu"}, {"authorId": "2146367061", "name": "Punit Singh Koura"}, {"authorId": "2116235416", "name": "Xian Li"}, {"authorId": "2146367747", "name": "Brian O'Horo"}, {"authorId": "2064737506", "name": "Gabriel Pereyra"}, {"authorId": "2155451431", "name": "Jeff Wang"}, {"authorId": "2065332326", "name": "Christopher Dewan"}, {"authorId": "1709797", "name": "Asli Celikyilmaz"}, {"authorId": "2137813791", "name": "Luke Zettlemoyer"}, {"authorId": "1759422", "name": "Veselin Stoyanov"}], "citations": [{"paperId": "b34111fe8e230d371dd9446d52388a84b5592390", "title": "Facial Affective Behavior Analysis with Instruction Tuning"}, {"paperId": "a5fd373a00d454132687d4633124f4a3adfa4190", "title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models"}, {"paperId": "917aa8165c0ab778ea3aea7a2b286c74cd2889f0", "title": "SMART: Submodular Data Mixture Strategy for Instruction Tuning"}, {"paperId": "6c6f63643eb97a8c309a881e4ba21461454478fe", "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models"}, {"paperId": "99ed8dd210fb84de7417424adf8f10ae463d2b99", "title": "Advancing Generative AI for Portuguese with Open Decoder Gerv\u00e1sio PT"}, {"paperId": "3bdd3d56ef9054aba47f83879b531a4842640295", "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning"}, {"paperId": "ed3057a99ddd416bf5dcaee83688ca7e43da678f", "title": "DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain"}, {"paperId": "2f8270afa120285b31df9779fbc8a6c5884bd059", "title": "Multi-dimensional Evaluation of Empathetic Dialog Responses"}, {"paperId": "a5805cca2c24c9125795c10daa87671cd5609972", "title": "Instruction Diversity Drives Generalization To Unseen Tasks"}, {"paperId": "0f6cd53c0cc1ee252433e0d37f419754e053b8a6", "title": "Suppressing Pink Elephants with Direct Principle Feedback"}, {"paperId": "a1f76db91c0debcf93ae9889736bce8470902113", "title": "Large Language Models: A Survey"}, {"paperId": "2565b1ae47fb8e47017bee18ef1c602d6e1f4e44", "title": "Rethinking Data Selection for Supervised Fine-Tuning"}, {"paperId": "9b86d92b01d923dbf386aeeb80ad4d79e530379e", "title": "MULTI: Multimodal Understanding Leaderboard with Text and Images"}, {"paperId": "69e6dd39bf13d290fb9d885da90cc037f0dd2975", "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities"}, {"paperId": "b7ad6a754911dc96190469a56962ddde3608e67a", "title": "SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity"}, {"paperId": "5b6622e03e20ae1acbcd9390f98d7739c7fc6517", "title": "VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models"}, {"paperId": "971a71c764ba85e65488f73f9a7a485912aca065", "title": "Airavata: Introducing Hindi Instruction-tuned LLM"}, {"paperId": "b513c87b436751f3a33ec8e5442532c2f203ecbf", "title": "InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions"}, {"paperId": "a050c9b0c321839e4427ab9defa3463be7825ac4", "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models"}, {"paperId": "140cfda71bfff852c3e205b7ad61854b78c76982", "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs"}, {"paperId": "221a0e0c798d4bbc8a057d869b7251e03f1b0790", "title": "ChatQA: Building GPT-4 Level Conversational QA Models"}, {"paperId": "6514abaf6bdd1c31ac5dc4ad40760ff8422c6a4e", "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation"}, {"paperId": "62ac45c894b38a9ec6ca089d1bea292281089d04", "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection"}, {"paperId": "fed283026f806f11d200d9f0dfee820af0bf903a", "title": "Prompting open-source and commercial language models for grammatical error correction of English learner text"}, {"paperId": "1c0a0ec50a639efe9569d0c57f73c8e1b47acbcd", "title": "Multi-Candidate Speculative Decoding"}, {"paperId": "1edc9a2b5210e23d5ceccc5cc1b5848dbb4ceab4", "title": "InFoBench: Evaluating Instruction Following Ability in Large Language Models"}, {"paperId": "5272acad9e4201e93dabe3fd99bd7ead9b1a544d", "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models"}, {"paperId": "b1b29e274aa116826f8e82177415f40743ba0392", "title": "COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training"}, {"paperId": "17a32c825bd746a2625eddc2728092171a9ef72a", "title": "Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model"}, {"paperId": "7a31971b0af439dec6fc484cca20df57f440b644", "title": "One Shot Learning as Instruction Data Prospector for Large Language Models"}, {"paperId": "117a9f9227ebe4acc67ff8a52eb9db2ab16390c8", "title": "Data Management For Large Language Models: A Survey"}, {"paperId": "c95c4fb96868d6512c32988632a7b101a42c455d", "title": "Dolphins: Multimodal Language Model for Driving"}, {"paperId": "fc53f8f3a84f1fc4993689d8f98cf6551d07a22d", "title": "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning"}, {"paperId": "e3f7ad05b1652c6ada78cffbe405bceb723bc70c", "title": "MoDS: Model-oriented Data Selection for Instruction Tuning"}, {"paperId": "e36584cf5c53d19a2d2b888ee05cc2f7afd52693", "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation"}, {"paperId": "9c48db369f4811e6566ff3bc7d71be159a544fe2", "title": "Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper"}, {"paperId": "488c486fef0d58259c46d7be42b285c1de118acb", "title": "Long-MIL: Scaling Long Contextual Multiple Instance Learning for Histopathology Whole Slide Image Analysis"}, {"paperId": "02e560ddd5656703819f36b6f02606fee923e3bb", "title": "Is \"A Helpful Assistant\" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts"}, {"paperId": "248c9663001cddba588709ac5fb67f2a549c01a0", "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks"}, {"paperId": "12868220782ded0a5eeb31b78ce527dce2d37332", "title": "Disinformation Capabilities of Large Language Models"}, {"paperId": "cbecf91df39ac5837e4c8bb021fcc1292beb24de", "title": "PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models"}, {"paperId": "ef72f4aae225529393264c2b2594e876bec3fd04", "title": "Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains"}, {"paperId": "3d13935886627982fc98971baa33d2f9f3115bff", "title": "Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer"}, {"paperId": "7b32eeb169e674a851156002035cb51abf79f8d3", "title": "Simple and Effective Input Reformulations for Translation"}, {"paperId": "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8", "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"}, {"paperId": "acee88189ec7a4abc4b9446a5e0c98168d72ad68", "title": "Efficiently Adapting Pretrained Language Models To New Languages"}, {"paperId": "79729ed54ad03fd403d88f8d1543b49c7a58b973", "title": "The language of prompting: What linguistic properties make a prompt successful?"}, {"paperId": "7c9452ea7d9f19b7f026c68169ced75995ef7268", "title": "ChatGPT-Powered Hierarchical Comparisons for Image Classification"}, {"paperId": "f8f9b9e5a1b9dd67fe61bd358f1d90f6cceeb4a1", "title": "Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace"}, {"paperId": "fb285de78ba95e30ad41ed97ddf014763e6279ad", "title": "TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise"}, {"paperId": "2d0117c1852688cdd35fbd818968a8657732642a", "title": "ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models"}, {"paperId": "cd2f4aaf98bb1e020cff310000c8049d3460c54e", "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark"}, {"paperId": "a8382f50cffe0c41f0929bf8ff1a83f581c44bac", "title": "Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering"}, {"paperId": "69ecf88a0d9752db7dc32b4917ee24b4974cea18", "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges"}, {"paperId": "9bee647cd2999fcd2b68b964bbe128a471d3707c", "title": "Generative Language Models Exhibit Social Identity Biases"}, {"paperId": "b37690747ba314455b8a992e794b25223e105d1c", "title": "BLESS: Benchmarking Large Language Models on Sentence Simplification"}, {"paperId": "ed5d6c4ee7161f951f09c4bd16046010c4acb47e", "title": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism"}, {"paperId": "878b51f52f53b9b209e9270e75d94bac93142d40", "title": "SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for Social Media NLP Research"}, {"paperId": "2a33e4c93002ab97a99577ac89837be4d448725e", "title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks"}, {"paperId": "a267cfbd6930c8e2c720104ae4a90e39461a6694", "title": "The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models"}, {"paperId": "ff4f3ac67c6d7d7da3d0f27ddafc0f9552a29b00", "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation"}, {"paperId": "34ca51ce10e8d1d6c950ba519329714a0184d004", "title": "KwaiYiiMath: Technical Report"}, {"paperId": "60b9ffd8935d46e0d43e143186124cecfa6e5118", "title": "LangNav: Language as a Perceptual Representation for Navigation"}, {"paperId": "282c568302701bc163d454702eae10e43ca784a3", "title": "The future landscape of large language models in medicine"}, {"paperId": "c36d7bc6adeee7d16df1194c44cb66c48c9ae681", "title": "Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models"}, {"paperId": "99e6c90c0ed18030164a731250d547b5e5735055", "title": "LLM for SoC Security: A Paradigm Shift"}, {"paperId": "957b601e2beefeaaf2e069e0130054051c8a7782", "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning"}, {"paperId": "368fb35a07076eba01c2e4700499323cd4524513", "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning"}, {"paperId": "cd4692ae5a13931a92798df824116263818208bb", "title": "RelBERT: Embedding Relations with Language Models"}, {"paperId": "15b0fadb0c703c915ad2caa3b5338415eb1ec0d2", "title": "Unbiased Watermark for Large Language Models"}, {"paperId": "1ecf2955a2f4f2039f36b0334e2c376a5c901d6c", "title": "LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models"}, {"paperId": "8d17234680db76f99efd22fbcb169f45d2d79d93", "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers"}, {"paperId": "e0e106fc72458cd46b42d6341f809b4474b6226b", "title": "Everyone Deserves A Reward: Learning Customized Human Preferences"}, {"paperId": "1a735015a1f7ef4f2ba2273ce5fcaaacfa9d1ea2", "title": "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning"}, {"paperId": "0b60ba1142fcb6b361b4695accbb772a08da8d14", "title": "Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis"}, {"paperId": "dc688bda050d568a3771d2c2d11d5e1fd8912560", "title": "MM-AU:Towards Multimodal Understanding of Advertisement Videos"}, {"paperId": "894ed1aba8e42a4ec27ba53ecde383b14c5128ca", "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"}, {"paperId": "e47d276bad18f441950c8136672ae6864e95323f", "title": "VIGC: Visual Instruction Generation and Correction"}, {"paperId": "1ce1738d7f224ebd7ad98e23692404f06697b5f4", "title": "Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models"}, {"paperId": "11cf88dce827bd67cbfa60400306318022e736d5", "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification"}, {"paperId": "f8b90d640158f61c4553518a8554a73b540e07e7", "title": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models"}, {"paperId": "8cf37154fc183d16e4c17c86309855248662b709", "title": "SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding"}, {"paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f", "title": "Instruction Tuning for Large Language Models: A Survey"}, {"paperId": "eb5cf10406a8ad31e0ebe56b36571d5db4758a62", "title": "PUMGPT: A Large Vision-Language Model for Product Understanding"}, {"paperId": "7ac38c3398f2696754bec69f296468e7a8237a64", "title": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"}, {"paperId": "64e802ea8e9dbe247c31fb06184c04dbf9e55e4e", "title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce"}, {"paperId": "40e0b9361d88b1879891eb6d16de110b30bf6c62", "title": "OctoPack: Instruction Tuning Code Large Language Models"}, {"paperId": "c74a13b251b6af6dfce49eeb128b1c0e2ddf955d", "title": "A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment"}, {"paperId": "cf4178eeabf1618bf75081a0a341fade91f3444a", "title": "Learning to Paraphrase Sentences to Different Complexity Levels"}, {"paperId": "76513f54fcecf7a380f77ad785f05c3bc869db4a", "title": "Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering"}, {"paperId": "46ac88bb0acbf736840ff8a392cec2bf43d917e1", "title": "Exploring Format Consistency for Instruction Tuning"}, {"paperId": "12acdfc7e32e9d603dc108008bb15e65439e7c79", "title": "Evaluating the Moral Beliefs Encoded in LLMs"}, {"paperId": "e06595ebb2fe4d73fe42e566b57d7a109df75615", "title": "Instruction-following Evaluation through Verbalizer Manipulation"}, {"paperId": "b34862afacf36e7011d40c67bb67c5ee9cf7da22", "title": "DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI"}, {"paperId": "5b0fe50dc6df8f4eba13f8177dcd4bbe5a2b0e23", "title": "A Survey of Techniques for Optimizing Transformer Inference"}, {"paperId": "725a467a9b3dd909685252d5170e7f2187781414", "title": "DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering"}, {"paperId": "ca31b8584b6c022ef15ddfe994fe361e002b7729", "title": "A Comprehensive Overview of Large Language Models"}, {"paperId": "ebddfdc5d845a788e8062eddbbf7a335737cb99b", "title": "What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"}, {"paperId": "7858a2994c740765037602c8fbaf628c8e9d9540", "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback"}, {"paperId": "da08e9f21ef361e0e1242f8849a18a4ea1a3d27e", "title": "SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions"}, {"paperId": "ebedc4d7a2356090904baba4104ef0832bc236df", "title": "A Survey on Multimodal Large Language Models"}, {"paperId": "473eb062612a17c965eaa62136322f0dec6b1f8e", "title": "Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow"}, {"paperId": "fd755dc7b5b206c17fd953db04e1c888d45b6e4e", "title": "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"}, {"paperId": "db1aa71314016e12e115fbe449a688f523e52e77", "title": "CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models"}, {"paperId": "e47e63781c0e7a2c0504b9381b76b5d01b62c53d", "title": "InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large Language Models"}, {"paperId": "fbd2c8089870814449f9254a711041bbae145a82", "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources"}, {"paperId": "d6fb1c21a46fb8b0f3f4383fd467b21e5b58c55f", "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models"}, {"paperId": "2286562f2b185ce476a23f218f2de83b0561fbe9", "title": "bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark"}, {"paperId": "aa828072e36be23887eeb3ac277901d8f893ef53", "title": "Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering"}, {"paperId": "b458fc5261595f44b36325e5eaea1f874d65138f", "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"}, {"paperId": "4487bdcf1eb42bdec83709ba0df5b32dcf388976", "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization"}, {"paperId": "9e8af0791e8c87452c8cff25dab5448a29c218d4", "title": "ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback"}, {"paperId": "ebf3a59aacdd9982283d7f41229ee2a93800d6ef", "title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks"}, {"paperId": "24811cadf16519910f643b6084107164e6ca4219", "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In"}, {"paperId": "f0888b9c0ef63e68c7758e6aec2370961c0eede9", "title": "On the Tool Manipulation Capability of Open-source Large Language Models"}, {"paperId": "a396176a31194976d4676c3f830209c129bac57c", "title": "A RelEntLess Benchmark for Modelling Graded Relations between Named Entities"}, {"paperId": "7d8905a1fd288068f12c8347caeabefd36d0dd6c", "title": "Gorilla: Large Language Model Connected with Massive APIs"}, {"paperId": "dbfd154190667087ed1cd6c7f75a81858c2f397e", "title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models"}, {"paperId": "76750c59ec126cc4bfdfef30648598bd5b94220b", "title": "Can Large Language Models Capture Dissenting Human Voices?"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164", "title": "Evaluating Factual Consistency of Summaries with Large Language Models"}, {"paperId": "0744580e75a74357e466a57082c85cb42f548aa9", "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning"}, {"paperId": "c218cd1772999517b137bbbc9872c4f67e540b7f", "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models"}, {"paperId": "5692501c10d0c1762842f92c66fcf0bffe2c0342", "title": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models"}, {"paperId": "e01c2a01d54365c8833b816de8ef39a752ffa9b0", "title": "Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors"}, {"paperId": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"}, {"paperId": "9c827a18a2a865e68c848b6823aec1768f9f9300", "title": "CoEdIT: Text Editing by Task-Specific Instruction Tuning"}, {"paperId": "54a8b153ed04a872da878d695239bdc413dc782c", "title": "InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"}, {"paperId": "390a17b29e7b68873eddce92a556df8d13b29f94", "title": "The Current State of Summarization"}, {"paperId": "3487c12512fa41d3a4d64f00cb842525a8590ad3", "title": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation"}, {"paperId": "04ee9597be4d6d2457214334e495e591000b5542", "title": "PMC-LLaMA: Towards Building Open-source Language Models for Medicine"}, {"paperId": "31efded912389bcff148c3205b42669ad434f7d9", "title": "An Evaluation on Large Language Model Outputs: Discourse and Memorization"}, {"paperId": "c01e43c65a04d766e429863bdf7cf65b895df20e", "title": "Chinese Open Instruction Generalist: A Preliminary Release"}, {"paperId": "352420ee61a8da783ca7750170793613b18b8d9c", "title": "Tool Learning with Foundation Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "6e4635f8632f34e934a055264bd3b1ce44595f9b", "title": "ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human"}, {"paperId": "dce4e4cf1cd8c42b8a400280a48283234ad7aafb", "title": "Are Large Language Models All You Need for Task-Oriented Dialogue?"}, {"paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea", "title": "Instruction Tuning with GPT-4"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "12d16f426edc6ab248fb476007bd1646282d4d68", "title": "Language Model Behavior: A Comprehensive Survey"}, {"paperId": "807f4a368e845f7649e645f91d1d4f2ce72beee3", "title": "A Comprehensive Survey on Instruction Following"}, {"paperId": "638b08154fbb71fd34db2aae6cb40045577fe0de", "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "20f7e8e9f64ce170fe3fa81ca1b32ff4826698ae", "title": "Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "b2cb1270421a9f3a82040244afb2041e7c4f20df", "title": "Hulk: Graph Neural Networks for Optimizing Regionally Distributed Computing Systems"}, {"paperId": "681cee58cf7e54199191cf9e0baf6851d8356704", "title": "Complex QA and language models hybrid architectures, Survey"}, {"paperId": "2029349c55c1dba3493c5b3bd25152f18ba21ae2", "title": "Augmented Language Models: a Survey"}, {"paperId": "0cf694b8f85ab2e11d45595de211a15cfbadcd22", "title": "Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks"}, {"paperId": "b2446e70637e0ef17a41c0bdab7f8baa4cdf7817", "title": "EvoText: Enhancing Natural Language Generation Models via Self-Escalation Learning for Up-to-Date Knowledge and Improved Performance"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "fbd49b25bdab98c171af49962a41139c73dacbde", "title": "Specializing Smaller Language Models towards Multi-Step Reasoning"}, {"paperId": "498f055c8a6c2231083e507a5c4f10f5dc1e0c60", "title": "Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes"}, {"paperId": "30c0cdc414f68211d5d0514df027cec22e005174", "title": "A Survey on In-context Learning"}, {"paperId": "8bf77f3f14d20b36a0a4b96693e0a6480f17aac1", "title": "HINT: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation"}, {"paperId": "89744cbaa080c82785b1cb8d54710bbbca32f8ed", "title": "Data Curation Alone Can Stabilize In-context Learning"}, {"paperId": "95c11cc5820ba32c60d5f2671f6567b9914a4978", "title": "ALERT: Adapt Language Models to Reasoning Tasks"}, {"paperId": "8fd462f6248d5e3f1b6602697c09489086b5655f", "title": "Distilling Reasoning Capabilities into Smaller Language Models"}, {"paperId": "bf72cfa87a5c55e430abf6d2a3d9b66eb9e1a717", "title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity"}, {"paperId": "4c868a92c615df3859433e28b2441bbce9e65fb3", "title": "Reward Reports for Reinforcement Learning"}, {"paperId": "4762b614958013039ed14c59367eca05165b0c21", "title": "INSTRUCTION-FINETUNED FOUNDATION MODELS FOR MULTIMODAL WEB NAVIGATION"}, {"paperId": "8aa98fbfb6f1e979dead13ce24075503fe47658e", "title": "A Survey for In-context Learning"}, {"paperId": "fb81264ac2fde6fecbdaaa2b96b9fa9e35bb2e08", "title": "In-context Learning of Large Language Models for Controlled Dialogue Summarization: A Holistic Benchmark and Empirical Analysis"}, {"paperId": "adc19c97fd19d67948db30614898ae7a8be3739c", "title": "Auto-Learning: An Adversarial Process of Two Pre-trained Models for Natural Language Generation"}, {"paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"}, {"paperId": "7ca954844bc1dd405bc43445b1c990e42d865095", "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society"}, {"paperId": "520711a1e93e6c4221f2a7c97c27a508379e8e37", "title": "Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts"}, {"paperId": "c148d80a82308d04e45ed480aaed1ff8fed0d5bc", "title": "Semantic-Augment: Augmenting the Semantic Space of Transformers Improves Generalization"}, {"paperId": "70bbed0c658df1f7f9e1516c3e94281e64b3ee56", "title": "Saama AI Research at SemEval-2023 Task 7: Exploring the Capabilities of Flan-T5 for Multi-evidence Natural Language Inference in Clinical Trial Data"}, {"paperId": "ac4ffaab10f6b6ad83e79ca5691f338abf5cff82", "title": "Instruction Mining: High-Quality Instruction Data Selection for Large Language Models"}, {"paperId": "bb07ac7a94ab5cefc6d40df46fe20b71382ef09d", "title": "Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art"}, {"paperId": "deb6dd39a68ea0d949f580a7944abf595185e150", "title": "Are LLMs All You Need for Task-Oriented Dialogue?"}, {"paperId": "734587af2a0d791d37a2fade87c8cb26e57ffd6f", "title": "LIS@DEFT\u201923 : les LLMs peuvent-ils r\u00e9pondre \u00e0 des QCM ? (a) oui; (b) non; (c) je ne sais pas."}, {"paperId": "b6511d2cad195b4e595737f080031647296136f6", "title": "A Comparative Study of Prompting Strategies for Legal Text Classification"}, {"paperId": "b848cd21b192041bde6de461e0a92e3065c9298c", "title": "Evaluating Large Language Models\u2019 Understanding of Financial Terminology via Definition Modeling"}, {"paperId": "24de1048791bac4972ecc16d1c3c1de23691407d", "title": "Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"}]}
