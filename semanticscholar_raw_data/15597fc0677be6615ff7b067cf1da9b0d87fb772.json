{"paperId": "15597fc0677be6615ff7b067cf1da9b0d87fb772", "publicationVenue": {"id": "406d9f60-417a-4dc5-a6b7-1fe4689a4ff7", "name": "IEEE International Conference on Cloud Computing", "type": "conference", "alternate_names": ["Int Conf Cloud Comput [services Soc", "CLOUD", "International Conference on Cloud Computing [Services Society]", "IEEE Int Conf Cloud Comput"]}, "title": "TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep Learning Inference in Function-as-a-Service", "abstract": "Deep neural networks (DNNs) have become core computation components within low latency Function as a Service (FaaS) prediction pipelines. Cloud computing, as the defacto backbone of modern computing infrastructure, has to be able to handle user-defined FaaS pipelines containing diverse DNN inference workloads while maintaining isolation and latency guarantees with minimal resource waste. The current solution for guaranteeing isolation and latency within FaaS is inefficient. A major cause of the inefficiency is the need to move large amount of data within and across servers. We propose TrIMS as a novel solution to address this issue. TrIMSis a generic memory sharing technique that enables constant data to be shared across processes or containers while still maintaining isolation between users. TrIMS consists of a persistent model store across the GPU, CPU, local storage, and cloud storage hierarchy, an efficient resource management layer that provides isolation, and a succinct set of abstracts, applicationAPIs, and container technologies for easy and transparent integration with FaaS, Deep Learning (DL) frameworks, and user code. We demonstrate our solution by interfacing TrIMS with the Apache MXNet framework and demonstrate up to 24x speedup in latency for image classification models, up to 210x speedup for large models, and up to8\u00d7system throughput improvement.", "venue": "IEEE International Conference on Cloud Computing", "year": 2018, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-11-24", "journal": {"name": "2019 IEEE 12th International Conference on Cloud Computing (CLOUD)", "pages": "372-382"}, "authors": [{"authorId": "2333308", "name": "Abdul Dakkak"}, {"authorId": "2133092186", "name": "Cheng Li"}, {"authorId": "2192182", "name": "Simon Garcia De Gonzalo"}, {"authorId": "46590281", "name": "Jinjun Xiong"}, {"authorId": "143668320", "name": "Wen-mei W. Hwu"}], "citations": [{"paperId": "e657bc68767368dd6c7131055aaf615d777305bf", "title": "A Survey of Serverless Machine Learning Model Inference"}, {"paperId": "aded5f10c600756cb6a853bf99e8d4dbf1401397", "title": "Fine-grained accelerator partitioning for Machine Learning and Scientific Computing in Function as a Service Platform"}, {"paperId": "5d97c5d83b84cd343badbfedc9362c6d184fd9da", "title": "Unveiling the frontiers of deep learning: innovations shaping diverse domains"}, {"paperId": "218c184ccf68682feef8c9a386d8cb053c66c2cf", "title": "Reducing the Cost of GPU Cold Starts in Serverless Deep Learning Inference Serving"}, {"paperId": "c6bdb833f6d2fa6e1f728dc01fbd4a6c6e341f8d", "title": "GPU-enabled Function-as-a-Service for Machine Learning Inference"}, {"paperId": "ddabec1bf278b2476ff8fb464f68c599a64d0307", "title": "Inferencing on Edge Devices: A Time- and Space-aware Co-scheduling Approach"}, {"paperId": "e5617de21e517ac5b6dcb1b041710dbd2c755ffc", "title": "Rise of the Planet of Serverless Computing: A Systematic Review"}, {"paperId": "745093391442971373d2af7549b00651a4636808", "title": "Deep Learning Workload Scheduling in GPU Datacenters: Taxonomy, Challenges and Vision"}, {"paperId": "d2807a1113b77fc03014cf2814cc9dc95839398c", "title": "Don't Let RPCs Constrain Your API"}, {"paperId": "9da092d7c7674e96830f8d6713a9a4f8101f984c", "title": "Trustworthy artificial intelligence"}, {"paperId": "9577150b53e55f8aa998d5001585d6b902519135", "title": "PERSEUS: Characterizing Performance and Cost of Multi-Tenant Serving for CNN Models"}, {"paperId": "98bef71b1f3e6322aba06aa7cce549cba8551366", "title": "The Design and Implementation of a Scalable Deep Learning Benchmarking Platform"}, {"paperId": "674fee9c9cbf0982f4697c0f75552e9ddd2c6b77", "title": "INFaaS: A Model-less Inference Serving System."}, {"paperId": "b57a15430b5bfe5c6757e4098311d12f778b1f84", "title": "A Case for Managed and Model-less Inference Serving"}, {"paperId": "883b5a6cbbf3c499c8402204a657abf1e836d310", "title": "Deep Learning Workload Scheduling in GPU Datacenters: A Survey"}, {"paperId": "3aa0fe9d05af104ec56dd0edc09d716919f7275a", "title": "A Literature Review on Serverless Computing"}, {"paperId": "31582f9e4327b9ea195717b022560d776d122faf", "title": "Serverless on Machine Learning: A Systematic Mapping Study"}, {"paperId": "6722b39592f1d993f5dc029b865591a5742dd744", "title": "Impact of Artificial Intelligence-enabled Software-defined Networks in Infrastructure and Operations: Trends and Challenges"}]}
