{"paperId": "2ef4ab54d00203f9ac610213ac3abc8e1fe541b4", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling", "abstract": "Over the last several years, end-to-end neural conversational agents have vastly improved in their ability to carry a chit-chat conversation with humans. However, these models are often trained on large datasets from the internet, and as a result, may learn undesirable behaviors from this data, such as toxic or otherwise harmful language. Researchers must thus wrestle with the issue of how and when to release these models. In this paper, we survey the problem landscape for safety for end-to-end conversational AI and discuss recent and related work. We highlight tensions between values, potential positive impact and potential harms, and provide a framework for making decisions about whether and how to release these models, following the tenets of value-sensitive design. We additionally provide a suite of tools to enable researchers to make better-informed decisions about training and releasing end-to-end conversational AI models.", "venue": "arXiv.org", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-07-07", "journal": {"name": "ArXiv", "volume": "abs/2107.03451"}, "authors": [{"authorId": "31461304", "name": "Emily Dinan"}, {"authorId": "17038002", "name": "Gavin Abercrombie"}, {"authorId": "2055868822", "name": "A. S. Bergman"}, {"authorId": "3416737", "name": "Shannon L. Spruit"}, {"authorId": "2022288", "name": "Dirk Hovy"}, {"authorId": "90841478", "name": "Y-Lan Boureau"}, {"authorId": "1681799", "name": "Verena Rieser"}], "citations": [{"paperId": "e6044533d10f595b1b095de48248fc0dc6644a89", "title": "Large language models as decision aids in neuro-oncology: a review of shared decision-making applications"}, {"paperId": "8ba57771dd6345821a0cbe83c4c7eb50f66b7b65", "title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks"}, {"paperId": "3d90d1d429fa9dacb50a103cdab5c16328665c2c", "title": "Prompt Stealing Attacks Against Large Language Models"}, {"paperId": "191df526eb5373215e2767ca0df5b4ec7d8db844", "title": "Mapping the Ethics of Generative AI: A Comprehensive Scoping Review"}, {"paperId": "e3fbbd46fd88e0c2487861a936f0b205e43d3ee1", "title": "Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data"}, {"paperId": "8b28792f8405b737229afb92c99c579b86d8aa98", "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations"}, {"paperId": "4ec1b2273af190811644233ff20095524a8de66c", "title": "Comprehensive Assessment of Toxicity in ChatGPT"}, {"paperId": "6f542f60a5d4f540d056bb49d47f89f1035ad0cf", "title": "A review of the explainability and safety of conversational agents for mental health to identify avenues for improvement"}, {"paperId": "c4b0f912cf6d9789e63e656b317808d255fa976a", "title": "DialCL-Bias: A Semantic and Contextual Framework to Identify Social Bias in Dialogue"}, {"paperId": "e33ff3cf8e209762ca27ca320cca248198194833", "title": "Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models"}, {"paperId": "1c9566ea496c2407877452ef0ea2606da44c8925", "title": "Enhancing Pipeline-Based Conversational Agents with Large Language Models"}, {"paperId": "c608fb120ff2de5b2ed25b02731ec092882d7cf8", "title": "Rethinking Machine Ethics - Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?"}, {"paperId": "386e634c381de8700d81d5b739462d0050484b28", "title": "Challenges of GPT-3-Based Conversational Agents for Healthcare"}, {"paperId": "df4d785dcdc0e8736fc46277fa03ed2eac3d63f6", "title": "Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "7e70b9ac85ff2b27fb2c1c67ea52c39552812dc4", "title": "CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care"}, {"paperId": "e11111dfda2a1f7aa9ecb8720032739233fb72f4", "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models"}, {"paperId": "6e30a511242cd48a1394d87ce8d2b682978014a0", "title": "DICES Dataset: Diversity in Conversational AI Evaluation for Safety"}, {"paperId": "a80d106b4536884af8da68078babc70086b1a607", "title": "Evaluating the Social Impact of Generative AI Systems in Systems and Society"}, {"paperId": "0b6edce3dde7e502c6b7c6d83bac0230ec912482", "title": "Improving Open Language Models by Learning from Organic Interactions"}, {"paperId": "066a9da13badca3791832f50f47103f31d681189", "title": "On \u201cScientific Debt\u201d in NLP: A Case for More Rigour in Language Model Pre-Training Research"}, {"paperId": "c398de8d4a18ec49b8f2eaaf3b0473186b99e1e1", "title": "Reimagining Retrieval Augmented Language Models for Answering Queries"}, {"paperId": "098c5080501af2d4918b8867c0ec27652dd195c1", "title": "From Human-Centered to Social-Centered Artificial Intelligence: Assessing ChatGPT's Impact through Disruptive Events"}, {"paperId": "eea0fc23005d4279968d8106009624b6ce74be14", "title": "ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification"}, {"paperId": "2709ca7273d83f7366ecc38154f6e5f5e1c901e1", "title": "Quality-agnostic Image Captioning to Safely Assist People with Vision Impairment"}, {"paperId": "3c24ac9fcc14c716208cc784a39bda847056c7f0", "title": "Towards Explainable and Safe Conversational Agents for Mental Health: A Survey"}, {"paperId": "02e80a115e98e6280c752ccd8820ddbac17db1f2", "title": "The Role of Large Language Models in the Recognition of Territorial Sovereignty: An Analysis of the Construction of Legitimacy"}, {"paperId": "7cfaec8004c6d9f4fb5cf10287d15513c35b0a63", "title": "Towards Safer Generative Language Models: A Survey on Safety Risks, Evaluations, and Improvements"}, {"paperId": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552", "title": "The Capacity for Moral Self-Correction in Large Language Models"}, {"paperId": "1d75f8de31bf47ec46fa5586056420ec8bc97e86", "title": "Using In-Context Learning to Improve Dialogue Safety"}, {"paperId": "e0a460d77dc29cea3cd5753e8e0bf57cfb812d6e", "title": "Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives"}, {"paperId": "6d1fd99a686f9be91355985e3149b3b5b8337a7a", "title": "Computer says \"No\": The Case Against Empathetic Conversational AI"}, {"paperId": "5784a10804c122d09349ff4e215cd12d8b72b6aa", "title": "MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions"}, {"paperId": "65723388e5efbfa2fa9431bb905e0e29e3851283", "title": "MULTI3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue"}, {"paperId": "8616da9215843353f2169916766054dfbd50a671", "title": "DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines"}, {"paperId": "3998502814e847e12aa4bf2ee2f911890b388bc5", "title": "AutoReply: Detecting Nonsense in Dialogue Introspectively with Discriminative Replies"}, {"paperId": "6615e9d1641e5b8141efa8e947a90d12b3158075", "title": "Cultural Incongruencies in Artificial Intelligence"}, {"paperId": "795736777f08e92a80c95dab7f205d1d7c28a10b", "title": "The CRINGE Loss: Learning what language not to model"}, {"paperId": "d17bddf9dc329bb9ff7883642699b84055db06fc", "title": "PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation"}, {"paperId": "e085bd71d3cc53a6eba3aa7e6940171a3a5d1086", "title": "Risk-graded Safety for Handling Medical Queries in Conversational AI"}, {"paperId": "40b672e817624a2357f279cd3d92aa0ae8ddd330", "title": "Data Feedback Loops: Model-driven Amplification of Dataset Biases"}, {"paperId": "089c3a879253d730e98eaed7b945fa6c4113ca1f", "title": "Why So Toxic?: Measuring and Triggering Toxic Behavior in Open-Domain Chatbots"}, {"paperId": "cd6652fe413d57d05b44e0f3aa036c54f0eef464", "title": "Towards Boosting the Open-Domain Chatbot with Human Feedback"}, {"paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7", "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"}, {"paperId": "a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd", "title": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage"}, {"paperId": "1bff5af76552f6d3c881ab051275d037f26f66be", "title": "Learning from data in the mixed adversarial non-adversarial case: Finding the helpers and ignoring the trolls"}, {"paperId": "914254fac74a2da051cccf6ca16afcaad416a079", "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"}, {"paperId": "1c25b49fb6a2789597320a9e3591a6e51f1ed6ed", "title": "Neural Generation Meets Real People: Building a Social, Informative Open-Domain Dialogue Agent"}, {"paperId": "7fbe8009a570fb92a75f199bbdfe936c12437b89", "title": "Why Robust Natural Language Understanding is a Challenge"}, {"paperId": "f2c17758e74707d379b87372528221656d14b697", "title": "Taxonomy of Risks posed by Language Models"}, {"paperId": "2cc999a2af940ffa2e194929e2f52fc6a9e4b7a8", "title": "Learning to Automate Follow-up Question Generation using Process Knowledge for Depression Triage on Reddit Posts"}, {"paperId": "e2c16d93bca8822b571dd388af4e309069d1a373", "title": "Target-Guided Dialogue Response Generation Using Commonsense and Data Augmentation"}, {"paperId": "ee5a743129e5785b92aff156a947ca8c6beabbbc", "title": "Robust Conversational Agents against Imperceptible Toxicity Triggers"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "6d23532a1e9a8116041fd5aac6b0ef8ddd6d8171", "title": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems"}, {"paperId": "4ae200e3e33045130f7abd1d38a82a8355dc6273", "title": "PANGUBOT: Efficient Generative Dialogue Pre-training from Pre-trained Language Model"}, {"paperId": "45c55fe92abc0ed4c2190fe039c9a23d70a0e33a", "title": "Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges"}, {"paperId": "876f3285465b6e9586a355c8acb8d96abe102e9a", "title": "Situating Search"}, {"paperId": "d795901e67de0c1eb512dc6323a5e8997e288aa7", "title": "Beyond Plain Toxic: Detection of Inappropriate Statements on Flammable Topics for the Russian Language"}, {"paperId": "de4635e95259118a545fdc0682407f416c16086a", "title": "AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation"}, {"paperId": "64acb0084c9dd8f454421ccc9289c1e389d7fcb8", "title": "Towards Identifying Social Bias in Dialog Systems: Framework, Dataset, and Benchmark"}, {"paperId": "a6f5b8f114b3eabbcd7f3f62091a481ca6f7f243", "title": "Predictability and Surprise in Large Generative Models"}, {"paperId": "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb", "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "3664cceee40f91c75d1ee2d2e17a050e846e52bb", "title": "COLD: A Benchmark for Chinese Offensive Language Detection"}, {"paperId": "8a6bda5739c9c975b49327b9fe891d908fdfa951", "title": "A Word on Machine Ethics: A Response to Jiang et al. (2021)"}, {"paperId": "35eb6755b63c45ff44d05ace8ea1b3b8c8db9eee", "title": "On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark"}, {"paperId": "30873c32db5a219a58be928d5692cce48be1d3a0", "title": "Few-Shot Bot: Prompt-Based Learning for Dialogue Systems"}, {"paperId": "0794333779d775abc2053052d1e7009066cbd4f1", "title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures"}, {"paperId": "93df98bf527e97f3876dfefb8b167a221b37e638", "title": "ConvAbuse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational AI"}, {"paperId": "4d2ed16ce4654132e6968fb979b0919fa11f2378", "title": "Refocusing on Relevance: Personalization in NLG"}, {"paperId": "917c63f2186119166b3379f5d2816bb1a2f39b09", "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling"}, {"paperId": "88064de690af282dbdf222774f03ff070b9df22b", "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation"}, {"paperId": "71e8ef02cdcb1248f5df7f55d404648624325970", "title": "\u201dI Like You, as a Friend\u201d: Voice Assistants\u2019 Response Strategies to Sexual Harassment and Their Relation to Gender"}, {"paperId": "6cb0efa435994a415bc090e5ab5a0ab5a6085897", "title": "Toward the Reliability of Dialog Systems"}, {"paperId": "c3aba0f3185989768b22a29f8f431d80a5710171", "title": "Chatbots & Dialogue Systems"}, {"paperId": "3607b92705cd1fba04eaf295f57be4b085791063", "title": "Dialogue Distillery: Crafting Interpolable, Interpretable, and Introspectable Dialogue from LLMs"}, {"paperId": "9754dc7a0444341db11d6fc46d21edbe233360b9", "title": "Evaluating How Users Game and Display Conversation with Human-Like Agents"}, {"paperId": "25eba922c7076e1a8a0a86fd0eed2bfcdbe0bbc3", "title": "AugESC: Large-scale Data Augmentation for Emotional Support Conversation with Pre-trained Language Models"}, {"paperId": "c7bc5a4b9a85e5c92ed3f480133a961fbc4e72e1", "title": "Language Model Accidents Waiting to Happen?"}, {"paperId": "eb1ac44bbc0fe07c5f31f459c7199211239e90b8", "title": "Open-domain Dialogue Generation: What We Can Do, Cannot Do, And Should Do Next"}, {"paperId": "fdbd1350e14171eaebd82eb2b11ee9141b0963c2", "title": "The AI Doctor Is In: A Survey of Task-Oriented Dialogue Systems for Healthcare Applications"}, {"paperId": "d3230937c11e7bf5ab10b91775dfc4c3339025a9", "title": "Unaware of Reality: Inconsistent Grounding in Conversational AI Anonymous"}, {"paperId": "395f6330e15a7209e9aec50dfe0c26df94bf2a06", "title": "Beyond Safety: Toward a Value-Sensitive Approach to the Design of AI Systems"}, {"paperId": "53f0a4b27ee6ff182cae970e45faac048dbefae4", "title": "MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Constructing Moral Discussions"}, {"paperId": "6626dadc76d1af9d19fc4c2a4fa3a4cf414e62e0", "title": "Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks"}, {"paperId": "378d987bb204dc230f5e20cbe7bc90acb21730d9", "title": "BiasAsker: Testing Social Biases in Dialog Systems"}]}
