{"paperId": "a99aba1d1a6657860417a312def18f3587c8113e", "publicationVenue": {"id": "d4610af5-85e0-480b-8773-5c71d92a7b99", "name": "International Conference on Architectural Support for Programming Languages and Operating Systems", "type": "conference", "alternate_names": ["ASPLOS", "Int Conf Archit Support Program Lang Oper Syst", "Archit Support Program Lang Oper Syst", "Architectural Support for Programming Languages and Operating Systems"], "url": "http://www.acm.org/sigplan/"}, "title": "Re-architecting I/O Caches for Emerging Fast Storage Devices", "abstract": "I/O caching has widely been used in enterprise storage systems to enhance the system performance with minimal cost. Using Solid-State Drives (SSDs) as an I/O caching layer on the top of arrays of Hard Disk Drives (HDDs) has been well studied in numerous studies. With emergence of ultra fast storage devices, recent studies suggest to use them as an I/O cache layer on top of mainstream SSDs in I/O intensive applications. Our detailed analysis shows despite significant potential of ultra-fast storage devices, existing I/O cache architectures may act as a major performance bottleneck in enterprise storage systems, which prevents to take advantage of the device full performance potentials. In this paper, using an enterprise-grade all-flash storage system, we first present a thorough analysis on the performance of I/O cache modules when ultra-fast memories are used as a caching layer on top of mainstream SSDs. Unlike traditional SSD-based caching on HDD arrays, we show the use of ultra-fast memory as an I/O cache device on SSD arrays exhibit completely unexpected performance behavior. As an example, we show two popular cache architectures exhibit similar throughput due to performance bottleneck on the traditional SSD/HDD devices, but with ultra-fast memory on SSD arrays, their true potential is released and show 5\u00d7 performance difference. We then propose an experimental evaluation framework to systematically examine the behavior of I/O cache modules on emerging ultra-fast devices. Our framework enables system architects to examine performance-critical design choices including multi-threading, locking granularity, promotion logic, cache line size, and flushing policy. We further offer several optimizations using the proposed framework, integrate the proposed optimizations, and evaluate them on real use-cases. The experiments on an industry-grade storage system show our I/O cache architecture optimally configured by the proposed framework provides up to 11\u00d7 higher throughput and up to 30\u00d7 improved tail latency over a non-optimal architecture.", "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2023-03-25", "journal": {"name": "Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3"}, "authors": [{"authorId": "50238381", "name": "Mohammadamin Ajdari"}, {"authorId": "2212360709", "name": "Pouria Peykani Sani"}, {"authorId": "1560627438", "name": "Amirhossein Moradi"}, {"authorId": "2212356989", "name": "Masoud Khanalizadeh Imani"}, {"authorId": "2212361432", "name": "Amir Hossein Bazkhanei"}, {"authorId": "144394335", "name": "H. Asadi"}], "citations": [{"paperId": "68f3029dc9ca6688a40fe6cea1a5bc6cbeae9466", "title": "FlashPage: A read cache for low-latency SSDs in web proxy servers"}, {"paperId": "049d459a6a4cd2e8ba5b895b25b744428d0fe925", "title": "WARM-tree: Making Quadtrees Write-efficient and Space-economic on Persistent Memories"}]}
