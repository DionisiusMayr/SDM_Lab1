{"paperId": "320ba22d10f72c3325ef279566ae2ecaef3b6935", "publicationVenue": null, "title": "Active Learning Policies for Solving Inverse Problems", "abstract": "In recent years, solving inverse problems for black-box simulators has become a point of focus for the machine learning community due to their ubiquity in science and engineering scenarios. In such settings, the simulator describes a forward process f : ( \u03c8 , x ) \u2192 y from simulator parameters \u03c8 and input data x to observations y , and the goal of the inverse problem is to optimise \u03c8 to minimise some observation loss. Simulator gradients are often unavailable or prohibitively expensive to obtain, making optimisation of these simulators particularly challenging. Moreover, in many applications, the goal is to solve a family of related inverse problems. Thus, starting optimisation ab-initio/from-scratch may be infeasible if the forward model is expensive to evaluate. In this paper, we propose a novel method for solving classes of similar inverse problems. We learn an active learning policy that guides the training of a surrogate and use the gradients of this surrogate to optimise the simulator parameters with gradient descent. After training the policy, downstream inverse problem optimisations require up to 90% fewer forward model evaluations than the baseline.", "venue": "", "year": null, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2275234276", "name": "Tim Bakker"}, {"authorId": "2275360143", "name": "Thomas Hehn"}, {"authorId": "5744388", "name": "F. V. Massoli"}], "citations": []}
