{"paperId": "be3dd37921a2b053653117d52973a833a797f0a2", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Hyperparameter Search Is All You Need For Training-Agnostic Backdoor Robustness", "abstract": "Commoditization and broad adoption of machine learning (ML) technologies expose users of these technologies to new security risks. Many models today are based on neural networks. Training and deploying these models for real-world applications involves complex hardware and software pipelines applied to training data from many sources. Models trained on untrusted data are vulnerable to poisoning attacks that introduce unwanted \u201cbackdoor\u201d functionality. Compromising a small fraction of the training data requires few resources from the attacker, but defending against these attacks is a challenge for ML developers. There have been dozens of defenses proposed in the research literature, most of them are expensive to integrate or incompatible with the existing model-training and deployment pipelines. The efficacy of each defense depends on the specifics of the dataset, ML task, model architecture, and training hyperparameters. Integrating and maintaining these defenses requires disruptive changes to the defender\u2019s environment. It is unrealistic to expect engineers in charge of model development and deployment to keep track of the research literature, evaluate every new defense, and repeatedly re-build their pipelines to integrate these defenses. In this paper, we take a pragmatic, developer-centric view and show how practitioners can answer two actionable questions: (1) how robust is my model to backdoor poisoning attacks?, and (2) how can I make it more robust without changing the training pipeline? We focus on the size of the compromised subset of the training data as a universal metric that applies to any poisoning attack and can be controlled by the defender. We use a primitive sub-task, which requires very few poisoned training inputs, to estimate this metric, thus providing a baseline on the model\u2019s robustness to poisoning. Next, we show how to leverage hyperparameter search\u2014a tool that ML developers already extensively use\u2014to balance the model\u2019s accuracy and robustness to poisoning, without any disruptive changes to the training and deployment pipeline. We demonstrate how to use our metric to estimate the robustness of models to backdoor attacks. We then design, implement, and evaluate a multi-stage hyperparameter search method we call Mithridates 1 that strengthens robustness by 3-5x with only a slight impact on the model\u2019s accuracy. We show that the hyperparameters found by our method increase robustness against multiple types of backdoor attacks. Finally, we discuss how to extend our method to AutoML and federated learning settings.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"name": "ArXiv", "volume": "abs/2302.04977"}, "authors": [{"authorId": "36103467", "name": "Eugene Bagdasaryan"}, {"authorId": "1723945", "name": "Vitaly Shmatikov"}], "citations": [{"paperId": "8e7dbcb0e3c78e21c9b963f0052d89417fa54993", "title": "Label Poisoning is All You Need"}]}
