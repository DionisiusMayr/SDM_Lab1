{"paperId": "795736777f08e92a80c95dab7f205d1d7c28a10b", "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "title": "The CRINGE Loss: Learning what language not to model", "abstract": "Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data \u2013 examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the \u201cCRINGE\u201d loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-11-10", "journal": {"pages": "8854-8874"}, "authors": [{"authorId": "46196592", "name": "Leonard Adolphs"}, {"authorId": "4800645", "name": "Tianyu Gao"}, {"authorId": "2155954521", "name": "Jing Xu"}, {"authorId": "35752280", "name": "Kurt Shuster"}, {"authorId": "2265067", "name": "Sainbayar Sukhbaatar"}, {"authorId": "145183709", "name": "J. Weston"}], "citations": [{"paperId": "0dfd1770d51e1f34e3c7423290d14da47c049641", "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"}, {"paperId": "553c86ba967f55db9caffe08240061b1282da893", "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection"}, {"paperId": "b1f243b586e87fe12ff8fe1a501f11ea5fc5ad44", "title": "On Prompt-Driven Safeguarding for Large Language Models"}, {"paperId": "04d64be16fb402f28348faffef484bd419c8bd8f", "title": "Self-Rewarding Language Models"}, {"paperId": "26b2adbe089ea36617c3ec0aa009319929da0550", "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity"}, {"paperId": "19df5eb2c74606414ed93633b4c61947cc42dbbb", "title": "Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss"}, {"paperId": "e5aed8e930b1efa1a5e0aad7ecf3038084cb0a33", "title": "Controlled Decoding from Language Models"}, {"paperId": "749d59f887c8ac83fd4f5178465e8b03e463358c", "title": "Large Language Model Alignment: A Survey"}, {"paperId": "7d8dd530278eb443409ba862bfc4e57c66c84640", "title": "SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices"}, {"paperId": "2dffb901382c6055a4d6bc9d07f4e9f6ae0e520e", "title": "Leveraging Implicit Feedback from Deployment Data in Dialogue"}, {"paperId": "d1fb9013ed41c67b18755f01f0b7c7c5ef0a047f", "title": "System-Level Natural Language Feedback"}, {"paperId": "0b6edce3dde7e502c6b7c6d83bac0230ec912482", "title": "Improving Open Language Models by Learning from Organic Interactions"}, {"paperId": "168d3bfab5cf0ed356c51eb6eaa18654d575a419", "title": "Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning"}, {"paperId": "672491163a327f80e08ce3ef4751e94c78631822", "title": "Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond"}, {"paperId": "82beb8a86d438e85a134182128d47607b1b04004", "title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models"}, {"paperId": "7e0b582f10af855c7a2e2a6320939bd7dd82ef62", "title": "Learn What NOT to Learn: Towards Generative Safety in Chatbots"}, {"paperId": "6db13f58ff662eefa823a660fa86faf8ddf75533", "title": "Controllable Text Generation with Language Constraints"}, {"paperId": "d40df59a1df8048a671bce60260b817e109a33f8", "title": "Reward Gaming in Conditional Text Generation"}, {"paperId": "72862b24372f77308ef4af62567bd6ee9f631f27", "title": "COUNT: COntrastive UNlikelihood Text Style Transfer for Text Detoxification"}]}
