{"paperId": "694d7381e95cc7ffa83448658ec7abd430906cda", "publicationVenue": {"id": "deedf64a-dd5c-4b33-b345-ff83bfb93d71", "name": "International Symposium on Computer Architecture", "type": "conference", "alternate_names": ["Int Symp Comput Archit", "ISCA"], "url": "http://www.cs.wisc.edu/~arch/www/"}, "title": "Optimizing CPU Performance for Recommendation Systems At-Scale", "abstract": "Deep Learning Recommendation Models (DLRMs) are very popular in personalized recommendation systems and are a major contributor to the data-center AI cycles. Due to the high computational and memory bandwidth needs of DLRMs, specifically the embedding stage in DLRM inferences, both CPUs and GPUs are used for hosting such workloads. This is primarily because of the heavy irregular memory accesses in the embedding stage of computation that leads to significant stalls in the CPU pipeline. As the model and parameter sizes keep increasing with newer recommendation models, the computational dominance of the embedding stage also grows, thereby, bringing into question the suitability of CPUs for inference. In this paper, we first quantify the cause of irregular accesses and their impact on caches and observe that off-chip memory access is the main contributor to high latency. Therefore, we exploit two well-known techniques: (1) Software prefetching, to hide the memory access latency suffered by the demand loads and (2) Overlapping computation and memory accesses, to reduce CPU stalls via hyperthreading to minimize the overall execution time. We evaluate our work on a single-core and 24-core configuration with the latest recommendation models and recently released production traces. Our integrated techniques speed up the inference by up to 1.59x, and on average by 1.4x.", "venue": "International Symposium on Computer Architecture", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2023-06-17", "journal": {"name": "Proceedings of the 50th Annual International Symposium on Computer Architecture"}, "authors": [{"authorId": "1746610", "name": "Dhruv Batra"}, {"authorId": "2220695947", "name": "Scott Cheng"}, {"authorId": "2220159622", "name": "Vishwas Kalagi"}, {"authorId": "2164897283", "name": "Vrushabh H. Sanghavi"}, {"authorId": "2220161324", "name": "Samvit Kaul"}, {"authorId": "33965807", "name": "Meenakshi Arunachalam"}, {"authorId": "10995410", "name": "Kiwan Maeng"}, {"authorId": "2111543", "name": "Adwait Jog"}, {"authorId": "1743609", "name": "A. Sivasubramaniam"}, {"authorId": "2065377015", "name": "M. Kandemir"}, {"authorId": "8948708", "name": "C. Das"}], "citations": [{"paperId": "f7155077dca27787e7033dc4f0f374b9484ac267", "title": "Providing scalable single\u2010operating\u2010system NUMA abstraction of physically discrete resources"}, {"paperId": "e74151c00c3d69875c75dcd01d4c6b31b92f7ae5", "title": "SySMOL: A Hardware-software Co-design Framework for Ultra-Low and Fine-Grained Mixed-Precision Neural Networks"}, {"paperId": "4c978bd290edb3797ccc5920b3b4f6c728f55640", "title": "Application-Aware Resource Allocation Based on Benefit\u2013Cost Ratio in Computing Power Network with Heterogeneous Computing Resources"}]}
