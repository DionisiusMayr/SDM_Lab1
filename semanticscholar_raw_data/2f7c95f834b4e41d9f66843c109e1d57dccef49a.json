{"paperId": "2f7c95f834b4e41d9f66843c109e1d57dccef49a", "publicationVenue": {"id": "d4610af5-85e0-480b-8773-5c71d92a7b99", "name": "International Conference on Architectural Support for Programming Languages and Operating Systems", "type": "conference", "alternate_names": ["ASPLOS", "Int Conf Archit Support Program Lang Oper Syst", "Archit Support Program Lang Oper Syst", "Architectural Support for Programming Languages and Operating Systems"], "url": "http://www.acm.org/sigplan/"}, "title": "STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining", "abstract": "Natural Language Processing (NLP) inference is seeing increasing adoption by mobile applications, where on-device inference is desirable for crucially preserving user data privacy and avoiding network roundtrips. Yet, the unprecedented size of an NLP model stresses both latency and memory, creating a tension between the two key resources of a mobile device. To meet a target latency, holding the whole model in memory launches execution as soon as possible but increases one app\u2019s memory footprints by several times, limiting its benefits to only a few inferences before being recycled by mobile memory management. On the other hand, loading the model from storage on demand incurs IO as long as a few seconds, far exceeding the delay range satisfying to a user; pipelining layerwise model loading and execution does not hide IO either, due to the high skewness between IO and computation delays. To this end, we propose Speedy Transformer Inference (STI). Built on the key idea of maximizing IO/compute resource utilization on the most important parts of a model, STI reconciles the latency v.s. memory tension via two novel techniques. First, model sharding. STI manages model parameters as independently tunable shards, and profiles their importance to accuracy. Second, elastic pipeline planning with a preload buffer. STI instantiates an IO/compute pipeline and uses a small buffer for preload shards to bootstrap execution without stalling at early stages; it judiciously selects, tunes, and assembles shards per their importance for resource-elastic execution, maximizing inference accuracy. Atop two commodity SoCs, we build STI and evaluate it against a wide range of NLP tasks, under a practical range of target latencies, and on both CPU and GPU. We demonstrate that STI delivers high accuracies with 1\u20132 orders of magnitude lower memory, outperforming competitive baselines.", "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2022-07-11", "journal": {"name": "Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2"}, "authors": [{"authorId": "2110519994", "name": "Liwei Guo"}, {"authorId": "1931741049", "name": "Wonkyo Choe"}, {"authorId": "1774176", "name": "F. Lin"}], "citations": [{"paperId": "886e9f83c61bdc489d9ef3423d378aad0a6d686e", "title": "OTAS: An Elastic Transformer Serving System via Token Adaptation"}, {"paperId": "0d558136634156da3677d89d7ad9654342334c4d", "title": "Training and Serving System of Foundation Models: A Comprehensive Survey"}, {"paperId": "13261129251c9e8891cff02c3aee15c4df6a5630", "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"}, {"paperId": "00e889fcfaf4396a20f37f681cf8b14f3e878879", "title": "LLMCad: Fast and Scalable On-device Large Language Model Inference"}, {"paperId": "dc7b27b0a1d891e16f80dd9b8eec0aa8baf95b36", "title": "EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models"}]}
