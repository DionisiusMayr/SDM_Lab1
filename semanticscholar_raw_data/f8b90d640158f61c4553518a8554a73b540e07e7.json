{"paperId": "f8b90d640158f61c4553518a8554a73b540e07e7", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models", "abstract": "Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-08-23", "journal": {"name": "ArXiv", "volume": "abs/2308.12014"}, "authors": [{"authorId": "2110071312", "name": "Jing Yao"}, {"authorId": "3393196", "name": "Xiaoyuan Yi"}, {"authorId": "2108045320", "name": "Xiting Wang"}, {"authorId": "2145270616", "name": "Jindong Wang"}, {"authorId": "2110971997", "name": "Xing Xie"}], "citations": [{"paperId": "33c8910107f3fcb17d140cc88554652508ae3674", "title": "Detoxifying Large Language Models via Knowledge Editing"}, {"paperId": "cef1e2542bd47e5e9ba7100835d383693428ca20", "title": "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models"}, {"paperId": "5a0573a3c15d094e8b3d488c11a660773f631070", "title": "Towards Measuring and Modeling\"Culture\"in LLMs: A Survey"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "2ef23e8b3c07885a850164d86ba748303e335377", "title": "CultureLLM: Incorporating Cultural Differences into Large Language Models"}, {"paperId": "5a5e68a2ffc64b4ff5173a03f7b20b55de6195ec", "title": "Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test"}, {"paperId": "cfcf8ab7c595c1849e8396167a29f3bd3359107c", "title": "Agent Alignment in Evolving Social Norms"}, {"paperId": "592ac35991e583fc37c26ee6659d2deb85142ad9", "title": "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives"}, {"paperId": "9bce4959082f9c133d9721bdb177dcb25d1d6b33", "title": "Control Risk for Potential Misuse of Artificial Intelligence in Science"}, {"paperId": "391eaeb1092c2b145ff0e5a2fa61637a42921fce", "title": "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"}, {"paperId": "9331818a22f1b80b397b4de8f0742403e2588436", "title": "Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values"}, {"paperId": "8ec7d50250203543a0098d99f04957b22bbe2c77", "title": "How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model"}, {"paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2", "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"}, {"paperId": "c4ef333deeeee9cc8cbf16a9e76bc4a60a43ae03", "title": "Post Turing: Mapping the landscape of LLM Evaluation"}, {"paperId": "8ce31d72dcfcd888015646b15f201d49aa71c648", "title": "Unpacking the Ethical Value Alignment in Big Models"}, {"paperId": "6f75e8b61f13562237851d8119cb2f9d49e073fb", "title": "Can LLM-Generated Misinformation Be Detected?"}, {"paperId": "2c3890b566c37927ac70eedfa3e6aecb79b13832", "title": "Probing the Moral Development of Large Language Models through Defining Issues Test"}, {"paperId": "d01dc13cc49373241f31dd86d6bde6aafee58659", "title": "Emotion-based Morality in Tagalog and English Scenarios (EMoTES-3K): A Parallel Corpus for Explaining (Im)morality of Actions"}]}
