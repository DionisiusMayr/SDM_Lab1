{"paperId": "63658795e406974788b8db875e33b3f7776dbb74", "publicationVenue": null, "title": "Scalable Ontology Reasoning Using GPU Cluster Approach", "abstract": "In recent years, there has been a need for techniques for large-scale ontology inference in order to infer new knowledge from existing knowledge at a high speed, and for a diversity of semantic services. With the recent advances in distributed computing, developments of ontology inference engines have mostly been studied based on Hadoop or Spark frameworks on large clusters. Parallel programming techniques using GPGPU, which utilizes many cores when compared with CPU, is also used for ontology inference. In this paper, by combining the advantages of both techniques, we propose a new method for reasoning large RDFS ontology data using a Spark in-memory framework and inferencing distributed data at a high speed using GPGPU. Using GPGPU, ontology reasoning over high-capacity data can be performed as a low cost with higher efficiency over conventional inference methods. In addition, we show that GPGPU can reduce the data workload on each node through the Spark cluster. In order to evaluate our approach, we used LUBM ranging from 10 to 120. Our experimental results showed that our proposed reasoning engine performs 7 times faster than a conventional approach which uses a Spark in-memory inference engine.", "venue": "", "year": 2016, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2016-01-15", "journal": {"name": "", "pages": "61-70", "volume": "43"}, "authors": [{"authorId": "73688108", "name": "Jinyung Hong"}, {"authorId": "67148132", "name": "Myungjoong Jeon"}, {"authorId": "97980350", "name": "Youngtack Park"}], "citations": [{"paperId": "78b3cec0505f08e337c8e9a93cad112834fb3c59", "title": "Spatial Computation on Spark Using GPGPU"}]}
