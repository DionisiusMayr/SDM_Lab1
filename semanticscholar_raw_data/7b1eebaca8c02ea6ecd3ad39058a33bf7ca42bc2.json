{"paperId": "7b1eebaca8c02ea6ecd3ad39058a33bf7ca42bc2", "publicationVenue": {"id": "9448f839-459b-45f3-8573-5eff3f032334", "name": "USENIX Annual Technical Conference", "type": "conference", "alternate_names": ["USENIX Annu Tech Conf", "USENIX", "USENIX ATC"], "url": "https://www.usenix.org/conferences/byname/131"}, "title": "Scaling Large Production Clusters with Partitioned Synchronization", "abstract": "The scale of computer clusters has grown signi\ufb01cantly in recent years. Today, a cluster may have 100 thousand machines and execute billions of tasks, especially short tasks, each day. As a result, the scheduler, which manages resource utilization in a cluster, also needs to be upgraded to work at a much larger scale. However, upgrading the scheduler \u2014 a central system component \u2014 in a large production cluster is a daunting task as we need to ensure the cluster\u2019s stability and robustness, e.g., user transparency should be guaranteed, and other cluster components and the existing scheduling policies need to remain unchanged. We investigated existing scheduler designs and found that most cannot handle the scale of our production clusters or may endanger their robustness. We analyzed one most suitable design that follows a shared-state architecture, and its limitations led us to a \ufb01ne-grained staleness-aware state sharing design, called partitioned synchronization (ParSync). ParSync features the simplicity required for maintaining the robustness of a production cluster, while achieving high scheduling ef\ufb01ciency and quality in scaling. ParSync has been deployed and is running stably in our production clusters.", "venue": "USENIX Annual Technical Conference", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "81-97"}, "authors": [{"authorId": "2115320278", "name": "Yihui Feng"}, {"authorId": "2118358237", "name": "Zhi Liu"}, {"authorId": "8890418", "name": "Yunjian Zhao"}, {"authorId": "35295263", "name": "Tatiana Jin"}, {"authorId": "47096554", "name": "Yidi Wu"}, {"authorId": "2145956913", "name": "Yang Zhang"}, {"authorId": "1717691", "name": "James Cheng"}, {"authorId": "2150358301", "name": "Chao Li"}, {"authorId": "143780672", "name": "Tao Guan"}], "citations": [{"paperId": "7c9a345f58a16a857c357313495b44e7df387af8", "title": "PPS: Fair and efficient black-box scheduling for multi-tenant GPU clusters"}, {"paperId": "4c77a3f43f520d95e5ee0fc5d2e0c559b2fad783", "title": "Rethinking Virtual Machines Live Migration for Memory Disaggregation"}, {"paperId": "10b591db5c6ccb770da45cc81d5ba6baa81189ce", "title": "Not All Resources are Visible: Exploiting Fragmented Shadow Resources in Shared-State Scheduler Architecture"}, {"paperId": "4f005efd52334f68cace00980c69fba6326e79f8", "title": "G\u00f6del: Unified Large-Scale Resource Management and Scheduling at ByteDance"}, {"paperId": "3967c7b34139d5afd70d4b39ee884d59d47c6251", "title": "Vela: A 3-Phase Distributed Scheduler for the Edge-Cloud Continuum"}, {"paperId": "8844062fc954dd5f0fa326ee756947eeece87e76", "title": "Learning to Score: Tuning Cluster Schedulers through Reinforcement Learning"}, {"paperId": "33196fe8321566ee614c3059b2d8c42eef102e82", "title": "Vineyard: Optimizing Data Sharing in Data-Intensive Analytics"}, {"paperId": "ecdd913f254105c138d850784c26204e049798b5", "title": "Differentiated Consistency for Worldwide Gossips"}, {"paperId": "616d5b1365fdc19befbc980e47a9cee52a4c827f", "title": "PICASSO: Unleashing the Potential of GPU-centric Training for Wide-and-deep Recommender Systems"}, {"paperId": "154be0d526dc549eab122f204a66545a664afbe5", "title": "Zero Overhead Monitoring for Cloud-native Infrastructure using RDMA"}]}
