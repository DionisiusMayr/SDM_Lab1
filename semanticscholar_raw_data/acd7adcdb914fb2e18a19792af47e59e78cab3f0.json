{"paperId": "acd7adcdb914fb2e18a19792af47e59e78cab3f0", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Overview of the PromptCBLUE Shared Task in CHIP2023", "abstract": "This paper presents an overview of the PromptCBLUE shared task (http://cips-chip.org.cn/2023/eval1) held in the CHIP-2023 Conference. This shared task reformualtes the CBLUE benchmark, and provide a good testbed for Chinese open-domain or medical-domain large language models (LLMs) in general medical natural language processing. Two different tracks are held: (a) prompt tuning track, investigating the multitask prompt tuning of LLMs, (b) probing the in-context learning capabilities of open-sourced LLMs. Many teams from both the industry and academia participated in the shared tasks, and the top teams achieved amazing test results. This paper describes the tasks, the datasets, evaluation metrics, and the top systems for both tasks. Finally, the paper summarizes the techniques and results of the evaluation of the various approaches explored by the participating teams.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-12-29", "journal": {"name": "ArXiv", "volume": "abs/2312.17522"}, "authors": [{"authorId": "2261473888", "name": "Wei Zhu"}, {"authorId": "2261365693", "name": "Xiaoling Wang"}, {"authorId": "2261360054", "name": "Mosha Chen"}, {"authorId": "2261281073", "name": "Buzhou Tang"}], "citations": [{"paperId": "e4d913a4a1e5286b93e4dca0e032c58c3794e873", "title": "ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models"}, {"paperId": "0b87b928e898299859906cf8cb107b4e2df979b9", "title": "Advancing Biomedical Text Mining with Community Challenges"}]}
