{"paperId": "bda02351d387bc63265f9e5671a69be8d386272c", "publicationVenue": null, "title": "Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization", "abstract": "Despite the recent success of Graph Neural Networks, it remains challenging to train a GNN on large graphs with millions of nodes and billions of edges, which are prevalent in many graph-based applications. Traditional sampling-based methods accelerate GNN training by dropping edges and nodes, which impairs the graph integrity and model performance. Differently, distributed GNN algorithms accelerate GNN training by utilizing multiple computing devices and can be classified into two types:\"partition-based\"methods enjoy low communication costs but suffer from information loss due to dropped edges, while\"propagation-based\"methods avoid information loss but suffer from prohibitive communication overhead caused by the neighbor explosion. To jointly address these problems, this paper proposes DIGEST (DIstributed Graph reprEsentation SynchronizaTion), a novel distributed GNN training framework that synergizes the complementary strength of both categories of existing methods. We propose to allow each device to utilize the stale representations of its neighbors in other subgraphs during subgraph parallel training. This way, our method preserves global graph information from neighbors to avoid information loss and reduce communication costs. Our convergence analysis demonstrates that DIGEST enjoys a state-of-the-art convergence rate. Extensive experimental evaluation on large, real-world graph datasets shows that DIGEST achieves up to 21.82 speedups without compromising performance compared to state-of-the-art distributed GNN training frameworks.", "venue": "", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2022-05-31", "journal": null, "authors": [{"authorId": "1712184172", "name": "Zheng Chai"}, {"authorId": "7583867", "name": "Guangji Bai"}, {"authorId": "144010790", "name": "Liang Zhao"}, {"authorId": "47585127", "name": "Yue Cheng"}], "citations": [{"paperId": "4ae7c4decd1df71c466f19d66d69b555945098c4", "title": "Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data"}, {"paperId": "46210e170045df3c0c50a17bb63e6de480d62f9d", "title": "FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training"}]}
