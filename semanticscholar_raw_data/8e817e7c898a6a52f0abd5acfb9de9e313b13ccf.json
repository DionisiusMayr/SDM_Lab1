{"paperId": "8e817e7c898a6a52f0abd5acfb9de9e313b13ccf", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI", "abstract": "The race to train language models on vast, diverse, and inconsistently documented datasets has raised pressing concerns about the legal and ethical risks for practitioners. To remedy these practices threatening data transparency and understanding, we convene a multi-disciplinary effort between legal and machine learning experts to systematically audit and trace 1800+ text datasets. We develop tools and standards to trace the lineage of these datasets, from their source, creators, series of license conditions, properties, and subsequent use. Our landscape analysis highlights the sharp divides in composition and focus of commercially open vs closed datasets, with closed datasets monopolizing important categories: lower resource languages, more creative tasks, richer topic variety, newer and more synthetic training data. This points to a deepening divide in the types of data that are made available under different license conditions, and heightened implications for jurisdictional legal interpretations of copyright and fair use. We also observe frequent miscategorization of licenses on widely used dataset hosting sites, with license omission of 72%+ and error rates of 50%+. This points to a crisis in misattribution and informed use of the most popular datasets driving many recent breakthroughs. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire audit, with an interactive UI, the Data Provenance Explorer, which allows practitioners to trace and filter on data provenance for the most popular open source finetuning data collections: www.dataprovenance.org.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-25", "journal": {"name": "ArXiv", "volume": "abs/2310.16787"}, "authors": [{"authorId": "29909347", "name": "S. Longpre"}, {"authorId": "1729586298", "name": "Robert Mahari"}, {"authorId": "2261703906", "name": "Anthony Chen"}, {"authorId": "2219414905", "name": "Naana Obeng-Marnu"}, {"authorId": "48217073", "name": "Damien Sileo"}, {"authorId": "122580886", "name": "William Brannon"}, {"authorId": "2037383772", "name": "Niklas Muennighoff"}, {"authorId": "2261493949", "name": "Nathan Khazam"}, {"authorId": "2631301", "name": "Jad Kabbara"}, {"authorId": "67322972", "name": "Kartik Perisetla"}, {"authorId": "2255396948", "name": "Xinyi Wu"}, {"authorId": "2261554775", "name": "Enrico Shippole"}, {"authorId": "1742448", "name": "K. Bollacker"}, {"authorId": "2261675987", "name": "Tongshuang Wu"}, {"authorId": "2261494161", "name": "Luis Villa"}, {"authorId": "2261492861", "name": "Sandy Pentland"}, {"authorId": "2261551123", "name": "Deb Roy"}, {"authorId": "2261493078", "name": "Sara Hooker"}], "citations": [{"paperId": "8719833751cf1bfc779c944fc7954a337b2c0833", "title": "Source-Aware Training Enables Knowledge Attribution in Language Models"}, {"paperId": "98755a7ca75afbb29a19f8129b9f25796ad0e0b7", "title": "Language models scale reliably with over-training and on downstream tasks"}, {"paperId": "21f8977648e25ce8b95020e6f01988af99209c82", "title": "A Safe Harbor for AI Evaluation and Red Teaming"}, {"paperId": "02ac355296f001a010b1db115d909c052767ccb3", "title": "Stable LM 2 1.6B Technical Report"}, {"paperId": "8bb920441491dc60cb00ee753d5825b9d0d042b6", "title": "On the Societal Impact of Open Foundation Models"}, {"paperId": "16fbff9f07963fb394aef9fbc209e8554badc455", "title": "Rethinking Software Engineering in the Foundation Model Era: A Curated Catalogue of Challenges in the Development of Trustworthy FMware"}, {"paperId": "9ccaeea2c76a9072ccebf3eea3438d2ce18f5723", "title": "Unintended Impacts of LLM Alignment on Global Representation"}, {"paperId": "1244fd52eb23246dbef2ef6c6bf58dbf361550cf", "title": "Building Efficient Universal Classifiers with Natural Language Inference"}, {"paperId": "02d03b97cb76b54107ea2ad8f4ec671ae3aa52ac", "title": "A Material Lens on Coloniality in NLP"}, {"paperId": "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c", "title": "Scaling Data-Constrained Language Models"}, {"paperId": "d4b49f1092d8731a7f98c93ea21276a278088f77", "title": "Janus: Fast Privacy-Preserving Data Provenance For TLS 1.3"}]}
