{"paperId": "66d927fdb6c2774131960c75275546fd5ee3dd72", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models", "abstract": "The vision and language generative models have been overgrown in recent years. For video generation, various open-sourced models and public-available services have been developed to generate high-quality videos. However, these methods often use a few metrics, e.g., FVD or IS, to evaluate the performance. We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities. Thus, we propose a novel framework and pipeline for exhaustively evaluating the performance of the generated videos. Our approach involves generating a diverse and comprehensive list of 700 prompts for text-to-video generation, which is based on an analysis of real-world user data and generated with the assistance of a large language model. Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmark, in terms of visual qualities, content qualities, motion qualities, and text-video alignment with 17 well-selected objective metrics. To obtain the final leaderboard of the models, we further fit a series of coefficients to align the objective metrics to the users' opinions. Based on the proposed human alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-17", "journal": {"name": "ArXiv", "volume": "abs/2310.11440"}, "authors": [{"authorId": "2142860074", "name": "Yaofang Liu"}, {"authorId": "30176430", "name": "Xiaodong Cun"}, {"authorId": "2259069666", "name": "Xuebo Liu"}, {"authorId": "2253795356", "name": "Xintao Wang"}, {"authorId": "2257199953", "name": "Yong Zhang"}, {"authorId": "2149052351", "name": "Haoxin Chen"}, {"authorId": "2258961576", "name": "Yang Liu"}, {"authorId": "2277505260", "name": "Tieyong Zeng"}, {"authorId": "2258962579", "name": "Raymond Chan"}, {"authorId": "2257019659", "name": "Ying Shan"}], "citations": [{"paperId": "ec45c3f0f88c8ce1deb5baa71c2c0e14ad64d249", "title": "Evaluating Text-to-Visual Generation with Image-to-Text Generation"}, {"paperId": "a90282f04fa9c70e791fc3c4d7cd94622c9030a9", "title": "EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing"}, {"paperId": "83683ae4485dbfc4bfa54ad9c2c5892c50f134bf", "title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis"}, {"paperId": "5b0836d24f6dd6a84910cff05c05d41e40c1973b", "title": "Subjective-Aligned Dataset and Metric for Text-to-Video Quality Assessment"}, {"paperId": "a70f267ce46eb8b46e9482e9bacb00e64b03c6ea", "title": "Generalized Predictive Model for Autonomous Driving"}, {"paperId": "615808738d2e4566049717bc95a435fb65835dc9", "title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models"}, {"paperId": "97cb2eb0d0517e34bf4202f0593600bb6fa043cd", "title": "Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis"}, {"paperId": "78b02a431af35f9fc02a36564073244a7f2de042", "title": "LightHouse: A Survey of AGI Hallucination"}, {"paperId": "2ee7301efe2829a8f34bbc70006e45f461092af8", "title": "VASE: Object-Centric Appearance and Shape Manipulation of Real Videos"}, {"paperId": "322ca4c62d68ca8814534042096bbec40f743679", "title": "AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI"}, {"paperId": "0c4f46e4dcae5527018e6432fb60cfe8c3354e97", "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation"}, {"paperId": "e0d62e25811018636c22d2cc76650b9d31968890", "title": "Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis"}, {"paperId": "3754df53a5135064ba312251ad47b478c4afb2ef", "title": "VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence"}, {"paperId": "4e9a8141da2a8c603722b07d096109207f8e0b66", "title": "VBench: Comprehensive Benchmark Suite for Video Generative Models"}, {"paperId": "d831988859f0c077b38094446d8585a8340af223", "title": "FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling"}, {"paperId": "ec816fd1286d404ba48f489f9b4236d086f7d342", "title": "I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models"}]}
