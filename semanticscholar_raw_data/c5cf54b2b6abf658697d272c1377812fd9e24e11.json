{"paperId": "c5cf54b2b6abf658697d272c1377812fd9e24e11", "publicationVenue": null, "title": "Scaling Generative Pre-training for User Ad Activity Sequences", "abstract": "User activity sequence modeling has significantly improved performance across a range tasks in advertising spanning across supervised learning tasks like ad response prediction to unsupervised tasks like robot and ad fraud detection. Self-supervised learning using autoregressive generative models has garnered interest due to performance improvements on time series and natural language data. In this paper, we present a scalable autoregressive generative pre-training framework to model user ad activity sequences and inspect its scaling properties with respect to model size, dataset size and compute. We show that test loss on pre-training task follows power law scaling with respect to model size, with larger models being more data and compute efficient than smaller models. We also demonstrate that improvement in pre-training test loss translates into better downstream task performance by benchmarking the models on conversion prediction and robot detection tasks in advertising.", "venue": "AdKDD@KDD", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": null, "authors": [{"authorId": "1389561411", "name": "Sharad Chitlangia"}, {"authorId": "1512244053", "name": "Krishna Reddy Kesari"}, {"authorId": "2217281991", "name": "Rajat Agarwal"}, {"authorId": "2236657422", "name": "Rajat Agarwal. . Scaling"}], "citations": [{"paperId": "32f65dd751e5c491d83c2f46e41f50afa1c6d2bb", "title": "Scaling Law of Large Sequential Recommendation Models"}]}
