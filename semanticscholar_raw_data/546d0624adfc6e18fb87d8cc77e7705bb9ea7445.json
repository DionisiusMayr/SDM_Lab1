{"paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445", "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "title": "LIMA: Less Is More for Alignment", "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.", "venue": "Neural Information Processing Systems", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-05-18", "journal": {"name": "ArXiv", "volume": "abs/2305.11206"}, "authors": [{"authorId": "2384711", "name": "Chunting Zhou"}, {"authorId": "144118452", "name": "Pengfei Liu"}, {"authorId": "2214843767", "name": "Puxin Xu"}, {"authorId": "1900163", "name": "Srini Iyer"}, {"authorId": "145478138", "name": "Jiao Sun"}, {"authorId": "3375249", "name": "Yuning Mao"}, {"authorId": "2378954", "name": "Xuezhe Ma"}, {"authorId": "1388010852", "name": "Avia Efrat"}, {"authorId": "2114104308", "name": "Ping Yu"}, {"authorId": "49297123", "name": "L. Yu"}, {"authorId": "2108244542", "name": "Susan Zhang"}, {"authorId": "134007132", "name": "Gargi Ghosh"}, {"authorId": "35084211", "name": "M. Lewis"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}, {"authorId": "39455775", "name": "Omer Levy"}], "citations": [{"paperId": "8dc4ac4446f787f0647e1e16a9c43b2880e04008", "title": "Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer"}, {"paperId": "5e8d6b947493dfed3cab8d7f00002c59f69c4c40", "title": "Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning"}, {"paperId": "3fa9def5ab2ede7e66635f750062419404c5f4a8", "title": "Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models"}, {"paperId": "8a63a1735fb2c403f1bf1ce20af217f3786b45ad", "title": "Towards detecting unanticipated bias in Large Language Models"}, {"paperId": "a5ed7a04640b11374871ccd6f6498bbc25aa8239", "title": "Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models"}, {"paperId": "8b57632b19df9ebd3e57e3adbef7fc6ec93bc506", "title": "HyperCLOVA X Technical Report"}, {"paperId": "8115ffbbadd1055424d18369dba66ce32a572800", "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback"}, {"paperId": "87f2db760cb864deb8c9f2cce93bd4dc2984969d", "title": "Large language models for automated Q&A involving legal documents: a survey on algorithms, frameworks and applications"}, {"paperId": "746b96ee17e329f1085a047116c05e12eaa3925a", "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation"}, {"paperId": "5ca298600704d76adfc546742f85010f168efd1f", "title": "Mechanisms of non-factual hallucinations in language models"}, {"paperId": "12bd7d6b646211d9b9328499df6fd12040277dea", "title": "Large Language Models Enhanced Collaborative Filtering"}, {"paperId": "c113d96c532c11eca026f50e80541eafb112e35f", "title": "COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"}, {"paperId": "6457474106e7af2c5f67769fbf915432b7d1b513", "title": "Qibo: A Large Language Model for Traditional Chinese Medicine"}, {"paperId": "553c86ba967f55db9caffe08240061b1282da893", "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection"}, {"paperId": "c9e4af8b55db19f9bc9359806ca9283053366fe5", "title": "Improving the Robustness of Large Language Models via Consistency Alignment"}, {"paperId": "357cc1d2792c91ee3b97abcba5c253310d6bb353", "title": "AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation"}, {"paperId": "7f8aa59691669cccef6dbb23f9e55472c78118bc", "title": "AgentGroupChat: An Interactive Group Chat Simulacra For Better Eliciting Emergent Behavior"}, {"paperId": "8c997fbe1606f544fe5a26c1177f640880b597bf", "title": "MELTing point: Mobile Evaluation of Language Transformers"}, {"paperId": "2d1adda7a9b0596d8b470c065da5a6528b364a7f", "title": "Automated Data Curation for Robust Language Model Fine-Tuning"}, {"paperId": "8202484d4127f1499b23aa941b9e2c0e48fd287a", "title": "X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment"}, {"paperId": "b3cdde45d87a3a90d86ebe20c1ee8c4ef0150d1c", "title": "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment"}, {"paperId": "b73174d12a90aea9c3ac8936ede9c2a60a492477", "title": "Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean"}, {"paperId": "6edf144fb397afde80d9acf9472dfaffd22c8072", "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models"}, {"paperId": "ab8e6df5001dbb9b48445220099425aff536b3e8", "title": "Knowledge Conflicts for LLMs: A Survey"}, {"paperId": "917aa8165c0ab778ea3aea7a2b286c74cd2889f0", "title": "SMART: Submodular Data Mixture Strategy for Instruction Tuning"}, {"paperId": "f969811d8338293b4602568f8337da1c231edadd", "title": "A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data Overlap Estimation in the Eduation Context"}, {"paperId": "54f96e08a7778da7ff0e9b03bb7e2baf77610cde", "title": "Beyond Memorization: The Challenge of Random Memory Access in Language Models"}, {"paperId": "e6dab08d936e0192b0ea197527f5a4e845db424a", "title": "Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts"}, {"paperId": "973814cd535facbf4f27c3de477b05bf19366030", "title": "ORPO: Monolithic Preference Optimization without Reference Model"}, {"paperId": "4146b447187e1a09b736564854007c403f986c69", "title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling"}, {"paperId": "cf65db0934696e1f8896da52811b3d7f79836abd", "title": "From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification"}, {"paperId": "a0b07f40de7b307dfc40e5c569af0d14d4160e8e", "title": "Common 7B Language Models Already Possess Strong Math Capabilities"}, {"paperId": "66e7edf09589527ebb58418632418758cee668cd", "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"}, {"paperId": "ae4635297ad87fcb3ec4105a51b5cbcb4075e5e2", "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error"}, {"paperId": "13f44206745d20971ca271401eff6772aa80de80", "title": "SaulLM-7B: A pioneering Large Language Model for Law"}, {"paperId": "78f0f1ba187359b4c48264d3d7ee838d470d587a", "title": "Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization"}, {"paperId": "087c74522fda4bc178eb23bebc988bf46007f4fb", "title": "Design2Code: How Far Are We From Automating Front-End Engineering?"}, {"paperId": "dd334c161aad0e459e8980b362f9a3c7fd6ae51f", "title": "PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval"}, {"paperId": "0ec88f8071d4a55e62a1b85661c1f11a01489047", "title": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation"}, {"paperId": "3bdd3d56ef9054aba47f83879b531a4842640295", "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning"}, {"paperId": "62863fbcff6bc411bd83dc50d3e1bcbe72f651ef", "title": "RECOST: External Knowledge Guided Data-efficient Instruction Tuning"}, {"paperId": "bdb826b267841b8948497064efd01b781ac10341", "title": "Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation"}, {"paperId": "cc49514a9859ae171a7cd7eaae111990605a5e4e", "title": "Tower: An Open Multilingual Large Language Model for Translation-Related Tasks"}, {"paperId": "d32764d479f338e0a1897cc3c35630f4ed0a39bf", "title": "SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection"}, {"paperId": "cbfc697db82e3f7b5e9d16f9998dc6ada862ee35", "title": "CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision"}, {"paperId": "31e27369f64c51ffbfd9bc8e3cbb20705edc6bff", "title": "Citation-Enhanced Generation for LLM-based Chatbots"}, {"paperId": "a7d12340cb1c89f47ddff8f2293b2e8561fe92cb", "title": "EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings"}, {"paperId": "208d9e72a80c9333c36f8ede204128e3c808af84", "title": "C3: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding"}, {"paperId": "27285b3760be8f0473245c13b97988265cd0467b", "title": "Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models"}, {"paperId": "c0082580c4b9e5c6c96cf06f1be67c0cbbafb753", "title": "A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models"}, {"paperId": "ad9330c7cb31adb2197dae8f7f9998142e071857", "title": "Zero-shot cross-lingual transfer in instruction tuning of large language model"}, {"paperId": "0edcd3f065caa1d9e2dad4458f5b2f56efa3162b", "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking"}, {"paperId": "9ccaeea2c76a9072ccebf3eea3438d2ce18f5723", "title": "Unintended Impacts of LLM Alignment on Global Representation"}, {"paperId": "04ebbac1a75b083ec871961b0e0807f5ac24393c", "title": "INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning"}, {"paperId": "32b8734b4834acf22fac8399026119fa3e60de79", "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?"}, {"paperId": "c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a", "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning"}, {"paperId": "44dc41803f49f7511f674ecb091d7a5c69fd5db2", "title": "Learning to Poison Large Language Models During Instruction Tuning"}, {"paperId": "e29743ff496d0b6e59d10b86fc9cd7f00a1adaf3", "title": "RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models"}, {"paperId": "c290633698501ea83144d61d001eb7ac7a42d853", "title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models"}, {"paperId": "49672006290b125aea958d1bae5d07c8e48ce8bb", "title": "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons"}, {"paperId": "1c0b3679919cd0531973fced1a1eb49745d9332d", "title": "Instruction-tuned Language Models are Better Knowledge Learners"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "a7d047dd9f41d5f3e7eaa39e5ba8c97cccc7276d", "title": "Reformatted Alignment"}, {"paperId": "2ac35475ccf0a6a89bbd04377a4fe61c175030a4", "title": "Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement"}, {"paperId": "042ceb6be64a0b9a40bdbf7c96fe4dd7fabd93eb", "title": "Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection"}, {"paperId": "15f3cee0b35891d43d518a7af444d0626de5c42d", "title": "Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought"}, {"paperId": "f15086f851680e1c05f1eb871436ff66c838b907", "title": "Foundation Models for Recommender Systems: A Survey and New Perspectives"}, {"paperId": "2451f9a452819802c7d4aab117f3a167c5b65a87", "title": "Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs"}, {"paperId": "63f3d3830b6478e5e0b5eb04e26ac0d3accbc7b0", "title": "Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining"}, {"paperId": "d244b2ba0fd5f573d19df0c7b362692c018748a1", "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models"}, {"paperId": "3dd564a7500861162904c6875a102118891a03f9", "title": "AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation"}, {"paperId": "b2df275f024c6c427698d3de865df2d4d0274aed", "title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?"}, {"paperId": "a5805cca2c24c9125795c10daa87671cd5609972", "title": "Instruction Diversity Drives Generalization To Unseen Tasks"}, {"paperId": "710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e", "title": "Multi-modal preference alignment remedies regression of visual instruction tuning on language model"}, {"paperId": "e12aec7435bd348ec4357faae7d7ccb8d83d954d", "title": "GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models"}, {"paperId": "ee85c7c666135f4aae32336968f09584029b6a35", "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning"}, {"paperId": "32c5b515cab893e5e4bf3f90c8b6c8262bd7ac09", "title": "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation"}, {"paperId": "e79671a83e25288fedd897e1c9e6152f70f7f52e", "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey"}, {"paperId": "490e815b3be11ba97631783d9ae946b8f8517fd6", "title": "Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues"}, {"paperId": "2139e414bdf6a5ea7ec4052d3f65a8d49991494b", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"}, {"paperId": "6acd1576ed0abfb034035cb52bd34bf43abcfea3", "title": "Knowledge Editing on Black-box Large Language Models"}, {"paperId": "b8dbdb9eb9141562f7d48bc3709fee2d23f5e5bd", "title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education"}, {"paperId": "a3a273bbd169ed6e67c2735449bf609662a42cb0", "title": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition"}, {"paperId": "4a5b69b3f79d9fed2eeb82585df35c6bb9a72194", "title": "EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models"}, {"paperId": "c991dedacba67949a28640cd8755de4c8ae297b0", "title": "A Survey on Data Selection for LLM Instruction Tuning"}, {"paperId": "e9aec062906c7fb16e540dc9fb7ed2cbcf129407", "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning"}, {"paperId": "b7ad6a754911dc96190469a56962ddde3608e67a", "title": "SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity"}, {"paperId": "11dfbd2f49624b73d0c417dbabbd0278ae25da44", "title": "SelectLLM: Can LLMs Select Important Instructions to Annotate?"}, {"paperId": "4bbc278bc27a399bda7bc3015d2f38bd34f8dff7", "title": "Large Language Model Adaptation for Financial Sentiment Analysis"}, {"paperId": "5b9fad3e2b2cc5dd23b01e0089bb7b6f6865cb82", "title": "Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model"}, {"paperId": "f21d0177e9374bb8579c1d9c71319f212f62b3d5", "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance"}, {"paperId": "6174f46a035c57f7ca4b9d581a31022f60dcbedd", "title": "PHOENIX: Open-Source Language Adaption for Direct Preference Optimization"}, {"paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f", "title": "Knowledge Verification to Nip Hallucination in the Bud"}, {"paperId": "d1679160211bcbedd46bf1826caad5a7e1877315", "title": "Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models"}, {"paperId": "c6e162aedf6a5ab0135e3b991577d77ca06673f9", "title": "SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning"}, {"paperId": "c1474dc03848cb26d118bc37c26636d7082c3854", "title": "Improving Domain Adaptation through Extended-Text Reading Comprehension"}, {"paperId": "2b9fdc60647701c9c79c3203c04fded35fe2d7ec", "title": "An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models"}, {"paperId": "de4dfb773ab455081e5fb1862e08f581c58d43bc", "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems"}, {"paperId": "3e8d4062ec4353ff2701c7769336dbdb97f8814c", "title": "Transformers are Multi-State RNNs"}, {"paperId": "78b02a431af35f9fc02a36564073244a7f2de042", "title": "LightHouse: A Survey of AGI Hallucination"}, {"paperId": "45230b5453f9100d0d3b158a1254a1c3aa691577", "title": "Artificial Intelligence for Operations Research: Revolutionizing the Operations Research Process"}, {"paperId": "5708f725e13362da80a1062f51df118fca3529ab", "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples"}, {"paperId": "798ece3c5491f613e5368bd2d818476a64b88905", "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers"}, {"paperId": "c3d1832ed0444f75d44116fabbdda891aebc4b01", "title": "LLaMA Pro: Progressive LLaMA with Block Expansion"}, {"paperId": "d98aa44f79fe798ad5ff0cac6e7bf32ee30bd156", "title": "GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse"}, {"paperId": "ec7cbe4cc97c8bd2819fe4107483d5a6ac31cd77", "title": "BIBench: Benchmarking Data Analysis Knowledge of Large Language Models"}, {"paperId": "cfce709a65f90312d2bdc1a6cf0380c19becf694", "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models"}, {"paperId": "1244fd52eb23246dbef2ef6c6bf58dbf361550cf", "title": "Building Efficient Universal Classifiers with Natural Language Inference"}, {"paperId": "7843287ffcd48bdf830f16465741a548c2ba8d66", "title": "Generative AI for Math: Part I - MathPile: A Billion-Token-Scale Pretraining Corpus for Math"}, {"paperId": "66ea57809a718f2634e4f2065569c0ba24659d44", "title": "Align on the Fly: Adapting Chatbot Behavior to Established Norms"}, {"paperId": "a76630ed22310b123183e32b7d56b69d5c449d58", "title": "RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models"}, {"paperId": "3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e", "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations"}, {"paperId": "6d465be006615460d41060f9f5068d51fc1f46b1", "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models"}, {"paperId": "5b1d02bfce832b8ab41b5209e637a9f42c26d912", "title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?"}, {"paperId": "2d4a853affeb0b164fc1134df612aea658f36459", "title": "Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning"}, {"paperId": "9d793e542b757f234431d209e711c6ef88aa29de", "title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin"}, {"paperId": "36697944858ab17ca23b23ae2043aa6c0b2e3d5d", "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention"}, {"paperId": "6cfbbf7604adda1df65932e3c4d157770a2df000", "title": "Alignment for Honesty"}, {"paperId": "83d12fa58743015f8abe4097fb58088f2d13c7f0", "title": "Rethinking the Instruction Quality: LIFT is What You Need"}, {"paperId": "1c6a5d033743f345447e45e1eb6d6c7cadee9f78", "title": "PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching"}, {"paperId": "600d9287efc4703bdb99ce39b5e8b37da0baa6f6", "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning"}, {"paperId": "117a9f9227ebe4acc67ff8a52eb9db2ab16390c8", "title": "Data Management For Large Language Models: A Survey"}, {"paperId": "92105d2f3716786528bd8e7498e13162b409cd37", "title": "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking"}, {"paperId": "da89cdeb0014666f4024f797d0c67cd45d92a7c9", "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity"}, {"paperId": "2baf63dede1a96cae314c4be99bd3cf9f49b148e", "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization"}, {"paperId": "e3f7ad05b1652c6ada78cffbe405bceb723bc70c", "title": "MoDS: Model-oriented Data Selection for Instruction Tuning"}, {"paperId": "ec23c1a8e326bd55cc3d6bfcb9388d40a442ac5c", "title": "Walking a Tightrope \u2013 Evaluating Large Language Models in High-Risk Domains"}, {"paperId": "ea9af64a79ffe964e43121e8f78883c146894910", "title": "LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms"}, {"paperId": "0fd454e62f76b9edcfa8baa43a7467368748483f", "title": "CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM Instruction Tuning"}, {"paperId": "89d008f2262f3fd6810f13483f900bf45c368a01", "title": "Automatic Analysis of Substantiation in Scientific Peer Reviews"}, {"paperId": "38552c3f0f6afd651cc93b59ef7db6dafa405bf0", "title": "A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest"}, {"paperId": "312fed7476dd58b8a14bbc2a15f9faa00423b988", "title": "FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models"}, {"paperId": "36a8e2185dfeb65259a6ca12dbcd80266319565f", "title": "GEO: Generative Engine Optimization"}, {"paperId": "2a86d281bef364e2ea2d4fc61fde46ca25b955f1", "title": "HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs"}, {"paperId": "145b706e88a17c1b45588af77c4e0aa9a92bdee3", "title": "Can Language Model Moderators Improve the Health of Online Discourse?"}, {"paperId": "a1e8a8842888c7cffecce53a87a800729e90c36d", "title": "R-Tuning: Teaching Large Language Models to Refuse Unknown Questions"}, {"paperId": "62c8c4e610abf823e6c4f5d3cfc2aa006c59036c", "title": "Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies"}, {"paperId": "f7a0501a246882103bc84cbc4f7270d1e7e428a8", "title": "Enabling Large Language Models to Learn from Rules"}, {"paperId": "9c60c766d65daa5554949fb3bce2df1ef58a5a16", "title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning"}, {"paperId": "05d2ced6a4fb7efb8d527a228ad792526a202235", "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities"}, {"paperId": "06ba45793c761583ffdd408141f81517b92f649f", "title": "Functionality learning through specification instructions"}, {"paperId": "709af143f78bc62413c50ea1a7ee75b0702c4f59", "title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming"}, {"paperId": "eb8e86d504084531691ff31d0fc23c82fbcee36c", "title": "GIELLM: Japanese General Information Extraction Large Language Model Utilizing Mutual Reinforcement Effect"}, {"paperId": "712f6dfcee099ee38d6d09af23e8bc0a7e82bb72", "title": "Fake Alignment: Are LLMs Really Aligned Well?"}, {"paperId": "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8", "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"}, {"paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2", "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"}, {"paperId": "c4ef333deeeee9cc8cbf16a9e76bc4a60a43ae03", "title": "Post Turing: Mapping the landscape of LLM Evaluation"}, {"paperId": "86c7cb73e3a42a130c8d43ae3e26cb9c8df381a1", "title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests"}, {"paperId": "0399533de2d1d21f456663d1bd5355c8b3c32a58", "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs"}, {"paperId": "dde6c1910d0496c9e5d5483c2c18271a2a660e6c", "title": "Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding"}, {"paperId": "f726cd8c53c8b4ecffe15f5106e0779f0475a3b9", "title": "Constituency Parsing using LLMs"}, {"paperId": "f8f9b9e5a1b9dd67fe61bd358f1d90f6cceeb4a1", "title": "Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace"}, {"paperId": "0bc01fd54372b9d66974920a3929ef9764779dd1", "title": "PockEngine: Sparse and Efficient Fine-tuning in a Pocket"}, {"paperId": "c9e4137f0e0cf224c60cf761106e0cf9363b4f0d", "title": "DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection"}, {"paperId": "8e817e7c898a6a52f0abd5acfb9de9e313b13ccf", "title": "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI"}, {"paperId": "ab90b84b42d43c3077c374cd34b3a48a881faf43", "title": "Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation"}, {"paperId": "c7ad19da81e24c387f0377fef6d19b0fce2cf470", "title": "Self-Guard: Empower the LLM to Safeguard Itself"}, {"paperId": "60988a0ebad89af503f17de977785814fb864635", "title": "Correction with Backtracking Reduces Hallucination in Summarization"}, {"paperId": "670b4f3aebb1156f608880e26f710d3dc04fee51", "title": "Specialist or Generalist? Instruction Tuning for Specific NLP Tasks"}, {"paperId": "75a85d74433d03a78a07bb95f6f261323a79eb80", "title": "CITB: A Benchmark for Continual Instruction Tuning"}, {"paperId": "5ee2d80a3b0e7b0a4c7d9418d7c3f2fee9b6011a", "title": "Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications"}, {"paperId": "2f8a1714e3bb01c522fede9c902bd7cf5320a802", "title": "Test-Time Self-Adaptive Small Language Models for Question Answering"}, {"paperId": "378a51082ddf7430f928b7dde59186c041eb4b6c", "title": "Tuna: Instruction Tuning using Feedback from Large Language Models"}, {"paperId": "c3068c7947ca290496c4d0280904686ba0b2903f", "title": "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning"}, {"paperId": "1c0e4c0e0899147ceb07a997dd57c64bb3384a47", "title": "Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model"}, {"paperId": "2ccc2463e702302acdb096adba9d736da5a64af3", "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective"}, {"paperId": "3c7b23b343eb24f0662c8b9033ec1f2d15cc4c27", "title": "EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset"}, {"paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855", "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"}, {"paperId": "f0227a0500f2875d9af3d62b5afb3bb93c2b4561", "title": "OpenAgents: An Open Platform for Language Agents in the Wild"}, {"paperId": "1efb48995732f58dbf2e251bfdf8571545033db9", "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology"}, {"paperId": "88c57f3a7267dbf56c477e3a77d620746bd487fe", "title": "Farzi Data: Autoregressive Data Distillation"}, {"paperId": "23e5395b4c6249c2873666bff73e203821c14719", "title": "Instruction Tuning with Human Curriculum"}, {"paperId": "88a3abf671d922ebd61a34007908a5f6b6978bd4", "title": "Harnessing Large Language Models' Empathetic Response Generation Capabilities for Online Mental Health Counselling Support"}, {"paperId": "0f0b8f6b9d6f9356f9504c0291ca59db9b20bf53", "title": "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization"}, {"paperId": "03fab98a9be74a253688840dba9144737a8ca92d", "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity"}, {"paperId": "dd7a74a09fc29cadcd47fafc4f7812bb8d2d7208", "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values"}, {"paperId": "34ca51ce10e8d1d6c950ba519329714a0184d004", "title": "KwaiYiiMath: Technical Report"}, {"paperId": "ac27dd71af3ee93e1129482ceececbae7dd0d0e8", "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"}, {"paperId": "0a5e2f812e0f8fc8e71dc74292193cea33c8d422", "title": "GenTKG: Generative Forecasting on Temporal Knowledge Graph"}, {"paperId": "4d4280dd89c048f137e6ebbbb6c9960778b9decb", "title": "Gender, Age, and Technology Education Influence the Adoption and Appropriation of LLMs"}, {"paperId": "a64067c6c4286fc60f4430829ae6b18519c088e3", "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models"}, {"paperId": "eab54bef4b479e0b06fc65c8cab2c0400fa69b8a", "title": "On the Evaluation and Refinement of Vision-Language Instruction Tuning Datasets"}, {"paperId": "5001630bcc65e8e0e621b19625629a2689724743", "title": "Generative Judge for Evaluating Alignment"}, {"paperId": "5088a04d1a9f42b967f3dcf791145e8aa367fc54", "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"}, {"paperId": "24df244bf7a6e8c93c5f183d3f62d39c0f773c68", "title": "SALMON: Self-Alignment with Principle-Following Reward Models"}, {"paperId": "67daf8c4fe1958d20ebdf95c2a36dd490c73836f", "title": "FireAct: Toward Language Agent Fine-tuning"}, {"paperId": "c7492913370b5726eaa6ced163a60de6c9d4bb7f", "title": "A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"}, {"paperId": "281424b0c400cae1c1022f2cb825959b21790723", "title": "Foundation Models Meet Visualizations: Challenges and Opportunities"}, {"paperId": "f8fa697302ef9d759b72404f5a8aa918865abdf4", "title": "A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions"}, {"paperId": "cddb552f6c3464a54a02b0b64b2d1af56c086606", "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "62e633f4b5cf8bc573e496602d3aa6e5919bbe61", "title": "Improving Automatic VQA Evaluation Using Large Language Models"}, {"paperId": "84b7c486c56bd3880cb8eb01de9ae90ba3ebdaed", "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models"}, {"paperId": "b1c9c455acb1393f1bf60ec4095ee343531e404e", "title": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use"}, {"paperId": "1d8472bbf71ccf60550126d136b53699a2f4f685", "title": "DOMINO: A Dual-System for Multi-step Visual Language Reasoning"}, {"paperId": "957b601e2beefeaaf2e069e0130054051c8a7782", "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "bff3dc34477ac2d10477cdd37c8e71d5688c40dd", "title": "The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising \"Alignment\" in Large Language Models"}, {"paperId": "0ea1d396ce3804054c1919d7b78d3bcddaa761c0", "title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models"}, {"paperId": "04b880e1e32f37b3796d41a47d013fa07095ae32", "title": "From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning"}, {"paperId": "e8df1cf6742b50a15500b8dd3dde3942e9c91418", "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"}, {"paperId": "4f5c4ae2026b2bd97e26c6969e54cc634895e477", "title": "Jointly Training Large Autoregressive Multimodal Models"}, {"paperId": "6f75e8b61f13562237851d8119cb2f9d49e073fb", "title": "Can LLM-Generated Misinformation Be Detected?"}, {"paperId": "772724892819d7e6f15ce536753fdc32d022c0e0", "title": "A Survey on Image-text Multimodal Models"}, {"paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce", "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"}, {"paperId": "166d1e5361465f8e235747d14641249cbb3b6fd2", "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models"}, {"paperId": "67134a23fff318d9a592bbd383bdaabc74a6ecd6", "title": "XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates"}, {"paperId": "29f032fc875576b5c3c6b1c2d76af8639bacfb88", "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"}, {"paperId": "bc9f29881c1d93d225f0a74fa700531202c7043a", "title": "OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch"}, {"paperId": "2f4bfc41704a090138afa4174c461896d5d9c98f", "title": "Adapting Large Language Models via Reading Comprehension"}, {"paperId": "0fb61be60088e80e565b84f44e49ba30630b6126", "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal"}, {"paperId": "b549adab842e48b037d8aca84336296f84a9fa81", "title": "Can Large Language Models Understand Real-World Complex Instructions?"}, {"paperId": "62b4e06f5249d22e4a153ec4a2dc934c6a014372", "title": "OWL: A Large Language Model for IT Operations"}, {"paperId": "e56dc21699e6283fce072ffc908cb9f66321760d", "title": "Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF"}, {"paperId": "844bc3b26b5c63ec3b251ae634c194dcfb41a7d2", "title": "InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning"}, {"paperId": "a4c921bdef167ae54cc3a40643e6e3ed13d49a61", "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions"}, {"paperId": "396305230ddcf915b19a19683a89e34d76321a33", "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models"}, {"paperId": "b574245f3db22b5eb7fe64bd8b0a147dab467b60", "title": "RAIN: Your Language Models Can Align Themselves without Finetuning"}, {"paperId": "a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9", "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"}, {"paperId": "2efadc1c928c8d92756e573f371b8d46087865ed", "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning"}, {"paperId": "b549b74b70ab1123d6771a67d4912daef8e13a30", "title": "From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models"}, {"paperId": "147763dbb9b9b0363ed0479c4cfa5844c537c344", "title": "USA: Universal Sentiment Analysis Model & Construction of Japanese Sentiment Text Classification and Part of Speech Dataset"}, {"paperId": "ba5aefce80edc7da110f53fd071f4fbd6b5195b9", "title": "Framework-based qualitative analysis of free responses of Large Language Models: Algorithmic fidelity"}, {"paperId": "1c594adf08fc2ab2fbe5ec1f2468cd7eca73b587", "title": "Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?"}, {"paperId": "d00735241af700d21762d2f3ca00d920241a15a4", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"}, {"paperId": "26089bdfdbca1e6eaaceca71e3116b715bec6d47", "title": "Explainability for Large Language Models: A Survey"}, {"paperId": "f2b9830dc6e0818263ca046fbc604caf98a9e5b3", "title": "Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence"}, {"paperId": "894ed1aba8e42a4ec27ba53ecde383b14c5128ca", "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"}, {"paperId": "50a124ba06f63bb6d462a02a3f442af8b15bd0f5", "title": "The Poison of Alignment"}, {"paperId": "1ce1738d7f224ebd7ad98e23692404f06697b5f4", "title": "Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models"}, {"paperId": "e3052ebca5eeae6a8a73e44517903d39746f5f3a", "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning"}, {"paperId": "f8b90d640158f61c4553518a8554a73b540e07e7", "title": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models"}, {"paperId": "d7e92d03dfa5427c0c5ef2b59de54733e0589606", "title": "InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"}, {"paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f", "title": "Instruction Tuning for Large Language Models: A Survey"}, {"paperId": "51ef336bb1bb9875d715abb78be93b58f952cb5c", "title": "Dataset Quantization"}, {"paperId": "ed3873864a14ed4f9ee09d3cf70d1ead2fa2be10", "title": "LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models"}, {"paperId": "40e0b9361d88b1879891eb6d16de110b30bf6c62", "title": "OctoPack: Instruction Tuning Code Large Language Models"}, {"paperId": "96f6ad72733599db609332987ec6b65e30f11d07", "title": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs"}, {"paperId": "897940fb5dd4d739b88c4659c4565d05f48d06b8", "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"}, {"paperId": "d6c2523ab97416c2692cbbeab082ed1790e8e55e", "title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"}, {"paperId": "f2ba9e7d9624bd94a786ea5e3161a9425a21a475", "title": "Self-Alignment with Instruction Backtranslation"}, {"paperId": "c93bdb1a1ca567c46354a1d9d00054354c8b7704", "title": "PIPPA: A Partially Synthetic Conversational Dataset"}, {"paperId": "c74a13b251b6af6dfce49eeb128b1c0e2ddf955d", "title": "A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment"}, {"paperId": "9fec5cf2f06e6fd8c5e6f6028226082d1ecec5b7", "title": "Extrapolating Large Language Models to Non-English by Aligning Languages"}, {"paperId": "cd73c6870a28d13f553356c61c877b6ee684b5b4", "title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation"}, {"paperId": "a2cec5152e3af9d99a4a3b8128fc247885b4b1c3", "title": "In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning"}, {"paperId": "7f55ef29a6f8b2771c5435bbeba29c87264fdc88", "title": "Shepherd: A Critic for Language Model Generation"}, {"paperId": "a50d4fd8f584276c0fd8560255884edd57aa926e", "title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue"}, {"paperId": "c2f9006993d9d84d48eb894aab3ba60f946d0e15", "title": "EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education"}, {"paperId": "2cf57d6317eaecc905adde1a6adf4db313d9d808", "title": "Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks"}, {"paperId": "3afc1746ad41d1d563c2ae4bed76d1727417dc14", "title": "Towards Artificial Intelligence Applications in Next Generation Cytopathology"}, {"paperId": "84adc377933eb09289304f63644ce546de9a4a2b", "title": "Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?"}, {"paperId": "37665dd5ae7245f087d663785c17eef068578676", "title": "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection"}, {"paperId": "56ad2c35353b57653b5f14c33afca72fe21d2d20", "title": "CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools"}, {"paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623", "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"}, {"paperId": "e0ca43a635d35fd0414ee76ca1e7c287715f5b00", "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback"}, {"paperId": "7a5b44ea10a51708e18786595c8d70b18950da11", "title": "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios"}, {"paperId": "ca31b8584b6c022ef15ddfe994fe361e002b7729", "title": "A Comprehensive Overview of Large Language Models"}, {"paperId": "3e664adb009dce373129a3563e4b2cb08731bc76", "title": "PolyLM: An Open Source Polyglot Large Language Model"}, {"paperId": "130d18d1d455336e1a5b06c85784894bb67d87ec", "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations"}, {"paperId": "ab82cd7194fc609d2d29e6b1a4399d6120f97fdd", "title": "Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning"}, {"paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e", "title": "Preference Ranking Optimization for Human Alignment"}, {"paperId": "bac54736112098616f0e1c90435888ef3e119d32", "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey"}, {"paperId": "455866ca838f356b53a7e3e5b344834f9e93dbbc", "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases"}, {"paperId": "59cd274a64c37d5d02ff986ca88878abbadacfb1", "title": "covLLM: Large Language Models for COVID-19 Biomedical Literature"}, {"paperId": "fbd2c8089870814449f9254a711041bbae145a82", "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources"}, {"paperId": "1a57a0ca82924ac645529892bd5cba56440f6790", "title": "Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models"}, {"paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "title": "Large Language Models are not Fair Evaluators"}, {"paperId": "9132b8abd2b3e07422abc6fc6cdb7f6fbe4fd2b0", "title": "Taming AI Bots: Controllability of Neural States in Large Language Models"}, {"paperId": "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c", "title": "Scaling Data-Constrained Language Models"}, {"paperId": "ce0154d9251f67c262512b6e598f3aa3ba9fe9a4", "title": "Passive learning of active causal strategies in agents and language models"}, {"paperId": "6942bde24d01c412bdd53414cc88459afa5fac7d", "title": "Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions"}, {"paperId": "5af9cf0b695faf2eb94d74bf76dab1a311638ca3", "title": "Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science"}, {"paperId": "7782055e33ee248ef47b1415624c2efb7a7e8410", "title": "GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP"}, {"paperId": "49cbfa5848aaa74ce01d29ee5328f1a2b466ce27", "title": "David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs"}, {"paperId": "a122863d239643453195424c04067e89406246e1", "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"}, {"paperId": "b82c1b0512d25307e3c81bb8d9df1607267a7a52", "title": "MemeCap: A Dataset for Captioning and Interpreting Memes"}, {"paperId": "5b8f0460d408a8688d9ee0cba127c779d3291d99", "title": "Aligning Large Language Models through Synthetic Feedback"}, {"paperId": "758985395372f5378fcf036094195b2848e13a21", "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning"}, {"paperId": "3f4a44b41f612c4f417adb085daa43623194d9f9", "title": "Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance"}, {"paperId": "e373a309f6a9be59605317b8cee92fa710c31889", "title": "InstructIE: A Bilingual Instruction-based Information Extraction Dataset"}, {"paperId": "74b05bba46db21e589a2cc0f916f81069b0368ef", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"}, {"paperId": "807f4a368e845f7649e645f91d1d4f2ce72beee3", "title": "A Comprehensive Survey on Instruction Following"}, {"paperId": "1d75f8de31bf47ec46fa5586056420ec8bc97e86", "title": "Using In-Context Learning to Improve Dialogue Safety"}, {"paperId": "a85ae093657f62bf13bf18b0652402026dd4186d", "title": "DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset"}, {"paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "title": "Can large language models reason about medical questions?"}, {"paperId": "b802df98ab31445df9770f0231b7e459b26f2fdd", "title": "Chain-of-LoRA: Enhancing the Instruction Fine-Tuning Performance of Low-Rank Adaptation on Diverse Instruction Set"}, {"paperId": "fa18ab4e2df14e5e8c83f95094fe83a1a3051fb0", "title": "Donkii: Characterizing and Detecting Errors in Instruction-Tuning Datasets"}, {"paperId": "ac771182d1780c863954243809d1e144433919f9", "title": "Aligning Large Language Models with Human: A Survey"}, {"paperId": "e730164e17975547564a1eaa70cea5884b16c89d", "title": "Instruction : Translate the phrase \u201d Bonne chance \u201d into English Response : Good Luck"}, {"paperId": null, "title": "2 3 2 2 3 2 8 2 3 2 1 2 1 0 2 0 2 2 0 0 2 6 4 2 2 1 1 2 1 2 1 0 1"}, {"paperId": "3d473cbb7a377cf960abff31748a1a39bb6c7d7c", "title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"}, {"paperId": "c477438beee6dc81b01f74c3684fde5450cd5c1d", "title": "Controlled Data Augmentation for Training Task-Oriented Dialog Systems with Low Resource Data"}, {"paperId": "9ee8e2e6c8b608d043065f4ac092c58f4c67b878", "title": "Analyzing Pre-trained and Fine-tuned Language Models"}, {"paperId": "743098ad19987faa102ba9d6e81e469b2e4e8511", "title": "M ETA T OOL B ENCHMARK : D ECIDING W HETHER TO U SE T OOLS AND W HICH TO U SE"}, {"paperId": "b9b997baec9f7a93ba0564930be5e7f9b581f725", "title": "The new reality of education in the face of advances in generative artificial intelligence"}, {"paperId": "153f5b66a3fcc9dbdff4e49c11468c06f32b1d25", "title": "Reinforcement Learning with Human Feedback (RLHF)"}]}
