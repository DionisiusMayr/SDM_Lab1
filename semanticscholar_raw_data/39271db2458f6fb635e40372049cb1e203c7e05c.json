{"paperId": "39271db2458f6fb635e40372049cb1e203c7e05c", "publicationVenue": {"id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e", "name": "Proceedings of the VLDB Endowment", "type": "journal", "alternate_names": ["Proceedings of The Vldb Endowment", "Proc VLDB Endow", "Proc Vldb Endow"], "issn": "2150-8097", "url": "http://dl.acm.org/toc.cfm?id=J1174", "alternate_urls": ["http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"]}, "title": "SANCUS: Staleness-Aware Communication-Avoiding Full-Graph Decentralized Training in Large-Scale Graph Neural Networks", "abstract": "Graph neural networks (GNNs) have emerged due to their success at modeling graph data. Yet, it is challenging for GNNs to efficiently scale to large graphs. Thus, distributed GNNs come into play. To avoid communication caused by expensive data movement between workers, we propose Sancus, a staleness-aware communication-avoiding decentralized GNN system. By introducing a set of novel bounded embedding staleness metrics and adaptively skipping broadcasts, Sancus abstracts decentralized GNN processing as sequential matrix multiplication and uses historical embeddings via cache. Theoretically, we show bounded approximation errors of embeddings and gradients with convergence guarantee. Empirically, we evaluate Sancus with common GNN models via different system setups on large-scale benchmark datasets. Compared to SOTA works, Sancus can avoid up to 74% communication with at least 1.86X faster throughput on average without accuracy loss.", "venue": "Proceedings of the VLDB Endowment", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-01", "journal": {"name": "Proc. VLDB Endow.", "pages": "1937-1950", "volume": "15"}, "authors": [{"authorId": "51112671", "name": "Jingshu Peng"}, {"authorId": "2111605555", "name": "Zhao Chen"}, {"authorId": "2237813", "name": "Yingxia Shao"}, {"authorId": "2115435801", "name": "Yanyan Shen"}, {"authorId": null, "name": "Lei Chen"}, {"authorId": "144115026", "name": "Jiannong Cao"}], "citations": [{"paperId": "81b2834f3f884785e94a8c4c1b0bf66f858e0d97", "title": "The Image Calculator: 10x Faster Image-AI Inference by Replacing JPEG with Self-designing Storage Format"}, {"paperId": "c3364b1fb446763cf56d5532f2ded29076e6d01f", "title": "Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling"}, {"paperId": "549d440dd1a8f54fb9022b47118685c1c4885f17", "title": "ADGNN: Towards Scalable GNN Training with Aggregation-Difference Aware Sampling"}, {"paperId": "6e419d87084f8cc153f79f6c399e48092ba237b0", "title": "HongTu: Scalable Full-Graph GNN Training on Multiple GPUs"}, {"paperId": "254c54a44391018ce432e3aac0b34895cc10c287", "title": "GraNNDis: Efficient Unified Distributed Training Framework for Deep GNNs on Large Clusters"}, {"paperId": "fe363b831be5132454ec358421496fb072747aae", "title": "NeutronStream: A Dynamic GNN Training Framework with Sliding Window for Graph Streams"}, {"paperId": "c07cf05a0fa90fa6207aa07912131646198066e0", "title": "Helios: An Efficient Out-of-core GNN Training System on Terabyte-scale Graphs with In-memory Performance"}, {"paperId": "d43d6e4bd9dca7ebfd0978ebfa29b40d1c4b08c0", "title": "DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks"}, {"paperId": "75f977e49f59a1758a65e6863c1b32a511741871", "title": "Accelerating Distributed GNN Training by Codes"}, {"paperId": "7bc76d815ae3bf267a970d2244c3b3730336977c", "title": "GNNPipe: Scaling Deep GNN Training with Pipelined Model Parallelism"}, {"paperId": "e980ae1d33a182dc0724348a08c33f0f8a028626", "title": "Lotan: Bridging the Gap between GNNs and Scalable Graph Analytics Engines"}, {"paperId": "894d61c709ec6f61899703458d90b09c663d7b11", "title": "A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware"}, {"paperId": "5acf75e1ee88a9695230708c3d6b11512583fefa", "title": "DUCATI: A Dual-Cache Training System for Graph Neural Networks on Giant Graphs with the GPU"}, {"paperId": "3229291b9b7e9a7200b543089e05a4442cbe6a9b", "title": "Scalable and Efficient Full-Graph GNN Training for Large Graphs"}, {"paperId": "40ae77b768a14693768a39c884ef5f9d1cd9e345", "title": "Link Local Differential Privacy in GNNs via Bayesian Estimation"}, {"paperId": "5aac333b25158c4f330474608c93f461f7ae6689", "title": "Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training"}, {"paperId": "99bae8cf92c006bec0255058a0f1153fc3292e5c", "title": "Mitigating Query-based Neural Network Fingerprinting via Data Augmentation"}, {"paperId": "ac813cf335b01a88d57e0fbaf8d78ab79ea62fc9", "title": "Learning Graph Neural Networks using Exact Compression"}, {"paperId": "ddda24087acfd9ed48b45efa8f387b37d83caea8", "title": "SUREL+: Moving from Walks to Sets for Scalable Subgraph-based Graph Representation Learning"}, {"paperId": "46210e170045df3c0c50a17bb63e6de480d62f9d", "title": "FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training"}, {"paperId": "5205f96c6894b1b1fed6683d0c67f4b1f3b25b45", "title": "Scalable Graph Convolutional Network Training on Distributed-Memory Systems"}, {"paperId": "aedd6853125ea7d0fe2dbcbe7e4f19699bc490d4", "title": "DistGNN-MB: Distributed Large-Scale Graph Neural Network Training on x86 via Minibatch Sampling"}, {"paperId": "1e79e33c77b2d8eaf643af0e1f5003057d7356b2", "title": "Distributed Graph Neural Network Training: A Survey"}, {"paperId": "4978520a959b103f9dd55ec3ce4545ab06191a18", "title": "Good Intentions: Adaptive Parameter Management via Intent Signaling"}, {"paperId": "1c63814a9ca53984330e23050b73c9abd0ebdc30", "title": "GNNPipe: Accelerating Distributed Full-Graph GNN Training with Pipelined Model Parallelism"}]}
