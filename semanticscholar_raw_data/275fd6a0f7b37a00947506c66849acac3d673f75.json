{"paperId": "275fd6a0f7b37a00947506c66849acac3d673f75", "publicationVenue": {"id": "7c9d091e-015e-4e5d-a11f-9bc369fcf414", "name": "IEEE Transactions on Parallel and Distributed Systems", "type": "journal", "alternate_names": ["IEEE Trans Parallel Distrib Syst"], "issn": "1045-9219", "url": "http://www.computer.org/tpds", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=71"]}, "title": "ETICA: Efficient Two-Level I/O Caching Architecture for Virtualized Platforms", "abstract": "In recent years, increased I/O demand of <italic>Virtual Machines</italic> (VMs) in large-scale data centers and cloud computing has encouraged system architects to design high-performance storage systems. One common approach to improving performance is to employ fast storage devices such as <italic>Solid-State Drives</italic> (SSDs) as an I/O caching layer for slower storage devices. SSDs provide high performance, especially on random requests, but they also have limited endurance: they support <italic>only</italic> a limited number of write operations and can therefore <italic>wear out</italic> relatively fast due to write operations. In addition to the write requests generated by the applications, each read miss in the SSD cache is served at the cost of imposing a write operation to the SSD (to copy the data block into the cache), resulting in an even larger number of writes into the SSD. Previous I/O caching schemes on virtualized platforms <italic>only</italic> partially mitigate the endurance limitations of SSD-based I/O caches; they mainly focus on assigning efficient cache write policies and cache space to the VMs. Moreover, existing cache space allocation schemes have inefficiencies: they <italic>do not</italic> take into account the impact of cache write policy in reuse distance calculation of the running workloads and hence, reserve cache blocks for accesses that would <italic>not</italic> be served by cache. In this article, we propose an <italic><underline>E</underline>fficient <underline>T</underline>wo-Level <underline>I</underline>/O <underline>C</underline>aching <underline>A</underline>rchitecture</italic> (ETICA) for virtualized platforms that can significantly improve I/O latency, endurance, and cost (in terms of cache size) while preserving the reliability of write-pending data blocks. As opposed to previous <italic>one-level</italic> I/O caching schemes in virtualized platforms, our proposed architecture 1) provides <italic>two</italic> levels of cache by employing both <italic>Dynamic Random-Access Memory</italic> (DRAM) and SSD in the I/O caching layer of virtualized platforms and 2) effectively partitions the cache space between running VMs to achieve maximum performance and minimum cache size. To manage the two-level cache, unlike the previous reuse distance calculation schemes such as <italic>Useful Reuse Distance</italic> (URD), which only consider the request type and neglect the impact of <italic>cache write policy</italic>, we propose a new metric, <italic>Policy Optimized reuse Distance</italic> (POD). The key idea of POD is to effectively calculate the reuse distance and estimate the amount of two-level DRAM+SSD cache space to allocate by considering both 1) the request type and 2) the cache write policy. Doing so results in enhanced performance and reduced cache size due to the allocation of cache blocks <italic>only</italic> for the requests that would be served by the I/O cache. ETICA maintains the reliability of write-pending data blocks and improves performance by 1) assigning an effective and fixed write policy at each level of the I/O cache hierarchy and 2) employing effective promotion and eviction methods between cache levels. Our extensive experiments conducted with a real implementation of the proposed two-level storage caching architecture show that ETICA provides 45 percent higher performance, compared to the state-of-the-art caching schemes in virtualized platforms, while improving both cache size and SSD endurance by 51.7 and 33.8 percent, respectively.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-14", "journal": {"name": "IEEE Transactions on Parallel and Distributed Systems", "pages": "2415-2433", "volume": "32"}, "authors": [{"authorId": "40914288", "name": "Saba Ahmadian"}, {"authorId": "2894539", "name": "Reza Salkhordeh"}, {"authorId": "145929920", "name": "O. Mutlu"}, {"authorId": "144394335", "name": "H. Asadi"}], "citations": [{"paperId": "e3df6c77d2c9e66958714c30f5847c19bdcd7442", "title": "Online Monte Carlo Planning with QoS Subgoals for Data Caching in ITS MEC Networks"}, {"paperId": "454df4f4d3508aa96a6392a1eca6da347bf70e57", "title": "Video File Allocation for Wear-Leveling in Distributed Storage Systems With Heterogeneous Solid-State-Disks (SSDs)"}, {"paperId": "4ef0a7257bba4413709abe66771902efac337897", "title": "An Enterprise-Grade Open-Source Data Reduction Architecture for All-Flash Storage Systems"}, {"paperId": "1a1e2efbbccc340a9040e53cbed3d18745ff9e11", "title": "Cache Replacement Algorithm Based on Dynamic Constraints in Microservice Platform"}, {"paperId": "d23f289655fa9d1ccfe5193db4135553a6dd7d5a", "title": "PADSA: Priority-Aware Block Data Storage Architecture for Edge Cloud Serving Autonomous Vehicles"}]}
