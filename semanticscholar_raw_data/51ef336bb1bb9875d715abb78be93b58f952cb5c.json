{"paperId": "51ef336bb1bb9875d715abb78be93b58f952cb5c", "publicationVenue": {"id": "7654260e-79f9-45c5-9663-d72027cf88f3", "name": "IEEE International Conference on Computer Vision", "type": "conference", "alternate_names": ["ICCV", "IEEE Int Conf Comput Vis", "ICCV Workshops", "ICCV Work"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"}, "title": "Dataset Quantization", "abstract": "State-of-the-art deep neural networks are trained with large amounts (millions or even billions) of data. The expensive computation and memory costs make it difficult to train them on limited hardware resources, especially for recent popular large language models (LLM) and computer vision models (CV). Recent popular dataset distillation methods are thus developed, aiming to reduce the number of training samples via synthesizing small-scale datasets via gradient matching. However, as the gradient calculation is coupled with the specific network architecture, the synthesized dataset is biased and performs poorly when used for training unseen architectures. To address these limitations, we present dataset quantization (DQ), a new framework to compress large-scale datasets into small subsets which can be used for training any neural network architectures. Extensive experiments demonstrate that DQ is able to generate condensed small datasets for training unseen network architectures with state-of-the-art compression ratios for lossless model training. To the best of our knowledge, DQ is the first method that can successfully distill large-scale datasets such as ImageNet-1k with a state-of-the-art compression ratio. Notably, with 60% data from ImageNet and 20% data from Alpaca\u2019s instruction tuning data, the models can be trained with negligible or no performance drop for both vision tasks (including classification, semantic segmentation, and object detection) as well as language tasks (including instruction tuning tasks such as BBH and DROP).", "venue": "IEEE International Conference on Computer Vision", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-08-21", "journal": {"name": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)", "pages": "17159-17170"}, "authors": [{"authorId": "18119920", "name": "Daquan Zhou"}, {"authorId": "151491771", "name": "Kaixin Wang"}, {"authorId": "123902785", "name": "Jianyang Gu"}, {"authorId": "2115815502", "name": "Xiang Peng"}, {"authorId": "35180251", "name": "Dongze Lian"}, {"authorId": "2108464858", "name": "Yifan Zhang"}, {"authorId": "2147330258", "name": "Yang You"}, {"authorId": "1698982", "name": "Jiashi Feng"}], "citations": [{"paperId": "45493689a83c3fc7ded37986cf5e7274ead307cc", "title": "FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models"}, {"paperId": "446a1fdaf86558061d5b24a6ad796b6e06de06cd", "title": "One Category One Prompt: Dataset Distillation using Diffusion Models"}, {"paperId": "7d19b58e6bbfe9a0d39577a328eb4d58d8f5f584", "title": "Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching"}, {"paperId": "c991dedacba67949a28640cd8755de4c8ae297b0", "title": "A Survey on Data Selection for LLM Instruction Tuning"}, {"paperId": "ebbe8dac9e4df3864828489ec45163f147e5b317", "title": "Dataset Distillation in Latent Space"}, {"paperId": "6435fc5f14591f02e99d3f308ae5f21d727748de", "title": "Does Graph Distillation See Like Vision Dataset Counterpart?"}, {"paperId": "701def2dc909eb049f22dbcb352b04f583d27518", "title": "Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching"}, {"paperId": "2d5b783c5fc19abdfafe764e0625a305c5cdde0e", "title": "Can pre-trained models assist in dataset distillation?"}, {"paperId": "69d3ebaa6dd84e6e30c384060e311a0027226457", "title": "Dataset Condensation via Generative Model"}, {"paperId": "631cee335dbae8f883f426b119686058c4b26951", "title": "Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models"}, {"paperId": "97d680e7de233e4a3023415d675afb6d681768e1", "title": "DREAM: Efficient Dataset Distillation by Representative Matching"}, {"paperId": "610c2022a96ec7aef7b8716fb90d806b5e94df51", "title": "Expanding Small-Scale Datasets with Guided Imagination"}]}
