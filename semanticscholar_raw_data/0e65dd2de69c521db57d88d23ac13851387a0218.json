{"paperId": "0e65dd2de69c521db57d88d23ac13851387a0218", "publicationVenue": null, "title": "Modelo para estimar performance de um Cluster Hadoop", "abstract": "The volume, variety and velocity of data presents a great challenge to extracting useful information in a timely manner, without causing impacts on other existing processes in organizations, promoting the use of clusters for storage and processing, and the use of cloud computing. This a good scenario for the Hadoop an open source framework scalable and e cient for running workloads on Big Data. With the advent of cloud computing one cluster with Hadoop framework can be allocated in minutes, however, ensure that the Hadoop has a good performance to accomplish their processing has several challenges, such as needs tweaking the settings of Hadoop for their workloads, allocate a cluster with the necessary resources to perform certain processes and de ne the resources required to perform processing in a known time interval. In this work, an approach that seeks to optimize the Hadoop for a given workload and estimate the computational resources required to realize a processing in a given time interval was proposed. The approach is based on collecting information, based rules for adjusting Hadoop settings for certain workload and simulations. The simplicity and lightness of the model allows the solution be adopted how a facilitator to overcome the challenges presented by Big Data, and facilitate the use of the Hadoop, even by users with little IT experience. The proposed model works with the MapReduce to de ne the main con guration parameters and determine the computational resources of nodes of cluster, to meet the desired runtime for a given workload requirements.", "venue": "", "year": 2014, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2014-12-02", "journal": {"name": "", "volume": ""}, "authors": [{"authorId": "134139219", "name": "J. D. Brito"}], "citations": []}
