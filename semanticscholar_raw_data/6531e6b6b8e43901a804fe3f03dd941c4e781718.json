{"paperId": "6531e6b6b8e43901a804fe3f03dd941c4e781718", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Collaborative Large Language Model for Recommender Systems", "abstract": "Recently, there is a growing interest in developing next-generation recommender systems (RSs) based on pretrained large language models (LLMs), fully utilizing their encoded knowledge and reasoning ability. However, the semantic gap between natural language and recommendation tasks is still not well addressed, leading to multiple issues such as spuriously-correlated user/item descriptors, ineffective language modeling on user/item contents, and inefficient recommendations via auto-regression, etc. In this paper, we propose CLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RS, aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model the user/item collaborative and content semantics. Accordingly, in the pretraining stage, a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora established from user-item interactions and user/item features, where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens that facilitates stable and effective language modeling. In addition, a novel mutual regularization strategy is introduced to encourage the CLLM4Rec to capture recommendation-oriented information from user/item contents. Finally, we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on the soft+hard prompts established from masked user-item interaction history, where recommendations of multiple items can be generated efficiently.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-02", "journal": {"name": "ArXiv", "volume": "abs/2311.01343"}, "authors": [{"authorId": "2261804201", "name": "Yaochen Zhu"}, {"authorId": "2264480350", "name": "Liang Wu"}, {"authorId": "2170992709", "name": "Qilnli Guo"}, {"authorId": "2264620213", "name": "Liangjie Hong"}, {"authorId": "2261788139", "name": "Jundong Li"}], "citations": [{"paperId": "9433efaee928bad3524d6b21184db270b85c521b", "title": "A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation"}, {"paperId": "f15086f851680e1c05f1eb871436ff66c838b907", "title": "Foundation Models for Recommender Systems: A Survey and New Perspectives"}, {"paperId": "14c7747267c875713be896cc6a3d5bbc16ccb015", "title": "Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review"}, {"paperId": "156af81de93f840df39c0973ef1343629427a7db", "title": "Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis"}, {"paperId": "9d2c9a7c3ff8f760b43fdd6a7082069a467c8956", "title": "Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation"}, {"paperId": "bac54736112098616f0e1c90435888ef3e119d32", "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey"}]}
