{"paperId": "d2a8294f1aede0ca6a89920475dc780e918a79aa", "publicationVenue": {"id": "61275a16-1e0d-479f-ac4e-f295310761f0", "name": "ACM Conference on Recommender Systems", "type": "conference", "alternate_names": ["Conf Recomm Syst", "RecSys", "ACM Conf Recomm Syst", "Conference on Recommender Systems"], "url": "http://recsys.acm.org/"}, "title": "InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models", "abstract": "Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- & time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning (DL) training jobs are dominated by model execution times, the most important factor in DLRM training performance is often online data ingestion. In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into the specific bottlenecks and challenges of the DLRM training pipeline at scale. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to both observe the performance impacts of online ingestion and to identify shortfalls in existing data pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune. InTune employs a reinforcement learning (RL) agent to learn how to distribute the CPU resources of a trainer machine across a DLRM data pipeline to more effectively parallelize data-loading and improve throughput. Our experiments show that InTune can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune achieves significantly higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus current state-of-the-art data pipeline optimizers while also improving both CPU & GPU utilization.", "venue": "ACM Conference on Recommender Systems", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2023-08-13", "journal": {"name": "Proceedings of the 17th ACM Conference on Recommender Systems"}, "authors": [{"authorId": "2003802919", "name": "Kabir Nagrecha"}, {"authorId": "2146022458", "name": "Lingyi Liu"}, {"authorId": "153654693", "name": "P. Delgado"}, {"authorId": "2231699049", "name": "Prasanna Padmanabhan"}], "citations": [{"paperId": "4fdc9900f9d8da4c46eac7b572d6b02da8da080c", "title": "Towards a Systems Theory of Algorithms"}, {"paperId": "3a29a9e6168cf1620fe76b3702e473de6f6fec67", "title": "Saturn: An Optimized Data System for Multi-Large-Model Deep Learning Workloads"}]}
