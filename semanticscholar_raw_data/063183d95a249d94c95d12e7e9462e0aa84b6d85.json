{"paperId": "063183d95a249d94c95d12e7e9462e0aa84b6d85", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models", "abstract": "All AI models are susceptible to learning biases in data that they are trained on. For generative dialogue models, being trained on real human conversations containing unbalanced gender and race/ethnicity references can lead to models that display learned biases, which we define here broadly as any measurable differences in the distributions of words or semantic content of conversations based on demographic groups. We measure the strength of such biases by producing artificial conversations between two copies of a dialogue model, conditioning one conversational partner to state a name commonly associated with a certain gender and/or race/ethnicity. We find that larger capacity models tend to exhibit more gender bias and greater stereotyping of occupations by gender. We show that several methods of tuning these dialogue models, specifically name scrambling, controlled generation, and unlikelihood training, are effective in reducing bias in conversation, including on a downstream conversational task. Name scrambling is also effective in lowering differences in token usage across conversations where partners have names associated with different genders or races/ethnicities.", "venue": "arXiv.org", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-07", "journal": {"name": "ArXiv", "volume": "abs/2109.03300"}, "authors": [{"authorId": "51324296", "name": "Eric Michael Smith"}, {"authorId": "81840293", "name": "Adina Williams"}], "citations": [{"paperId": "99b1fd7dd675ee0c4c1ee69eccb9c415d1998fdc", "title": "Identifying and Improving Disability Bias in GAI-Based Resume Screening"}, {"paperId": "4560e2a2be6b4de619308284bb4e01020d8384f2", "title": "Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision"}, {"paperId": "33ba6ff1d2b599178e60d029da10b41f7a3c4729", "title": "\"One-Size-Fits-All\"? Examining Expectations around What Constitute\"Fair\"or\"Good\"NLG System Behaviors"}, {"paperId": "996a5fe1906dbef0a439d0f8ccc505c2a59e357e", "title": "Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems"}, {"paperId": "a3692592f4f19ea5ab3ee837afd29fb9c550bad2", "title": "Generative AI May Prefer to Present National-level Characteristics of Cities Based on Stereotypical Geographic Impressions at the Continental Level"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "0b6edce3dde7e502c6b7c6d83bac0230ec912482", "title": "Improving Open Language Models by Learning from Organic Interactions"}, {"paperId": "969559ec5fcdb98dc5690640b2560d8df6fa9591", "title": "Reducing Sensitivity on Speaker Names for Text Generation from Dialogues"}, {"paperId": "f0e9998ddec97b5275eada7308104d867a1dda19", "title": "This Prompt is Measuring : Evaluating Bias Evaluation in Language Models"}, {"paperId": "f78fe02f681a0a9a6867b007bd39e3884de64a91", "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization"}, {"paperId": "a9a53c28f3b964cf561c05bf204b4c06f6454eec", "title": "Bipartite-play Dialogue Collection for Practical Automatic Evaluation of Dialogue Systems"}, {"paperId": "a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd", "title": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage"}, {"paperId": "030b4834bb553e229fb627285fc5407f4434498b", "title": "AugLy: Data Augmentations for Adversarial Robustness"}, {"paperId": "011095a0082e5e301f9bf30267b193c1c9e7e370", "title": "Perturbation Augmentation for Fairer NLP"}, {"paperId": "8c90bfe05c06fd47eaec0f5b1662e06862572afe", "title": "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements"}, {"paperId": "7ef43bacd43393ff116e6fcda6a52a6902e016d7", "title": "\u201cI\u2019m sorry to hear that\u201d: Finding New Biases in Language Models with a Holistic Descriptor Dataset"}, {"paperId": "cfe166f0916d2fd251d1f735fc85f6ab77aedca8", "title": "AugLy: Data Augmentations for Robustness"}, {"paperId": "8a6bda5739c9c975b49327b9fe891d908fdfa951", "title": "A Word on Machine Ethics: A Response to Jiang et al. (2021)"}, {"paperId": "ab06a5c808bfd10680057b8b9899e669bcbc3e51", "title": "\"I'm sorry to hear that\": finding bias in language models with a holistic descriptor dataset"}, {"paperId": "f19298ea19dcd7bd69ab76cf6a18f801052e26e4", "title": "On the Machine Learning of Ethical Judgments from Natural Language"}]}
