{"paperId": "f56cda7ee6b3cfa427d045b6cc754ec68349c511", "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods in Natural Language Processing", "type": "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts", "abstract": "Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-26", "journal": {"name": "ArXiv", "volume": "abs/2108.11830"}, "authors": [{"authorId": "3458166", "name": "Ashutosh Baheti"}, {"authorId": "2729164", "name": "Maarten Sap"}, {"authorId": "1863425", "name": "Alan Ritter"}, {"authorId": "2065904932", "name": "Mark O. Riedl"}], "citations": [{"paperId": "dfec3613435e108667eb1bd67e1945c82a712fbb", "title": "Stanceosaurus 2.0 - Classifying Stance Towards Russian and Spanish Misinformation"}, {"paperId": "6ecb6f2c4544e3c7f8dfd33a8e2f14735653b6ef", "title": "Improving Dialog Safety using Socially Aware Contrastive Learning"}, {"paperId": "f2a4c6e3e62aee692cfcf8bf6f08efe8fb44a751", "title": "Are authorities denying or supporting? Detecting stance of authorities towards rumors in Twitter"}, {"paperId": "d8785264bbce47ca1ea7f97e7f3bc4ca6cbe824c", "title": "A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation"}, {"paperId": "aac83deb068875d2dffb58b2c8fdeffc6ad2a38a", "title": "A First Look at Toxicity Injection Attacks on Open-domain Chatbots"}, {"paperId": "1658674b715e66ef3a8cf24369a1d1691580f4a9", "title": "CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs"}, {"paperId": "ac1ce912a8a3614ad4c5f467eca8f20b315a3428", "title": "Rethinking Conversational Agents in the Era of LLMs: Proactivity, Non-collaborativity, and Beyond"}, {"paperId": "b1d2a29860e69c6ce9987ddefbe112feb1efa16a", "title": "Large Language Models can Share Images, Too!"}, {"paperId": "3c7b23b343eb24f0662c8b9033ec1f2d15cc4c27", "title": "EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset"}, {"paperId": "451a657dabf80ebc43f6a3be518250b2cd5dfe1a", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models"}, {"paperId": "8e1d9e6e81d41ab3b598298835a233474892504e", "title": "A Benchmark for Understanding Dialogue Safety in Mental Health Support"}, {"paperId": "49f97e7b63b0c6e592e256c0d47eb1f4150ad7e1", "title": "Proactive Conversational Agents in the Post-ChatGPT World"}, {"paperId": "258934fa8c76d468f1ff9fa5cd6df84b16985dee", "title": "Enhancing Offensive Language Detection with Data Augmentation and Knowledge Distillation"}, {"paperId": "1c296df26baf7daa72eba1619a1bd15b3168fa48", "title": "Improved Instruction Ordering in Recipe-Grounded Conversation"}, {"paperId": "e3e8c75e9c34c7538754cb83377f7e68a5aeaf6b", "title": "Healing Unsafe Dialogue Responses with Weak Supervision Signals"}, {"paperId": "60d90e96e7c434861697194fa47f1978d86b9d28", "title": "Leftover-Lunch: Advantage-based Offline Reinforcement Learning for Language Models"}, {"paperId": "969559ec5fcdb98dc5690640b2560d8df6fa9591", "title": "Reducing Sensitivity on Speaker Names for Text Generation from Dialogues"}, {"paperId": "744a98cc2736fa71d3984602e10b68319a47c65e", "title": "BiasAsker: Measuring the Bias in Conversational AI System"}, {"paperId": "c01bd362596abd2b9c6bcbed783a19e4a94a586c", "title": "A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects"}, {"paperId": "297273d2a3f696c0811a3274c0abf23490a89de7", "title": "Transcending the \u201cMale Code\u201d: Implicit Masculine Biases in NLP Contexts"}, {"paperId": "861761d8c14c824a4dcb9a39a8f2d5871575916b", "title": "Proactive Conversational Agents"}, {"paperId": "7c3025bd4cd8f3064ce481c95124a5102d9307b7", "title": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection"}, {"paperId": "1d75f8de31bf47ec46fa5586056420ec8bc97e86", "title": "Using In-Context Learning to Improve Dialogue Safety"}, {"paperId": "9f102146a8c6fe6ba4910be96d54331aa7b491d9", "title": "Language Model Detoxification in Dialogue with Contextualized Stance Control"}, {"paperId": "f78fe02f681a0a9a6867b007bd39e3884de64a91", "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization"}, {"paperId": "8616da9215843353f2169916766054dfbd50a671", "title": "DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines"}, {"paperId": "1ee9231e160121d8b27f9739b75db9cba58ed05c", "title": "Stanceosaurus: Classifying Stance Towards Multicultural Misinformation"}, {"paperId": "59e0ef773d2383875c6711f136e7159af06dcdbb", "title": "Language Detoxification with Attribute-Discriminative Latent Space"}, {"paperId": "f2e6410a9e01a2f05dc293557916a574167a6d65", "title": "The State of Profanity Obfuscation in Natural Language Processing"}, {"paperId": "655a45678f2522d406c7606549bb3cf09683f385", "title": "Reducing Offensive Replies in Open Domain Dialogue Systems"}, {"paperId": "8fa138f5869c2ce3b365cdccbdef2738d9a8aa96", "title": "StEduCov: An Explored and Benchmarked Dataset on Stance Detection in Tweets towards Online Education during COVID-19 Pandemic"}, {"paperId": "36c50e6638dddc8324eef9bfa064bfcab80cbef4", "title": "ProsocialDialog: A Prosocial Backbone for Conversational Agents"}, {"paperId": "8f926c0c3f1557a9241b7e75609082a1f207a75e", "title": "InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning"}, {"paperId": "e2c16d93bca8822b571dd388af4e309069d1a373", "title": "Target-Guided Dialogue Response Generation Using Commonsense and Data Augmentation"}, {"paperId": "7ef43bacd43393ff116e6fcda6a52a6902e016d7", "title": "\u201cI\u2019m sorry to hear that\u201d: Finding New Biases in Language Models with a Holistic Descriptor Dataset"}, {"paperId": "ee5a743129e5785b92aff156a947ca8c6beabbbc", "title": "Robust Conversational Agents against Imperceptible Toxicity Triggers"}, {"paperId": "4ae200e3e33045130f7abd1d38a82a8355dc6273", "title": "PANGUBOT: Efficient Generative Dialogue Pre-training from Pre-trained Language Model"}, {"paperId": "a63e3a10d094ecce8d9021767537c30ee5aaaa9b", "title": "Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models"}, {"paperId": "64acb0084c9dd8f454421ccc9289c1e389d7fcb8", "title": "Towards Identifying Social Bias in Dialog Systems: Framework, Dataset, and Benchmark"}, {"paperId": "a361b203fb2485bfbc092d65625e25a1df22c4c1", "title": "Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models"}, {"paperId": "3664cceee40f91c75d1ee2d2e17a050e846e52bb", "title": "COLD: A Benchmark for Chinese Offensive Language Detection"}, {"paperId": "38e772f62b0974597836b4e893bc6f3e9b51e3cf", "title": "Partner Personas Generation for Diverse Dialogue Generation"}, {"paperId": "35eb6755b63c45ff44d05ace8ea1b3b8c8db9eee", "title": "On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark"}, {"paperId": "34be38f7f18f3fb4a58256ddf96365f2934551dd", "title": "Revealing Persona Biases in Dialogue Systems"}, {"paperId": "2ae108f21571ad13bd7dc4a43ae263025dce4879", "title": "SafeConv: Explaining and Correcting Conversational Unsafe Behavior"}, {"paperId": "6abededed5b4eb1bdeba0fa53db5ae37526882ef", "title": "Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond"}, {"paperId": "6cb0efa435994a415bc090e5ab5a0ab5a6085897", "title": "Toward the Reliability of Dialog Systems"}, {"paperId": "f139e9c24dc1f7b6a046dd4b81c3a8a72f2dec33", "title": "InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning"}, {"paperId": "315320de0c8e8c30a5758ba7966c8fe5c935b82a", "title": "Automating Counterspeech in Dialogue Systems"}, {"paperId": "64a8effea41797ad725f865c0bea9fc7d6869910", "title": "On Controlling Fallback Responses for Grounded Dialogue Generation"}, {"paperId": "96ebc81050b403dc7d69c8ce99551609b1b4159e", "title": "Improving Multi-label Malevolence Detection in Dialogues through Multi-faceted Label Correlation Enhancement"}, {"paperId": "29fb34f80b0e42da74e0d451b8fdaf2e4f4e8375", "title": "Classifying and Automatically Neutralizing Hate Speech with Deep Learning Ensembles and Dataset Ensembles"}, {"paperId": "eb1ac44bbc0fe07c5f31f459c7199211239e90b8", "title": "Open-domain Dialogue Generation: What We Can Do, Cannot Do, And Should Do Next"}, {"paperId": "08f7aeca8f94bedbeb425bef1f0b3fcd9361f785", "title": "SafetyKit: First Aid for Measuring Safety in Open-domain Conversational Systems"}, {"paperId": "cb9082315a417b301282c33d547ba13d76d80993", "title": "Towards Automatic Generation of Messages Countering Online Hate Speech and Microaggressions"}, {"paperId": "26cf6d2e72821471597cc9e6d664b5a946b20fb1", "title": "Partner Personas Generation for Dialogue Response Generation"}, {"paperId": "e7289607fd5047ba68ab1ae3aca6931226e149de", "title": "PERSONACHATGEN: Generating Personalized Dialogues using GPT-3"}, {"paperId": "5e251503c1ce484cc50accac7b0ad695f32c1a91", "title": "Primary Program Committees"}, {"paperId": "6626dadc76d1af9d19fc4c2a4fa3a4cf414e62e0", "title": "Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks"}, {"paperId": "378d987bb204dc230f5e20cbe7bc90acb21730d9", "title": "BiasAsker: Testing Social Biases in Dialog Systems"}, {"paperId": "9eae922928b13ef6111da6d60e855e71c3848396", "title": "S AFETY B ENCH : Identifying Safety-Sensitive Situations for Open-domain Conversational Systems"}]}
