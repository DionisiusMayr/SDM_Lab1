{"paperId": "a7d12340cb1c89f47ddff8f2293b2e8561fe92cb", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings", "abstract": "This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models showed that their scores on EHRNoteQA correlate more closely with their performance in addressing real-world medical questions evaluated by clinicians than their scores from other LLM benchmarks. This underscores the significance of EHRNoteQA in evaluating LLMs for medical applications and highlights its crucial role in facilitating the integration of LLMs into healthcare systems. The dataset will be made available to the public under PhysioNet credential access, promoting further research in this vital field.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2024-02-25", "journal": {"name": "ArXiv", "volume": "abs/2402.16040"}, "authors": [{"authorId": "2217192784", "name": "Sunjun Kweon"}, {"authorId": "2177426085", "name": "Jiyoun Kim"}, {"authorId": "2286879839", "name": "Heeyoung Kwak"}, {"authorId": "2286876172", "name": "Dongchul Cha"}, {"authorId": "2287742679", "name": "Hangyul Yoon"}, {"authorId": "2287764391", "name": "Kwanghyun Kim"}, {"authorId": "2226779328", "name": "Seunghyun Won"}, {"authorId": "2286057903", "name": "Edward Choi"}], "citations": [{"paperId": "b641a4288c0db2cf20da1b0863a26c910b90501f", "title": "CLUE: A Clinical Language Understanding Evaluation for LLMs"}]}
