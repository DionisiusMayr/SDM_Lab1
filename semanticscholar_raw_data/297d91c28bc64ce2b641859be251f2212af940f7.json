{"paperId": "297d91c28bc64ce2b641859be251f2212af940f7", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Reality Bites: Assessing the Realism of Driving Scenarios with Large Language Models", "abstract": "Large Language Models (LLMs) are demonstrating outstanding potential for tasks such as text generation, summarization, and classification. Given that such models are trained on a humongous amount of online knowledge, we hypothesize that LLMs can assess whether driving scenarios generated by autonomous driving testing techniques are realistic, i.e., being aligned with real-world driving conditions. To test this hypothesis, we conducted an empirical evaluation to assess whether LLMs are effective and robust in performing the task. This reality check is an important step towards devising LLM-based autonomous driving testing techniques. For our empirical evaluation, we selected 64 realistic scenarios from \\deepscenario--an open driving scenario dataset. Next, by introducing minor changes to them, we created 512 additional realistic scenarios, to form an overall dataset of 576 scenarios. With this dataset, we evaluated three LLMs (\\gpt, \\llama, and \\mistral) to assess their robustness in assessing the realism of driving scenarios. Our results show that: (1) Overall, \\gpt achieved the highest robustness compared to \\llama and \\mistral, consistently throughout almost all scenarios, roads, and weather conditions; (2) \\mistral performed the worst consistently; (3) \\llama achieved good results under certain conditions; and (4) roads and weather conditions do influence the robustness of the LLMs.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-03-14", "journal": {"name": "ArXiv", "volume": "abs/2403.09906"}, "authors": [{"authorId": "2180778205", "name": "Jiahui Wu"}, {"authorId": "2110374063", "name": "Chengjie Lu"}, {"authorId": "144104767", "name": "Aitor Arrieta"}, {"authorId": "145456500", "name": "T. Yue"}, {"authorId": "2249914761", "name": "Shaukat Ali"}], "citations": []}
