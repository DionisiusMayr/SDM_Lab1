{"paperId": "4f5c4ae2026b2bd97e26c6969e54cc634895e477", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Jointly Training Large Autoregressive Multimodal Models", "abstract": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-27", "journal": {"name": "ArXiv", "volume": "abs/2309.15564"}, "authors": [{"authorId": "2185505465", "name": "Emanuele Aiello"}, {"authorId": "49297123", "name": "L. Yu"}, {"authorId": "2247227174", "name": "Yixin Nie"}, {"authorId": "2201435", "name": "Armen Aghajanyan"}, {"authorId": "9185192", "name": "Barlas O\u011fuz"}], "citations": [{"paperId": "e291850b23d1c1ec49bc68e9e9266880898216b2", "title": "The (R)Evolution of Multimodal Large Language Models: A Survey"}, {"paperId": "db7498f569be9852a04b2bb5bd68bd2885820bea", "title": "World Model on Million-Length Video And Language With Blockwise RingAttention"}, {"paperId": "a050c9b0c321839e4427ab9defa3463be7825ac4", "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models"}, {"paperId": "2141ed804636a1cf339d606cd03fd3b3e9582133", "title": "VILA: On Pre-training for Visual Language Models"}, {"paperId": "769a924d0af014acec326f50c15c5d70d258a969", "title": "LLMGA: Multimodal Large Language Model based Generation Assistant"}, {"paperId": "eab54bef4b479e0b06fc65c8cab2c0400fa69b8a", "title": "On the Evaluation and Refinement of Vision-Language Instruction Tuning Datasets"}, {"paperId": "115d7a1c587587384038d61bb8a3af8c5b7de571", "title": "Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction"}, {"paperId": "e7d09b6f2bc878cf2c993acf675f409d0b55f35a", "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"}, {"paperId": "00a9a469bb019bf33eeee438c110f704b71cda73", "title": "Retrieving Multimodal Information for Augmented Generation: A Survey"}]}
