{"paperId": "44dc41803f49f7511f674ecb091d7a5c69fd5db2", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Learning to Poison Large Language Models During Instruction Tuning", "abstract": "The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\\%. Our work highlights the need for stronger defenses against data poisoning attack, offering insights into safeguarding LLMs against these more sophisticated attacks. The source code can be found on this GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-21", "journal": {"name": "ArXiv", "volume": "abs/2402.13459"}, "authors": [{"authorId": "2062242240", "name": "Yao Qiang"}, {"authorId": "2261896802", "name": "Xiangyu Zhou"}, {"authorId": "2284862847", "name": "Saleh Zare Zade"}, {"authorId": "2284863387", "name": "Mohammad Amin Roshani"}, {"authorId": "2266592629", "name": "Douglas Zytko"}, {"authorId": "2261934883", "name": "Dongxiao Zhu"}], "citations": []}
