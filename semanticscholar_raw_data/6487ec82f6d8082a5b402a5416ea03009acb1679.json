{"paperId": "6487ec82f6d8082a5b402a5416ea03009acb1679", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "State of the Art on Diffusion Models for Visual Computing", "abstract": "The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion-based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-10-11", "journal": {"name": "ArXiv", "volume": "abs/2310.07204"}, "authors": [{"authorId": "2142552068", "name": "Ryan Po"}, {"authorId": "2140325536", "name": "Wang Yifan"}, {"authorId": "3407706", "name": "Vladislav Golyanik"}, {"authorId": "3451442", "name": "Kfir Aberman"}, {"authorId": "50329510", "name": "J. Barron"}, {"authorId": "2254256512", "name": "Amit H. Bermano"}, {"authorId": "2257039304", "name": "Eric Ryan Chan"}, {"authorId": "2112779", "name": "Tali Dekel"}, {"authorId": "2248172435", "name": "Aleksander Holynski"}, {"authorId": "20615377", "name": "Angjoo Kanazawa"}, {"authorId": "2257113317", "name": "C. K. Liu"}, {"authorId": "46458089", "name": "Lingjie Liu"}, {"authorId": "2577533", "name": "B. Mildenhall"}, {"authorId": "2209612", "name": "M. Nie\u00dfner"}, {"authorId": "2257038709", "name": "Bjorn Ommer"}, {"authorId": "1680185", "name": "C. Theobalt"}, {"authorId": "1798011", "name": "Peter Wonka"}, {"authorId": "2256985147", "name": "Gordon Wetzstein"}], "citations": [{"paperId": "21ff703fdfc19bd9a0a2d5120070be427e14a369", "title": "TC4D: Trajectory-Conditioned Text-to-4D Generation"}, {"paperId": "bc1341ae6827d480e7f58258fc472a821de53865", "title": "Generative Active Learning for Image Synthesis Personalization"}, {"paperId": "f0fa7aca54062647ba0b8a681765d72b8b526cb6", "title": "Recent Trends in 3D Reconstruction of General Non-Rigid Scenes"}, {"paperId": "e6cb30effd90d563bc680430ecc1317842d33b6e", "title": "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation"}, {"paperId": "e5dd95af05d946e790da293190dd41eb53d979b8", "title": "LightIt: Illumination Modeling and Control for Diffusion Models"}, {"paperId": "41a66997ce0a366bba3becf7c3f37c9aebb13fbd", "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis"}, {"paperId": "34a95da15fe7a5b27830c940b0142060b8679065", "title": "PolyOculus: Simultaneous Multi-view Image-based Novel View Synthesis"}, {"paperId": "9761bcf49892601a3bec07d616c13c7f8bb7ac6c", "title": "Diffusion Model-Based Image Editing: A Survey"}, {"paperId": "8231dfb2f5fbffa3816acad3dc75de8ecd74a4ac", "title": "Advances in 3D Generation: A Survey"}, {"paperId": "791be248757a6914aac68c553574a5cbd3996c31", "title": "UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models"}, {"paperId": "94f7d8bce3bb848d127c8f113afc5bb0243579df", "title": "Lumiere: A Space-Time Diffusion Model for Video Generation"}, {"paperId": "e382fe486f698c2e06bf416e84ceeff06ceb3966", "title": "HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation"}, {"paperId": "3f19484f941f209f45a51b1b69160e24e9b9dc99", "title": "GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation"}, {"paperId": "3af22b2a713ac428cf3d582de49b1494ae1d978f", "title": "Personalized Restoration via Dual-Pivot Tuning"}, {"paperId": "a5b4f03ae8602f563d0e76a7c9c43a2be77f770d", "title": "Environment-Specific People"}, {"paperId": "c253528e5eb600aa67625d05004d3f0f9af24514", "title": "Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians"}, {"paperId": "37f8af5864e6622b3c44d28abccaf08a1bb95b52", "title": "Intrinsic Image Diffusion for Indoor Single-view Material Estimation"}, {"paperId": "8ea07ff25a06b25f953656a0960d584d99e68e8e", "title": "PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models"}, {"paperId": "55249d00e0c7712b61c2485a842d670ca19ea3d9", "title": "NeRFiller: Completing Scenes via Generative 3D Inpainting"}, {"paperId": "0cfdcbc1429c43ad38c9ba8b8a0b9b9b71e8b6ce", "title": "Orthogonal Adaptation for Modular Customization of Diffusion Models"}, {"paperId": "da2686ac2d469467ac28982d3d1d79d6b6420190", "title": "Generative Powers of Ten"}, {"paperId": "24b7abcd5a876146d613cbca0ad5e09d9851b259", "title": "EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Motion Generation"}, {"paperId": "e7e7975e4d3b7d94d4f411241ab53c5cda23b3e3", "title": "Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models"}, {"paperId": "71329a4c44c316fabc8388b3d8d10af7a778479c", "title": "3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing"}, {"paperId": "08518b90b85aca9ca6e9ffafd32b259b98e1260c", "title": "MorpheuS: Neural Dynamic 360{\\deg} Surface Reconstruction from Monocular RGB-D Video"}, {"paperId": "db3d415e0d2fee8fb1c35c6c6e121933a6565fc2", "title": "SparseGS: Real-Time 360\u00b0 Sparse View Synthesis using Gaussian Splatting"}, {"paperId": "31d164696b6768fa00aa0038c9fccd65120f7121", "title": "Gaussian Shell Maps for Efficient 3D Human Generation"}, {"paperId": "ff0baeb316df06d017f381c80a2c3aa6399fe938", "title": "4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling"}, {"paperId": "070c827308bec794584b5ca38de57f8e1f5ce191", "title": "Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer"}, {"paperId": "20de4b9425c3d0ad78818dbe532d65b27c1eea41", "title": "ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person Interactions"}, {"paperId": "52ec42a7b92d6845253cab140ab4f156efe59cab", "title": "Functional Diffusion"}, {"paperId": "f648a294077470314b6fb34b145fb87292523f01", "title": "The Chosen One: Consistent Characters in Text-to-Image Diffusion Models"}, {"paperId": "9e4d7b3504c0900d596a432278d0deca6243a0db", "title": "DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model"}, {"paperId": "68311312babfe5df4a0b05b00354ca6f5add6965", "title": "DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary Sparse Sensors Using Autoregressive Diffusion"}, {"paperId": "c3ef04f6eb9e59a03d41862db043196274d3fc05", "title": "GazeFusion: Saliency-Guided Image Generation"}]}
