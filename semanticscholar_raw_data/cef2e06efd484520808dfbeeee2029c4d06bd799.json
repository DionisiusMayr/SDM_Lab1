{"paperId": "cef2e06efd484520808dfbeeee2029c4d06bd799", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models", "abstract": "Despite the remarkable success of Large Language Models (LLMs), the massive size poses significant deployment challenges, particularly on resource-constrained hardware. While existing LLM compression methods focus on quantization, pruning remains relatively unexplored due to the high cost of training-based approaches and data collection challenges. One-shot pruning methods, although cost-effective and data-free, have become dominant in LLM pruning, but lead to performance decline under the structured pruning setting. In this work, we introduce a new paradigm for structurally pruning LLMs, called Compresso. Our approach, through the collaboration of the proposed resource-efficient pruning algorithm and the LLM itself, learns optimal pruning decisions during the training process. Compresso addresses the challenges of expensive training costs and data collection by incorporating Low-Rank Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning process. Then, we further augment the pruning algorithm by introducing a collaborative prompt that fosters collaboration between the LLM and the pruning algorithm, significantly boosting the overall performance. To this end, Compresso prunes LLaMA-7B to 5.4B, maintaining original performance and even surpassing LLaMA-7B in reading comprehension by 2.62%. Extensive experiments demonstrate that Compresso significantly outperforms one-shot pruning baselines across various sparsity ratios, achieving up to 2.21%, 11.43%, 7.04%, and 4.81% higher scores on the commonsense reasoning, reading comprehension, MMLU, and BBH benchmarks, respectively.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-08", "journal": {"name": "ArXiv", "volume": "abs/2310.05015"}, "authors": [{"authorId": "2257132390", "name": "Song Guo"}, {"authorId": "2257094139", "name": "Jiahang Xu"}, {"authorId": "48571328", "name": "L. Zhang"}, {"authorId": "2257128651", "name": "Mao Yang"}], "citations": [{"paperId": "dbf829c977c121c3704d070d7800d29fe5914756", "title": "LLM Inference Unveiled: Survey and Roofline Model Insights"}, {"paperId": "2fe05b1f953da5dcf6ec5fe7bc72bfb3dbd9ea30", "title": "Model Compression and Efficient Inference for Large Language Models: A Survey"}]}
