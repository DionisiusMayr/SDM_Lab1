{"paperId": "62b4e06f5249d22e4a153ec4a2dc934c6a014372", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "OWL: A Large Language Model for IT Operations", "abstract": "With the rapid development of IT operations, it has become increasingly crucial to efficiently manage and analyze large volumes of data for practical applications. The techniques of Natural Language Processing (NLP) have shown remarkable capabilities for various tasks, including named entity recognition, machine translation and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various NLP downstream tasks. However, there is a lack of specialized LLMs for IT operations. In this paper, we introduce the OWL, a large language model trained on our collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks. Furthermore, we evaluate the performance of our OWL on the OWL-Bench established by us and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-17", "journal": {"name": "ArXiv", "volume": "abs/2309.09298"}, "authors": [{"authorId": "2234806", "name": "Hongcheng Guo"}, {"authorId": "37081450", "name": "Jian Yang"}, {"authorId": "2108421213", "name": "Jiaheng Liu"}, {"authorId": "46554649", "name": "Liqun Yang"}, {"authorId": "2165382882", "name": "Linzheng Chai"}, {"authorId": "2107018151", "name": "Jiaqi Bai"}, {"authorId": "2243329942", "name": "Junran Peng"}, {"authorId": "2243804125", "name": "Xiaorong Hu"}, {"authorId": "2244169322", "name": "Chao Chen"}, {"authorId": "2243794847", "name": "Dongfeng Zhang"}, {"authorId": "2269755995", "name": "Xu Shi"}, {"authorId": "2152309872", "name": "Tieqiao Zheng"}, {"authorId": "2182735047", "name": "Liangfan Zheng"}, {"authorId": "2243445798", "name": "Bo Zhang"}, {"authorId": "2117101322", "name": "Ke Xu"}, {"authorId": "2144279674", "name": "Zhoujun Li"}], "citations": [{"paperId": "16861054ec33d376d8f0182f7bec89f118c70ef9", "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging"}, {"paperId": "186c3023473ad6093b654d4f4d8da64e0749fc32", "title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models"}, {"paperId": "94db8a625418800c8ae7b48157a9cad1c8129051", "title": "A Survey on Knowledge Distillation of Large Language Models"}, {"paperId": "28e9fb24bfb25f6e40d3897a5cf9c39f6f6b8d1f", "title": "C-ICL: Contrastive In-context Learning for Information Extraction"}, {"paperId": "c911d99205cbeaea42e0690916a119b84ae0aaf5", "title": "MLAD: A Unified Model for Multi-system Log Anomaly Detection"}, {"paperId": "33230a9d2e4d0ef9c923813a07107b2e3bc56605", "title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models"}, {"paperId": "557b10adfad3b9963f18f7438935d8c86109691e", "title": "M2C: Towards Automatic Multimodal Manga Complement"}]}
