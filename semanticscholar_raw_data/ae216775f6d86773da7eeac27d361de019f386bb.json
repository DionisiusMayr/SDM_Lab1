{"paperId": "ae216775f6d86773da7eeac27d361de019f386bb", "publicationVenue": {"id": "0942fb86-c16f-4084-9902-10ddcfe18180", "name": "Micro", "type": "conference", "alternate_names": ["Int Symp Microarchitecture", "MICRO", "International Symposium on Microarchitecture", "Annual IEEE/ACM International Symposium on Microarchitecture", "Annu IEEE/ACM Int Symp Microarchitecture"], "issn": "0271-9002", "alternate_issns": ["2151-4143", "2673-8023"], "url": "http://www.microarch.org/"}, "title": "EDEN: Enabling Energy-Efficient, High-Performance Deep Neural Network Inference Using Approximate DRAM", "abstract": "The effectiveness of deep neural networks (DNN) in vision, speech, and language processing has prompted a tremendous demand for energy-efficient high-performance DNN inference systems. Due to the increasing memory intensity of most DNN workloads, main memory can dominate the system's energy consumption and stall time. One effective way to reduce the energy consumption and increase the performance of DNN inference systems is by using approximate memory, which operates with reduced supply voltage and reduced access latency parameters that violate standard specifications. Using approximate memory reduces reliability, leading to higher bit error rates. Fortunately, neural networks have an intrinsic capacity to tolerate increased bit errors. This can enable energy-efficient and high-performance neural network inference using approximate DRAM devices. Based on this observation, we propose EDEN, the first general framework that reduces DNN energy consumption and DNN evaluation latency by using approximate DRAM devices, while strictly meeting a user-specified target DNN accuracy. EDEN relies on two key ideas: 1) retraining the DNN for a target approximate DRAM device to increase the DNN's error tolerance, and 2) efficient mapping of the error tolerance of each individual DNN data type to a corresponding approximate DRAM partition in a way that meets the user-specified DNN accuracy requirements. We evaluate EDEN on multi-core CPUs, GPUs, and DNN accelerators with error models obtained from real approximate DRAM devices. We show that EDEN's DNN retraining technique reliably improves the error resiliency of the DNN by an order of magnitude. For a target accuracy within 1% of the original DNN, our results show that EDEN enables 1) an average DRAM energy reduction of 21%, 37%, 31%, and 32% in CPU, GPU, and two different DNN accelerator architectures, respectively, across a variety of state-of-the-art networks, and 2) an average (maximum) speedup of 8% (17%) and 2.7% (5.5%) in CPU and GPU architectures, respectively, when evaluating latency-bound neural networks.", "venue": "Micro", "year": 2019, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2019-10-12", "journal": {"name": "Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture"}, "authors": [{"authorId": "35217389", "name": "Skanda Koppula"}, {"authorId": "3126431", "name": "Lois Orosa"}, {"authorId": "11827442", "name": "A. G. Ya\u011fl\u0131k\u00e7\u0131"}, {"authorId": "1379562685", "name": "Roknoddin Azizi"}, {"authorId": "51017214", "name": "Taha Shahroodi"}, {"authorId": "112984718", "name": "Konstantinos Kanellopoulos"}, {"authorId": "145929920", "name": "O. Mutlu"}], "citations": [{"paperId": "18096945a1feb10129f3dba558840ba34990f3a6", "title": "A Methodology to Study the Impact of Spiking Neural Network Parameters considering Event-Based Automotive Data"}, {"paperId": "8c5ef03e9118253ec019965bf142d1be7998a453", "title": "Functionally-Complete Boolean Logic in Real DRAM Chips: Experimental Characterization and Analysis"}, {"paperId": "763488734b9b5f6f475838c43746cf16f3fe5b0a", "title": "Rethinking the Producer-Consumer Relationship in Modern DRAM-Based Systems"}, {"paperId": "daa311cec5f058de4ba32d1669bccf288840a21e", "title": "Understanding Timing Error Characteristics from Overclocked Systolic Multiply\u2013Accumulate Arrays in FPGAs"}, {"paperId": "74e25a602aabf95a285355822dd3c0134629ce90", "title": "Simulating an Integrated Photonic Image Classifier for Diffractive Neural Networks."}, {"paperId": "b173f464ef74f8bfa424e51e4ae43ec000eb67ad", "title": "Unity ECC: Unified Memory Protection Against Bit and Chip Errors"}, {"paperId": "b12d3e853b4595d42cc0500d5ef0fb2062eb576e", "title": "Ergodic Approximate Deep Learning Accelerators"}, {"paperId": "c75daf38ec84d306179277d08904101bc5f68d60", "title": "Swordfish: A Framework for Evaluating Deep Neural Network-based Basecalling using Computation-In-Memory with Non-Ideal Memristors"}, {"paperId": "640d9870690f8312a3d87d13f78af3be76ee129e", "title": "SG-Float: Achieving Memory Access and Computing Power Reduction Using Self-Gating Float in CNNs"}, {"paperId": "14dff004e398f90c62c3a7142358bc657dc3e8cb", "title": "Temperature-Aware Memory Mapping and Active Cooling of Neural Processing Units"}, {"paperId": "15a3fd454031c368af8b9c19180edf16a0203fe6", "title": "Approximate Computing Survey, Part II: Application-Specific & Architectural Approximation Techniques and Applications"}, {"paperId": "6ab8bb78451e13f64102380c2988fa3bea64cf75", "title": "BERRY: Bit Error Robustness for Energy-Efficient Reinforcement Learning-Based Autonomous Systems"}, {"paperId": "f69ed4afadd9712cfc0698cdc909c96f0e9cf77a", "title": "NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes"}, {"paperId": "2485ba13b77215270f84971505e9566a0152ae11", "title": "Low Latency Energy-Efficient Neural Decoders for Block Codes"}, {"paperId": "25723c4c930002afcbd2dbefad7b72c21dc95857", "title": "FAQ: Mitigating the Impact of Faults in the Weight Memory of DNN Accelerators through Fault-Aware Quantization"}, {"paperId": "4a64bc9d8f6ab653216dd81762cd934669f7555f", "title": "CAMEL: Co-Designing AI Models and eDRAMs for Efficient On-Device Learning"}, {"paperId": "e0613848ed5a1a2faf0d71d5d818e00f8e212874", "title": "Energy-Efficient Approximate Edge Inference Systems"}, {"paperId": "97376bf989117a55caaa4fb5ac62be7110778534", "title": "LayCO: Achieving Least Lossy Accuracy for Most Efficient RRAM-Based Deep Neural Network Accelerator via Layer-Centric Co-Optimization"}, {"paperId": "b632301f92c1b16859092282b9f8e8c0996f12cd", "title": "Moses: Exploiting Cross-Device Transferable Features for on-Device Tensor Program Optimization"}, {"paperId": "59d6d7aae4ce08fc91c5071e6b270eb1682514f2", "title": "A Theory of I/O-Efficient Sparse Neural Network Inference"}, {"paperId": "8b57f5a57ac693c5e6d2026b579d0e3da628fd68", "title": "Review of Approximate Computing in Image Processing Applications"}, {"paperId": "4c5d837ac09025dd8d4c4a6db48bddb8841d4a08", "title": "TuRaN: True Random Number Generation Using Supply Voltage Underscaling in SRAMs"}, {"paperId": "0c7607f8da3936420c778a6418e2ea5d89f7775c", "title": "DRAM Bender: An Extensible and Versatile FPGA-Based Infrastructure to Easily Test State-of-the-Art DRAM Chips"}, {"paperId": "7037793094142ca20218d4d0ffc25470faf112a3", "title": "NEON: Enabling Efficient Support for Nonlinear Operations in Resistive RAM-based Neural Network Accelerators"}, {"paperId": "27b5c333fccd3674189f508327f5663af6f1e7cd", "title": "An Approximate Memory Based Defense Against Model Inversion Attacks to Neural Networks"}, {"paperId": "dda5728ddca81df3721744f1441ee8302c075456", "title": "Accelerating Neural Network Inference With Processing-in-DRAM: From the Edge to the Cloud"}, {"paperId": "ac6b94543492cc4cac95d72240179dcf018ff38d", "title": "EnforceSNN: Enabling resilient and energy-efficient spiking neural network inference considering approximate DRAMs for embedded systems"}, {"paperId": "9f19e1043ed5c0b367df746e02bcfacfb709fe56", "title": "FeFET-Based Binarized Neural Networks Under Temperature-Dependent Bit Errors"}, {"paperId": "09a804d0694d7f81693e4ef010c198c6b7028d91", "title": "Using Algorithmic Transformations and Sensitivity Analysis to Unleash Approximations in CNNs at the Edge"}, {"paperId": "3c5ebbc16ff04a7ea7df900a0ef513699ea9ee0d", "title": "Implementing a Timing Error-Resilient and Energy-Efficient Near-Threshold Hardware Accelerator for Deep Neural Network Inference"}, {"paperId": "40b261c056da0e26f598b75000a6bccafffd09fd", "title": "A Case for Transparent Reliability in DRAM Systems"}, {"paperId": "90888d81d91a89ff815bfe0b2895b501539c4448", "title": "Deep learning algorithms to develop Flood susceptibility map in Data-Scarce and Ungauged River Basin in India"}, {"paperId": "82a99e0d626847fadb6be938f979a4aec573e9a1", "title": "Hardware Approximate Techniques for Deep Neural Network Accelerators: A Survey"}, {"paperId": "f5fc7450ddb9ee3b603db09608db3f9f53d84eeb", "title": "SmartApprox: Learning-based configuration of approximate memories for energy-efficient execution"}, {"paperId": "b1777d08e3848e47bf68e69164f24338b5b9f78b", "title": "EcoFlow: Efficient Convolutional Dataflows for Low-Power Neural Network Accelerators"}, {"paperId": "f25a7a9d343d6cdffda06f8af3ba802137d16d10", "title": "Look-up-Table Based Processing-in-Memory Architecture With Programmable Precision-Scaling for Deep Learning Applications"}, {"paperId": "fb28d0134763d6337c4cfbe3964a2ab1007e7d4f", "title": "ADROIT: An Adaptive Dynamic Refresh Optimization Framework for DRAM Energy Saving In DNN Training"}, {"paperId": "92ffe7a89067ab79c0c0a19111c0cc6b039b28af", "title": "Brain-Inspired Computing: Adventure from Beyond CMOS Technologies to Beyond von Neumann Architectures ICCAD Special Session Paper"}, {"paperId": "62bc74c87efc71004c8b0c7e29ecc40d51c68cfd", "title": "A Deeper Look into RowHammer\u2019s Sensitivities: Experimental Analysis of Real DRAM Chips and Implications on Future Attacks and Defenses"}, {"paperId": "dfc3171f8b5f0fbb6d36f838a98ba0ffd0d605ee", "title": "MoRS: An Approximate Fault Modeling Framework for Reduced-Voltage SRAMs"}, {"paperId": "4f0b7a42c5a2e7062c74179d5a7628a486c08947", "title": "Shift-BNN: Highly-Efficient Probabilistic Bayesian Neural Network Training via Memory-Friendly Pattern Retrieving"}, {"paperId": "41f4d36fa8708abb0f77e18db7e3e15bb73f46e7", "title": "Automated HW/SW Co-design for Edge AI: State, Challenges and Steps Ahead: Special Session Paper"}, {"paperId": "f77a451dcd82fb6a35f079ac50bda2dfe3ef31a7", "title": "HARP: Practically and Effectively Identifying\u00a0Uncorrectable\u00a0Errors\u00a0in\u00a0Memory\u00a0Chips That Use On-Die Error-Correcting Codes"}, {"paperId": "ffe1c489980cc280117a1fdeb2aa2229787c89ec", "title": "Energy-Efficient CNNs Accelerator Implementation on FPGA with Optimized Storage and Dataflow"}, {"paperId": "55498b39456290025c43e26cd269b11b353fe281", "title": "ReSpawn: Energy-Efficient Fault-Tolerance for Spiking Neural Networks considering Unreliable Memories"}, {"paperId": "c47b7c74f9d6a9f8545c5acbe8c2b0ce73007f1d", "title": "SEAMS"}, {"paperId": "f68617d9df81e8c48f7fdc699009089a8a7bc58a", "title": "An in-memory computing architecture based on two-dimensional semiconductors for multiply-accumulate operations"}, {"paperId": "77c2faaf3f170822c3e37c4ecb4c8f13894b27ab", "title": "CODIC: A Low-Cost Substrate for Enabling Custom In-DRAM Functionalities and Optimizations"}, {"paperId": "9c567e891126ed64844419d234ccf3331080f741", "title": "MetaSys: A Practical Open-source Metadata Management System to Implement and Evaluate Cross-layer Optimizations"}, {"paperId": "b40032881c6dbcca58dc0922fb20fbcbea186776", "title": "Zero Aware Configurable Data Encoding by Skipping Transfer for Error Resilient Applications"}, {"paperId": "8e85281351edfd13081f1175fb102836ccc4baca", "title": "DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks"}, {"paperId": "e59adee86b666ad76164b3446cfee5068a15e5c9", "title": "Arithmetic-Intensity-Guided Fault Tolerance for Neural Network Inference on GPUs"}, {"paperId": "874f06ec7c78444087fca79ddeb2bfd1b72576a8", "title": "Random and Adversarial Bit Error Robustness: Energy-Efficient and Secure DNN Accelerators"}, {"paperId": "17b76435138374c1bd9173eede81ac4e8a6a6e46", "title": "A Lightweight Error-Resiliency Mechanism for Deep Neural Networks"}, {"paperId": "b96a224c612b76de782ce4df646248642c9323fe", "title": "SparkXD: A Framework for Resilient and Energy-Efficient Spiking Neural Network Inference using Approximate DRAM"}, {"paperId": "50143cb4502e2dfd8b8fd8bcbf6a4702426de67a", "title": "E2CNNs: Ensembles of Convolutional Neural Networks to Improve Robustness Against Memory Errors in Edge-Computing Devices"}, {"paperId": "262cb6d67818d16e5b973ea4e3f1da1fa2442dd1", "title": "Bit Error Tolerance Metrics for Binarized Neural Networks"}, {"paperId": "dc0e42f205f03e412ea293ea0338b5c33f748823", "title": "Margin-Maximization in Binarized Neural Networks for Optimizing Bit Error Tolerance"}, {"paperId": "7d4648dc5f8a6d7586814fa448fed896468885f9", "title": "FeFET and NCFET for Future Neural Networks: Visions and Opportunities"}, {"paperId": "dbf3a09267be8d397f73ac773517e02d6234d275", "title": "CSCNN: Algorithm-hardware Co-design for CNN Accelerators using Centrosymmetric Filters"}, {"paperId": "ceae40ef15d8230940716b2f9c4a1f705ea8b05c", "title": "The Granularity Gap Problem: A Hurdle for Applying Approximate Memory to Complex Data Layout"}, {"paperId": "f41ee09c8d4eb4866b9fbb0a92731efba9ef1bda", "title": "Correlation Modeling of Intelligent Manufacturing Industry Upgrading and International Trade based on Deep Neural Network"}, {"paperId": "8dd6c84dce582389df1c0b72471d7ab1e0b9f27b", "title": "Understanding Power Consumption and Reliability of High-Bandwidth Memory with Voltage Underscaling"}, {"paperId": "f58babc873f629090f213499f3dee18a2c507329", "title": "Intelligent Architectures for Intelligent Computing Systems"}, {"paperId": "db2c7eb1bad3ff523d303cf34a16cfa90fec680d", "title": "Reducing solid-state drive read latency by optimizing read-retry"}, {"paperId": "437d3bab7da11a3cdef8284cbac98f6e8f7522dc", "title": "Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead"}, {"paperId": "9a70e291365d609d22aae399d3ac32dd69baace9", "title": "MIDAS: Model Inversion Defenses Using an Approximate Memory System"}, {"paperId": "2087aa3006a526c5887bf822961ce4f263b27811", "title": "AXES: Approximation Manager for Emerging Memory Architectures"}, {"paperId": "95dd1f7f543c84f358975edc132f5aeac4051cda", "title": "Risk-5: Controlled Approximations for RISC-V"}, {"paperId": "9570d89435633ebf904e63d1b07ee0758f851b30", "title": "SAME-Infer: Software Assisted Memory Resilience for Efficient Inference at the Edge"}, {"paperId": "9e5140f40c2dc97eff173fefe9d73337f4a523fc", "title": "FIGARO: Improving System Performance via Fine-Grained In-DRAM Data Relocation and Caching"}, {"paperId": "6679cfabe152f418ea9d32efff4db874886fcfa8", "title": "Bit-Exact ECC Recovery (BEER): Determining DRAM On-Die ECC Functions by Exploiting DRAM Data Retention Characteristics"}, {"paperId": "8ba77353e6f14c295e4697e743b59058df89a377", "title": "Temperature-Aware Optimization of Monolithic 3D Deep Neural Network Accelerators"}, {"paperId": "af525b77d9b4da946b2ac5b6e3b0c5c38d8b40e9", "title": "Sensibilidade a erros em aplica\u00e7\u00f5es na arquitetura RISC-V"}, {"paperId": "04621c422475a040e7a661a79559c64531dacc25", "title": "Intelligent Architectures for Intelligent Machines"}, {"paperId": "e2e215ec97419f15de4027155bc7b7dc70d7fc98", "title": "Bit Error Robustness for Energy-Efficient DNN Accelerators"}, {"paperId": "a1db46169a8c6ab333ae36d1223c70cc7a1490b3", "title": "On Mitigating Random and Adversarial Bit Errors"}, {"paperId": "5f020fcab6635c5290d3609c419fb8b9bb63027e", "title": "An Experimental Study of Reduced-Voltage Operation in Modern FPGAs for Neural Network Acceleration"}, {"paperId": "acdfe34aab867d1a0101c88dba299a56274388d8", "title": "CLR-DRAM: A Low-Cost DRAM Architecture Enabling Dynamic Capacity-Latency Trade-Off"}, {"paperId": "c0d41f90eb35372e2346112995815556c8232600", "title": "DSAGEN: Synthesizing Programmable Spatial Accelerators"}, {"paperId": "de573320ad3947af18cb6d52e85d43a7727ed81c", "title": "FT-CNN: Algorithm-Based Fault Tolerance for Convolutional Neural Networks"}, {"paperId": "41e70b64c645693fac4c640a25a77909a32e4b7f", "title": "Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead"}, {"paperId": "127d3d658c35056a4457cbab7a966a469b9c5d60", "title": "Approximate Hardware Techniques for Energy-Quality Scaling Across the System"}, {"paperId": "47a0ffedf83d3f9a9a998bd29bd40b015afb0613", "title": "On the Resilience of Deep Learning for Reduced-voltage FPGAs"}, {"paperId": "fe77e603004d7ba298fe140d5279e167e1c30555", "title": "on System-level Design Methods for Deep Learning on Heterogeneous Architectures (SLOHA 2021)"}, {"paperId": "eb4b8a3a5bba61a689598e6969fc0ea082363217", "title": "ZEM: Zero-Cycle Bit-Masking Module for Deep Learning Refresh-Less DRAM"}]}
