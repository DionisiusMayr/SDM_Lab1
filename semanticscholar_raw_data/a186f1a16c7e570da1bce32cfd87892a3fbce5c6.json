{"paperId": "a186f1a16c7e570da1bce32cfd87892a3fbce5c6", "publicationVenue": null, "title": "Contrastive Self-Supervised Learning for Tabular Data", "abstract": "Much of the worlds data is stored in databases or spreadsheets and consists of columns of features which comprise of continuous, categorical or binary variables. A range of challenging and important machine learning tasks rely on data of this format such as applications in healthcare, business analytics and recommendation systems. However, large amounts of labelled data for a specific task can often be either expensive or impossible to obtain. Self-supervised learning is a technique in machine learning in which representations are learned from large amounts of unlabelled data and then used to assist other models trained on tasks for which there is only a small amount of labelled data. Recent research has shown that this approach can improve performance on tabular data when labelled data is scarce, and therefore finding better self-supervised learning methods for tabular data is an important area of research. Contrastive learning is a framework for learning representations that has recently received a great deal of attention, particularly in computer vision, because of it\u2019s state-of-the-art performance in self-supervised learning. However, whilst it has been applied successfully to image data, the vast majority of current contrastive learning methods require the use of bespoke augmentations that are only applicable to images. Tabular data is a domain where data augmentations are not as widely used because there do not exist strong inductive biases about the latent information in the inputs for all datasets that would be equivalent to, for example, translation invariance for images. There has therefore been almost no research into contrastive learning for tabular data, in spite of the fact that contrastive learning does not demand the use of augmentations, and could potentially be used to achieve equivalent improvements in performance for tabular data if adapted correctly. In this thesis, we explore the use of contrastive learning for tabular data and develop a novel approach called Masked Contrastive Learning (MCL) that does not require domainspecific augmentations and we show that it can be used to achieve state-of-the-art results for self-supervised learning on tabular data. We also extend our approach using a novel masking scheme we develop which allows the mask generator to learn to retain the structural informa-", "venue": "", "year": 2021, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": null, "journal": null, "authors": [{"authorId": "2155723510", "name": "Hugh Bishop"}], "citations": []}
