{"paperId": "89641466373aa9ce2976e3f384b0791a7bd0931c", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs", "abstract": "Large Language Models (LLMs) are now widely used in various applications, making it crucial to align their ethical standards with human values. However, recent jail-breaking methods demonstrate that this alignment can be undermined using carefully constructed prompts. In our study, we reveal a new threat to LLM alignment when a bad actor has access to the model's output logits, a common feature in both open-source LLMs and many commercial LLM APIs (e.g., certain GPT models). It does not rely on crafting specific prompts. Instead, it exploits the fact that even when an LLM rejects a toxic request, a harmful response often hides deep in the output logits. By forcefully selecting lower-ranked output tokens during the auto-regressive generation process at a few critical output positions, we can compel the model to reveal these hidden responses. We term this process model interrogation. This approach differs from and outperforms jail-breaking methods, achieving 92% effectiveness compared to 62%, and is 10 to 20 times faster. The harmful content uncovered through our method is more relevant, complete, and clear. Additionally, it can complement jail-breaking strategies, with which results in further boosting attack performance. Our findings indicate that interrogation can extract toxic knowledge even from models specifically designed for coding tasks.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-12-08", "journal": {"name": "ArXiv", "volume": "abs/2312.04782"}, "authors": [{"authorId": "2220690905", "name": "Zhuo Zhang"}, {"authorId": "2052467415", "name": "Guangyu Shen"}, {"authorId": "48927894", "name": "Guanhong Tao"}, {"authorId": "46378881", "name": "Siyuan Cheng"}, {"authorId": "2156004395", "name": "Xiangyu Zhang"}], "citations": [{"paperId": "2c4d2d889a1f0ff9598de829a001df11a95d3294", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack"}, {"paperId": "2f4cc3f4a1c70cd5aca14c1304037491cd3aeb9b", "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content"}, {"paperId": "97c17683dbd3bea6972b93f5ab8ebb208b344ee6", "title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs"}, {"paperId": "9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2", "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates"}, {"paperId": "f75f401f046d508753d6b207f3f19414f489bd08", "title": "Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia"}]}
