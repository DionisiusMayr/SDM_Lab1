{"paperId": "c3ed9325fe3ff66b86c0731320c9fe54c8af7576", "publicationVenue": {"id": "ed2921d1-8132-4e3d-bdd9-60eb0338de5c", "name": "International Symposium on Modeling and Optimization in Mobile, Ad-Hoc and Wireless Networks", "type": "conference", "alternate_names": ["WiOpt", "Modeling and Optimization in Mobile, Ad-Hoc and Wireless Networks", "Model Optim Mob Ad-hoc Wirel Netw", "Int Symp Model Optim Mob Ad-hoc Wirel Netw"], "url": "http://www.wiopt.org/"}, "title": "DOLL: Distributed OnLine Learning Using Preemptible Cloud Instances", "abstract": "To defray the increasingly massive costs of running large machine learning workloads, much work has proposed running them on preemptible cloud instances, a discount tier of virtual machine rentals that may be interrupted at the cloud provider's discretion. This work, however, largely ignores the fact that much data used for machine learning comes from streams of diverse sources, e.g., wirelessly connected cameras or hospital health records. Processing datastreams on preemptible instances presents new challenges: processing data as they arrive may engender bottlenecks when scaling the system to handle higher throughput, particularly if the instances are frequently interrupted. Ours is the first work to design, analyze, and optimize a system that uses a set of datastreams to train a machine learning model on preemptible instances. Our system, DOLL, uses queueing and batching to parallelize and scale SGD (stochastic gradient descent)-based optimizers to large numbers of workers and datastreams, as well as heterogeneous data arrival rates across streams. Expected error convergence guarantees are then derived for DOLL's training process. We use this guarantee to optimize the cost of requisitioning preemptible and on-demand instances given an error target and wall-clock time deadline; this optimization is validated on experiments demonstrating substantial cost savings with little impact on model error.", "venue": "International Symposium on Modeling and Optimization in Mobile, Ad-Hoc and Wireless Networks", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-08-30", "journal": {"name": "2023 21st International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)", "pages": "175-182"}, "authors": [{"authorId": "2174601751", "name": "Harry Jiang"}, {"authorId": "145612180", "name": "Xiaoxi Zhang"}, {"authorId": "1393650147", "name": "Carlee Joe-Wong"}], "citations": [{"paperId": "71dcb50a9ea50465f5a4813b52dff299085d3aa5", "title": "DYNAMITE: Dynamic Interplay of Mini-Batch Size and Aggregation Frequency for Federated Learning with Static and Streaming Dataset"}, {"paperId": "97c98e57044e7523e1c61329d23992a5300815f6", "title": "spotDNN: Provisioning Spot Instances for Predictable Distributed DNN Training in the Cloud"}]}
