{"paperId": "42da19a0537bcdc0d59eee3870eae7428b342b62", "publicationVenue": {"id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29", "name": "International Conference on Machine Learning", "type": "conference", "alternate_names": ["ICML", "Int Conf Mach Learn"], "url": "https://icml.cc/"}, "title": "Accelerating Gossip SGD with Periodic Global Averaging", "abstract": "Communication overhead hinders the scalability of large-scale distributed training. Gossip SGD, where each node averages only with its neighbors, is more communication-efficient than the prevalent parallel SGD. However, its convergence rate is reversely proportional to quantity $1-\\beta$ which measures the network connectivity. On large and sparse networks where $1-\\beta \\to 0$, Gossip SGD requires more iterations to converge, which offsets against its communication benefit. This paper introduces Gossip-PGA, which adds Periodic Global Averaging into Gossip SGD. Its transient stage, i.e., the iterations required to reach asymptotic linear speedup stage, improves from $\\Omega(\\beta^4 n^3/(1-\\beta)^4)$ to $\\Omega(\\beta^4 n^3 H^4)$ for non-convex problems. The influence of network topology in Gossip-PGA can be controlled by the averaging period $H$. Its transient-stage complexity is also superior to Local SGD which has order $\\Omega(n^3 H^4)$. Empirical results of large-scale training on image classification (ResNet50) and language modeling (BERT) validate our theoretical findings.", "venue": "International Conference on Machine Learning", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-19", "journal": {"pages": "1791-1802"}, "authors": [{"authorId": "2109367314", "name": "Yiming Chen"}, {"authorId": "1841311", "name": "K. Yuan"}, {"authorId": "2363741", "name": "Yingya Zhang"}, {"authorId": "1642296684", "name": "Pan Pan"}, {"authorId": "50125871", "name": "Yinghui Xu"}, {"authorId": "6833606", "name": "W. Yin"}], "citations": [{"paperId": "f73eccd11a70212d68e79e5b11e37f35dd1c908e", "title": "Vanishing Variance Problem in Fully Decentralized Neural-Network Systems"}, {"paperId": "9104a14a784cbbdc0e20eb34a31b9dfb1f17d1b3", "title": "Accelerating Gradient Tracking with Periodic Global Averaging"}, {"paperId": "19bb64946d8169ad6fd228916f71e20894d40c5d", "title": "Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity"}, {"paperId": "8ea73b7673c2ea8bb28453bb135654a60afdac8c", "title": "Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees"}, {"paperId": "0dd9a11c5098668330b3ecf5960fd37103810514", "title": "Distributed Stochastic Bilevel Optimization: Improved Complexity and Heterogeneity Analysis"}, {"paperId": "c52e5efd20b4ed97cb3d8548d8849d9b6410ab1a", "title": "A Loopless Distributed Algorithm for Personalized Bilevel Optimization"}, {"paperId": "751a06db6863f48d42c43a9b65b530a31217a1d9", "title": "Towards Better Understanding the Influence of Directed Networks on Decentralized Stochastic Optimization"}, {"paperId": "ff950bee2aae7ed85e304f397bca9c33615bdaf4", "title": "Communication and Energy Efficient Decentralized Learning Over D2D Networks"}, {"paperId": "965427d702a8a09945449ce09be8c7b46dbabb98", "title": "Communication-Efficient Federated Optimization over Semi-Decentralized Networks"}, {"paperId": "976636f979fe5b6c336f979d86e4f94555f687e8", "title": "Over-the-Air Federated Learning and Optimization"}, {"paperId": "7bc4857e9256adf472a2d95e7f487a7c10631b67", "title": "Epidemic Learning: Boosting Decentralized Learning with Randomized Communication"}, {"paperId": "dd9c530f7994d2f8d5ddc8684bef8ae8bd875774", "title": "Joint Model Pruning and Topology Construction for Accelerating Decentralized Machine Learning"}, {"paperId": "a1513d2ffbb58de8229dd94c0d3e71ea2969e799", "title": "Decentralized Local Updates with Dual-Slow Estimation and Momentum-based Variance-Reduction for Non-Convex Optimization"}, {"paperId": "0fff24d83092c5af1d603e4507f55cfa1d6420f0", "title": "Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence"}, {"paperId": "8dec534566e4b8e451e06600be90b09f5eaca477", "title": "On the Limit Performance of Floating Gossip"}, {"paperId": "9eb1cb0bd5430a4dbb3ad5f5e88773e1ba58ec37", "title": "Decentralized Gradient Tracking with Local Steps"}, {"paperId": "3ea459c34bc6c4e34bc2de9cb83b1f422fe3b036", "title": "Revisiting Optimal Convergence Rate for Smooth and Non-convex Stochastic Decentralized Optimization"}, {"paperId": "ae6d2927119206d04cd13e38b4d9870da65446cb", "title": "Tackling Data Heterogeneity: A New Unified Framework for Decentralized SGD with Sample-induced Topology"}, {"paperId": "10044d70ffefdfc57ebc69ae6d52e1f5178bb775", "title": "Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with Communication Compression"}, {"paperId": "2269112fef4a0bbaedf84465c8acd1f1fc6eaf77", "title": "Sign bit is enough: a learning synchronization framework for multi-hop all-reduce with ultimate compression"}, {"paperId": "0bdd34a438dd31a28ea4b79c50b4a528fdf471c2", "title": "Decentralized Optimization Over the Stiefel Manifold by an Approximate Augmented Lagrangian Function"}, {"paperId": "87b6eff7ef8aa498e7e0a640ce50f876707aebb2", "title": "BlueFog: Make Decentralized Algorithms Practical for Optimization and Deep Learning"}, {"paperId": "f0974cca1c0989f8d3472d34681241ac7c1c4b46", "title": "Exponential Graph is Provably Efficient for Decentralized Deep Training"}, {"paperId": "5733764f67e2acfde72315e38bb60fb199f43726", "title": "A Unified and Refined Convergence Analysis for Non-Convex Decentralized Learning"}, {"paperId": "efea624caa48c0f0c14b68f444f37d41f1ad32e8", "title": "Removing Data Heterogeneity Influence Enhances Network Topology Dependence of Decentralized SGD"}, {"paperId": "9540e0ae29bcbbf10aac4585030b423e11163fe4", "title": "DecentLaM: Decentralized Momentum SGD for Large-batch Deep Training"}, {"paperId": "4124698f5fad11297f21977fa01736065f5bba5c", "title": "Crossover\u2010SGD: A gossip\u2010based communication in distributed deep learning for alleviating large mini\u2010batch problem and enhancing scalability"}, {"paperId": "6322985bb03f3fcc2982ec8d2eba4c93e38865f0", "title": "Distributed Artificial Intelligence Empowered by End-Edge-Cloud Computing: A Survey"}, {"paperId": "a59bff592a8854074a552ef6962d5bc58c21951a", "title": "Decentralized Machine Learning Training: A Survey on Synchronization, Consolidation, and Topologies"}, {"paperId": "b4c27615a88b8eb4bd806e74116281b1a1f50ff5", "title": "Accelerating Adaptive Federated Optimization with Local Gossip Communications"}, {"paperId": "0aa7cb075978ed8c24f3e2a8ddc3ccb14df3e9a5", "title": "Hybrid Local SGD for Federated Learning with Heterogeneous Communications"}, {"paperId": "e2c05dbd7c42d9c4ae7eba12198fcf74f2dfd0e6", "title": "Classification of X-Ray Images of the Chest Using Convolutional Neural Networks"}]}
