{"paperId": "ef12b71e7c7cc7483e3c2147ebf9753a31554266", "publicationVenue": {"id": "27b93d96-c4b9-4a23-a3a0-061ad54deebc", "name": "IEEE International Symposium on High-Performance Parallel Distributed Computing", "type": "conference", "alternate_names": ["HPDC", "IEEE Int Symp High-performance Parallel Distrib Comput"], "url": "http://www.hpdc.org/"}, "title": "TLPGNN: A Lightweight Two-Level Parallelism Paradigm for Graph Neural Network Computation on GPU", "abstract": "Graph Neural Networks (GNNs) are an emerging class of deep learning models on graphs, with many successful applications, such as, recommendation systems, drug discovery, and social network analysis. The GNN computation includes both regular neural network operations and general graph convolution operations, which take the majority of the total computation time. Though several recent works have been proposed to accelerate the computation for GNNs, they face the limitations of heavy pre-processing, low efficient atomic operations, and unnecessary kernel launches. In this paper, we design TLPGNN, a lightweight two-level parallelism paradigm for GNN computation. First, we conduct a systematic analysis on the hardware resource usage of GNN workloads to deeply understand the specialties of GNN workloads. With the insightful observations, we then divide the GNN computation into two levels, i.e., vertex parallelism for the first level and feature par- allelism for the second. Next, we employ a novel hybrid dynamic workload assignment to address the imbalanced workload distribution. Furthermore, we fuse the kernels to reduce the number of kernel launches and cache the frequently accessed data into registers to avoid unnecessary memory traffics. Together, TLPGNN is able to significantly outperform existing GNN computation systems, such as DGL, GNNAdivsor, and FeatGraph, by 5.6\u00d7, 7.7\u00d7, and 3.3\u00d7, respectively, on the average.", "venue": "IEEE International Symposium on High-Performance Parallel Distributed Computing", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2022-06-27", "journal": {"name": "Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing"}, "authors": [{"authorId": "2167292330", "name": "Qiang Fu"}, {"authorId": "3028841", "name": "Yuede Ji"}, {"authorId": "48186750", "name": "Huimin Huang"}], "citations": [{"paperId": "a48d60f216c9c7714f14bd6196ee8dc4b3070269", "title": "GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System"}, {"paperId": "b7cdce5df8ced4861984dcee1366ef4e79666df0", "title": "CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks"}, {"paperId": "85691014466d48647a01d8726c68d0e9258fc667", "title": "Single-GPU GNN Systems: Traps and Pitfalls"}, {"paperId": "917cd596c08acf32dd777f5cef2f10840b95f900", "title": "Accelerating Maximal Biclique Enumeration on GPUs"}, {"paperId": "66096f3b7a4ee48fa66b863c28cdd130872f513e", "title": "JITSPMM: Just-in-Time Instruction Generation for Accelerated Sparse Matrix-Matrix Multiplication"}, {"paperId": "27a51a16f08ac4e997b71af7bcda2407e0de48ba", "title": "Peek: A Prune-Centric Approach for K Shortest Path Computation"}, {"paperId": "6988501587a310dca1fe0a6b5d3e188b26ff8f12", "title": "Redundancy-Free High-Performance Dynamic GNN Training with Hierarchical Pipeline Parallelism"}, {"paperId": "894d61c709ec6f61899703458d90b09c663d7b11", "title": "A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware"}, {"paperId": "460fc9c30d825459b264e9fe247c2345925fdd06", "title": "PIGEON: Optimizing CUDA Code Generator for End-to-End Training and Inference of Relational Graph Neural Networks"}, {"paperId": "ebe8a3c68b88e7c2747f92414a59ba6e708f0670", "title": "PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs"}, {"paperId": "141c3de5103c8cb11b3119470f1aad438445399a", "title": "A Comprehensive Survey on Distributed Training of Graph Neural Networks"}, {"paperId": "1e79e33c77b2d8eaf643af0e1f5003057d7356b2", "title": "Distributed Graph Neural Network Training: A Survey"}, {"paperId": "68e2203456d937bdce0dc776021293f880dfa998", "title": "RTS-GAT: Spatial Graph Attention-Based Spatio-Temporal Flow Prediction for Big Data Retailing"}]}
