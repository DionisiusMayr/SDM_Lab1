{"paperId": "004699d8ed6d1929e484aba461391fadb1f5b0a7", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly", "abstract": "Large Language Models (LLM) and foundation models are popular as they offer new opportunities for individuals and businesses to improve natural language processing, interact with data, and retrieve information faster. However, training or fine-tuning LLMs requires a vast amount of data, which can be challenging to access due to legal or technical restrictions and may require private computing resources. Federated Learning (FL) is a solution designed to overcome these challenges and expand data access for deep learning applications. This paper takes a hardware-centric approach to explore how LLMs can be brought to modern edge computing systems. Our study fine-tunes the FLAN-T5 model family, ranging from 80M to 3B parameters, using FL for a text summarization task. We provide a micro-level hardware benchmark, compare the model FLOP utilization to a state-of-the-art data center GPU, and study the network utilization in realistic conditions. Our contribution is twofold: First, we evaluate the current capabilities of edge computing systems and their potential for LLM FL workloads. Second, by comparing these systems with a data-center GPU, we demonstrate the potential for improvement and the next steps toward achieving greater computational efficiency at the edge.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-04", "journal": {"name": "ArXiv", "volume": "abs/2310.03150"}, "authors": [{"authorId": "2220057323", "name": "Herbert Woisetschl\u00e4ger"}, {"authorId": "2154763233", "name": "Alexander Isenko"}, {"authorId": "2255363698", "name": "Shiqiang Wang"}, {"authorId": "39509913", "name": "R. Mayer"}, {"authorId": "2254179381", "name": "Hans-Arno Jacobsen"}], "citations": [{"paperId": "1d10aa5e7122d1df6d559999987c76de3a088f62", "title": "Training Machine Learning models at the Edge: A Survey"}, {"paperId": "88a30d7676108ecafcd8a85c2c60b3d5d1fbde50", "title": "A Survey on Efficient Federated Learning Methods for Foundation Model Training"}, {"paperId": "06eae596ea3e996453039f6a2cc68732cbba884b", "title": "Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes"}, {"paperId": "e0a7de219b4621495b91f98dd2bd588279e899ac", "title": "Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives"}]}
