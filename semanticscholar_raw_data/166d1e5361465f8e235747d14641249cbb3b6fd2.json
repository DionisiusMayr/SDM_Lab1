{"paperId": "166d1e5361465f8e235747d14641249cbb3b6fd2", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models", "abstract": "Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 translation directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-20", "journal": {"name": "ArXiv", "volume": "abs/2309.11674"}, "authors": [{"authorId": "2108835452", "name": "Haoran Xu"}, {"authorId": "2152658577", "name": "Young Jin Kim"}, {"authorId": "2243335598", "name": "Amr Sharaf"}, {"authorId": "3032929", "name": "H. Awadalla"}], "citations": [{"paperId": "acc7a0a881d0914a5856768e22b7347ce06bd445", "title": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning"}, {"paperId": "cc49514a9859ae171a7cd7eaae111990605a5e4e", "title": "Tower: An Open Multilingual Large Language Model for Translation-Related Tasks"}, {"paperId": "2c40e142a4f27d14591b30f7afeb994d42287566", "title": "Fine-tuning Large Language Models for Domain-specific Machine Translation"}, {"paperId": "de643836277d108e1f8410ea85b0c3ed0e686163", "title": "MaLA-500: Massive Language Adaptation of Large Language Models"}, {"paperId": "6174f46a035c57f7ca4b9d581a31022f60dcbedd", "title": "PHOENIX: Open-Source Language Adaption for Direct Preference Optimization"}, {"paperId": "ebd1c04c61f73f46def3305ca11d038c46665b65", "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"}, {"paperId": "9831f8c43efff346d8bb00e9b4cec59ac96bc7d8", "title": "Fine-tuning Large Language Models for Adaptive Machine Translation"}, {"paperId": "157a20a2994577d989be8f15e6c983f738e8551c", "title": "Context Matter: Data-Efficient Augmentation of Large Language Models for Scientific Applications"}, {"paperId": "6dc8f09f3f39d41c21ec167b8a0ed9f12552f487", "title": "Trustworthy Large Models in Vision: A Survey"}]}
