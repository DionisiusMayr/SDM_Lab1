{"paperId": "1bf45f35f6f32822d20c22715b1b9543d74b2c3f", "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference on Learning Representations", "type": "conference", "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "title": "Visual hyperacuity with moving sensor and recurrent neural computations", "abstract": "Dynamical phenomena, such as recurrent neuronal activity and perpetual motion of the eye, are typically overlooked in models of bottom-up visual perception. Recent experiments suggest that a tiny inter-saccadic eye motion (\u201c\ufb01xational drift\u201d) enhances visual acuity beyond the limit imposed by the density of retinal photoreceptors. Here we hypothesize that such an enhancement is enabled by recurrent neuronal computations in early visual areas. Speci\ufb01cally, we explore a setting involving a low-resolution dynamical sensor that moves with respect to a static scene, with drift-like tiny steps. This setting mimics a dynamical eye, viewing objects in perceptually-challenging conditions. The dynamical sensory input is classi\ufb01ed by a convolutional neural network with recurrent connectivity added to its lower layers, in analogy to recurrent connectivity in early visual areas. Applying our system to CiFAR-10 and CiFAR-100 datasets down-sampled via 8x8 sensor, we found that (i) classi\ufb01cation accuracy, which is drastically reduced by this down-sampling, is mostly restored to its 32x32 baseline level when using a moving sensor and recurrent connectivity, (ii) in this setting, neurons in the early layers exhibit a wide repertoire of selectivity patterns, spanning the spatio-temporal selectivity space, with neurons preferring different combinations of spatial and temporal patterning, and (iii) curved sensor\u2019s trajectories improve visual acuity compared to straight trajectories, echoing recent experimental \ufb01ndings involving eye-tracking in challenging conditions. Our work sheds light on the possible role of recurrent connectivity in early vision", "venue": "International Conference on Learning Representations", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": null, "authors": [{"authorId": "18535698", "name": "A. Rivkind"}, {"authorId": "2181918057", "name": "Or Ram"}, {"authorId": "2245598", "name": "Eldad Assa"}, {"authorId": "2181918054", "name": "Michael Kreiserman"}, {"authorId": "1885258", "name": "E. Ahissar"}], "citations": [{"paperId": "47eeae49651aa20ca2e67be7317f5fba84a63f61", "title": "Inferring visual space from ultra-fine extra-retinal knowledge of gaze position"}, {"paperId": "496349bd33b2793f66b151eda9266e856bc1e9ab", "title": "Learning to Look by Self-Prediction"}]}
