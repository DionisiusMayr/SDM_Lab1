{"paperId": "945db0077b6d19b720f5998b3f61300013c4f885", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models", "abstract": "Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios. Code will be made available at https://github.com/yuhuixu1993/qa-lora.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-26", "journal": {"name": "ArXiv", "volume": "abs/2309.14717"}, "authors": [{"authorId": "48615640", "name": "Yuhui Xu"}, {"authorId": "3041937", "name": "Lingxi Xie"}, {"authorId": "7787721", "name": "Xiaotao Gu"}, {"authorId": "2145229597", "name": "Xin Chen"}, {"authorId": "144188238", "name": "Heng Chang"}, {"authorId": "1983351", "name": "Hengheng Zhang"}, {"authorId": "2249294812", "name": "Zhensu Chen"}, {"authorId": "21458018", "name": "Xiaopeng Zhang"}, {"authorId": "2106415186", "name": "Qi Tian"}], "citations": [{"paperId": "ee4014497ccf2f65d6e05d3956b0e6b0c7369bae", "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"}, {"paperId": "4de26a743221a48d5f0dd99068f6b66ea454c2bf", "title": "Mixed-precision Supernet Training from Vision Foundation Models using Low Rank Adapter"}, {"paperId": "916b4926cda574dc3f9486bb9994b6f2788dd800", "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey"}, {"paperId": "fe9db1e1e5ce1c2ae6334b043c98102c0bc392f6", "title": "Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks"}, {"paperId": "63b331998b17fdb879073873cf06004a6df18b44", "title": "A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge"}, {"paperId": "2fe05b1f953da5dcf6ec5fe7bc72bfb3dbd9ea30", "title": "Model Compression and Efficient Inference for Large Language Models: A Survey"}, {"paperId": "5c3cee8d2ce4bd7d424e6bc281ac74acf47ad0e9", "title": "Separable Multi-Concept Erasure from Diffusion Models"}, {"paperId": "4be46ae53206440cfa6cab48f7523df5a701a65f", "title": "Zero-shot Generative Large Language Models for Systematic Review Screening Automation"}, {"paperId": "83de13bd492b9e72c314e308f0d77014154a6a74", "title": "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment"}, {"paperId": "74e0f3400b6ef05a5eb7bdac1b6d0758cbf5b1f1", "title": "NOLA: Networks as Linear Combination of Low Rank Random Basis"}]}
