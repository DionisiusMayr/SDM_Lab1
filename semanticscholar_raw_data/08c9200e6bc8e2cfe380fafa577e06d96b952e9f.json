{"paperId": "08c9200e6bc8e2cfe380fafa577e06d96b952e9f", "publicationVenue": {"id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40", "name": "International Conference on Learning Representations", "type": "conference", "alternate_names": ["Int Conf Learn Represent", "ICLR"], "url": "https://iclr.cc/"}, "title": "SWIFT: Rapid Decentralized Federated Learning via Wait-Free Model Communication", "abstract": "The decentralized Federated Learning (FL) setting avoids the role of a potentially unreliable or untrustworthy central host by utilizing groups of clients to collaboratively train a model via localized training and model/gradient sharing. Most existing decentralized FL algorithms require synchronization of client models where the speed of synchronization depends upon the slowest client. In this work, we propose SWIFT: a novel wait-free decentralized FL algorithm that allows clients to conduct training at their own speed. Theoretically, we prove that SWIFT matches the gold-standard iteration convergence rate $\\mathcal{O}(1/\\sqrt{T})$ of parallel stochastic gradient descent for convex and non-convex smooth optimization (total iterations $T$). Furthermore, we provide theoretical results for IID and non-IID settings without any bounded-delay assumption for slow clients which is required by other asynchronous decentralized FL algorithms. Although SWIFT achieves the same iteration convergence rate with respect to $T$ as other state-of-the-art (SOTA) parallel stochastic algorithms, it converges faster with respect to run-time due to its wait-free structure. Our experimental results demonstrate that SWIFT's run-time is reduced due to a large reduction in communication time per epoch, which falls by an order of magnitude compared to synchronous counterparts. Furthermore, SWIFT produces loss levels for image classification, over IID and non-IID data settings, upwards of 50% faster than existing SOTA algorithms.", "venue": "International Conference on Learning Representations", "year": 2022, "fieldsOfStudy": ["Computer Science", "Mathematics"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-10-25", "journal": {"name": "ArXiv", "volume": "abs/2210.14026"}, "authors": [{"authorId": "104055943", "name": "Marco Bornstein"}, {"authorId": "71520803", "name": "Tahseen Rabbani"}, {"authorId": "153581393", "name": "Evana Wang"}, {"authorId": "3387859", "name": "A. S. Bedi"}, {"authorId": "2117426487", "name": "Furong Huang"}], "citations": [{"paperId": "8ea73b7673c2ea8bb28453bb135654a60afdac8c", "title": "Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees"}, {"paperId": "3c076bc8f72ac423913883afddeced819604ff75", "title": "Spectral Co-Distillation for Personalized Federated Learning"}, {"paperId": "f14d119b612c57420f78d92e4777fe2016ae31f3", "title": "GNN-Based Neighbor Selection and Resource Allocation for Decentralized Federated Learning"}, {"paperId": "0f9ded926e889e35695e2028a059c96476fc8daa", "title": "Asynchronous SGD on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization"}, {"paperId": "b8913ddb4652158e7fe523a9f42a2254796dca52", "title": "Convergence Analysis of Decentralized ASGD"}, {"paperId": "57b3491dd7b2faacde921f5f7051c2c591682ea0", "title": "Decentralized SGD and Average-direction SAM are Asymptotically Equivalent"}, {"paperId": "f30e36102bfc342b5e5b4a699e9e9bea4e159714", "title": "T ACKLING THE D ATA H ETEROGENEITY IN A SYN - CHRONOUS F EDERATED L EARNING WITH C ACHED U P - DATE C ALIBRATION"}]}
