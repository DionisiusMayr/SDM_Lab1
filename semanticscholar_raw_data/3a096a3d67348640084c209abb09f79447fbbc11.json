{"paperId": "3a096a3d67348640084c209abb09f79447fbbc11", "publicationVenue": {"id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd", "name": "Neural Information Processing Systems", "type": "conference", "alternate_names": ["Neural Inf Process Syst", "NeurIPS", "NIPS"], "url": "http://neurips.cc/"}, "title": "Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset", "abstract": "One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take context into account. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a 256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.", "venue": "Neural Information Processing Systems", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-07-01", "journal": {"name": "ArXiv", "volume": "abs/2207.00220"}, "authors": [{"authorId": "153322217", "name": "Peter Henderson"}, {"authorId": "2053832474", "name": "M. Krass"}, {"authorId": "2118604716", "name": "Lucia Zheng"}, {"authorId": "2820009", "name": "Neel Guha"}, {"authorId": "144783904", "name": "Christopher D. Manning"}, {"authorId": "1746807", "name": "Dan Jurafsky"}, {"authorId": "2056459887", "name": "Daniel E. Ho"}], "citations": [{"paperId": "cc5cb7520eed9f6d4afb669ea40f38c5596e8900", "title": "FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning"}, {"paperId": "87f2db760cb864deb8c9f2cce93bd4dc2984969d", "title": "Large language models for automated Q&A involving legal documents: a survey on algorithms, frameworks and applications"}, {"paperId": "b51b468b1269b848e7458c654187795547b6250c", "title": "Rethinking Machine Learning Benchmarks in the Context of Professional Codes of Conduct"}, {"paperId": "13f44206745d20971ca271401eff6772aa80de80", "title": "SaulLM-7B: A pioneering Large Language Model for Law"}, {"paperId": "8a6f2e040e7180bd9574f0297305ce580f3c215d", "title": "An archival perspective on pretraining data"}, {"paperId": "7b00cb1fe1773f964d123dab6d4812c7bc63de06", "title": "Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems"}, {"paperId": "4baded2905a0b7aebbd061d7298c2e6947067028", "title": "The Right Model for the Job: An Evaluation of Legal Multi-Label Classification Baselines"}, {"paperId": "7d011d6a9e1704acc29bab88d616089089ea1006", "title": "PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation"}, {"paperId": "46f65f5aa8723f1b0a48242913ca6d85f0967acb", "title": "LaCour!: Enabling Research on Argumentation in Hearings of the European Court of Human Rights"}, {"paperId": "e36584cf5c53d19a2d2b888ee05cc2f7afd52693", "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation"}, {"paperId": "ec23c1a8e326bd55cc3d6bfcb9388d40a442ac5c", "title": "Walking a Tightrope \u2013 Evaluating Large Language Models in High-Risk Domains"}, {"paperId": "cd370a4b149a3ffcebc8f2973952e82f723a973b", "title": "Unifying Corroborative and Contributive Attributions in Large Language Models"}, {"paperId": "0ae7bc864a66feae9e5a189a9ac24cf4c19aa0dd", "title": "Large Language Models are legal but they are not: Making the case for a powerful LegalLLM"}, {"paperId": "3f13324da3856cba6f66ccbb81faa0fed4e78b28", "title": "A Search for Prompts: Generating Structured Answers from Contracts"}, {"paperId": "282c568302701bc163d454702eae10e43ca784a3", "title": "The future landscape of large language models in medicine"}, {"paperId": "9673b01f61d37b76511120cff6b8d4c4a460eefd", "title": "Automatic Anonymization of Swiss Federal Supreme Court Rulings"}, {"paperId": "2f4bfc41704a090138afa4174c461896d5d9c98f", "title": "Adapting Large Language Models via Reading Comprehension"}, {"paperId": "a11c695b34bb7f5cc8e6e96d8e6e037229394514", "title": "Security and Privacy on Generative Data in AIGC: A Survey"}, {"paperId": "cfb7948c8a09d0a64afecceb7efe3362318dbe17", "title": "SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore"}, {"paperId": "4b474c1f42eefbf14ca85c951f2a22ce031b6cb7", "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models"}, {"paperId": "9f4017de7deded49c032a83d7844efcd9ea1aa21", "title": "Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification"}, {"paperId": "9b2a802cccdbba0cee4d98ee9fd32301c298d0d7", "title": "Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators"}, {"paperId": "121a7077db9314d63cc9ef5718e196ff0d75ab92", "title": "BioSignal Copilot: Leveraging the power of LLMs in drafting reports for biomedical signals"}, {"paperId": "142e934dd5d6c53f877c30243d436255e3a0dde7", "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models"}, {"paperId": "3d50bc44a45a5d9ee98104e45c71c4f072a1d2cd", "title": "Legal Holding Extraction from Italian Case Documents using Italian-LEGAL-BERT Text Summarization"}, {"paperId": "67c6295d6af7fe79b5f7e0309e85cfc76495f123", "title": "SCALE: Scaling up the Complexity for Advanced Language Model Evaluation"}, {"paperId": "6f2b45846939267457abf13f2bc8618d23c7a2a4", "title": "LexGPT 0.1: pre-trained GPT-J models with Pile of Law"}, {"paperId": "80cee5037d01470edc7fbd20c564f2e1fc2c6b85", "title": "MultiLegalPile: A 689GB Multilingual Legal Corpus"}, {"paperId": "0fbf7ea1a3bd1754ed9aa12ed25906b731ece589", "title": "Training Data Extraction From Pre-trained Language Models: A Survey"}, {"paperId": "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c", "title": "Scaling Data-Constrained Language Models"}, {"paperId": "a3e9c959c64dbe967690a6103c0c6e8e2e73056d", "title": "Legal Extractive Summarization of U.S. Court Opinions"}, {"paperId": "2743ce22ace6362be2a01900106bf4b62280e22c", "title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development"}, {"paperId": "78f599fbd62dcc4a8dbab9d2f6056815dfc5b84c", "title": "The MiniPile Challenge for Data-Efficient Language Models"}, {"paperId": "98e6e0b3b811193e89b1a033da6c0a454220877a", "title": "Foundation Models and Fair Use"}, {"paperId": "2226d26497dc6b1c5922992c9b22ce08899379c0", "title": "Making a Computational Attorney"}, {"paperId": "554d59aa0f61edb9fa896fbf5cd574058dd0500a", "title": "LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain"}, {"paperId": "dbdc6909f46413a995fbaf6d331c41613c28da62", "title": "Large Language Models as Fiduciaries: A Case Study Toward Robustly Communicating With Artificial Intelligence Through Legal Standards"}, {"paperId": "15809750e5b9b6e56294f653db88579a59d4cb56", "title": "AI & Law: Formative Developments, State-of-the-Art Approaches, Challenges & Opportunities"}, {"paperId": "810e7ff45d5cdb2318f88a2787a37b2fadd9bf82", "title": "Legal-Tech Open Diaries: Lesson learned on how to develop and deploy light-weight models in the era of humongous Language Models"}, {"paperId": "44e02c4735e2e6cce3214e30bba1e30a92804bdd", "title": "Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey"}, {"paperId": "a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c", "title": "Incorporating Context into Subword Vocabularies"}, {"paperId": "557a5147d88ad361b807d4c93decac6d57d2d5e9", "title": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans"}, {"paperId": "9947a338f83bf1207cb7a5c8cf0810311fa66201", "title": "Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law"}, {"paperId": "a7589daf988106718f31e07bfdd043b7f24b1232", "title": "Extreme Classification of European Union Law Documents Driven by Entity Embeddings"}, {"paperId": "c838b50cd48ef4dbc941ed68b96c8ccb3b6dae65", "title": "Can we Pretrain a SotA Legal Language Model on a Budget From Scratch?"}, {"paperId": "7e8453fa32708b2690178c3a7a6618aba20928af", "title": "Training Socially Aligned Language Models in Simulated Human Society"}, {"paperId": "44d530c6ddd209babd6a54add41f941d8726cda2", "title": "Some Practical Analyses of the Judgment Documents of Labor Litigations for Social Conflicts and Similar Cases"}, {"paperId": "40c8771a5f450965992e12cd6d35a7ed2170e763", "title": "Automatic Rhetorical Roles Classification for Legal Documents using LEGAL-TransformerOverBERT"}, {"paperId": "472846ac997cee768ba365093d3de9ab8f5e0f21", "title": "Super-SCOTUS: A multi-sourced dataset for the Supreme Court of the US"}, {"paperId": "07a20548ac1eac67eff6a00ba71b4d78c6e58475", "title": "Pre-training Transformers on Indian Legal Text"}]}
