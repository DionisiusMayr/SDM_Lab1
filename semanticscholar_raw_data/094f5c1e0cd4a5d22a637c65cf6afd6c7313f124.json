{"paperId": "094f5c1e0cd4a5d22a637c65cf6afd6c7313f124", "publicationVenue": null, "title": "Examining the Effect of Pre-training on Time Series Classification", "abstract": "Although the pre-training followed by fine-tuning paradigm is used extensively in many fields, there is still some controversy surrounding the impact of pre-training on the fine-tuning process. Currently, experimental findings based on text and image data lack consensus. To delve deeper into the unsupervised pre-training followed by fine-tuning paradigm, we have extended previous research to a new modality: time series. In this study, we conducted a thorough examination of 150 classification datasets derived from the Univariate Time Series (UTS) and Multivariate Time Series (MTS) benchmarks. Our analysis reveals several key conclusions. (i) Pre-training can only help improve the optimization process for models that fit the data poorly, rather than those that fit the data well. (ii) Pre-training does not exhibit the effect of regularization when given sufficient training time. (iii) Pre-training can only speed up convergence if the model has sufficient ability to fit the data. (iv) Adding more pre-training data does not improve generalization, but it can strengthen the advantage of pre-training on the original data volume, such as faster convergence. (v) While both the pre-training task and the model structure determine the effectiveness of the paradigm on a given dataset, the model structure plays a more significant role.", "venue": "", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": null, "publicationDate": "2023-09-11", "journal": null, "authors": [{"authorId": "34260749", "name": "Jiashu Pu"}, {"authorId": "2000896394", "name": "Shiwei Zhao"}, {"authorId": "2239055014", "name": "Ling Cheng"}, {"authorId": "2152554888", "name": "Yongzhu Chang"}, {"authorId": "2239054423", "name": "Runze Wu"}, {"authorId": "80892810", "name": "Tangjie Lv"}, {"authorId": "48263731", "name": "Rongsheng Zhang"}], "citations": []}
