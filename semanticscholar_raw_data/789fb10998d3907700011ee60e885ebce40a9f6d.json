{"paperId": "789fb10998d3907700011ee60e885ebce40a9f6d", "publicationVenue": {"id": "7431ff67-91dc-41fa-b322-1b1ca657025f", "name": "International Conference on Information and Knowledge Management", "type": "conference", "alternate_names": ["Conference on Information and Knowledge Management", "Conf Inf Knowl Manag", "Int Conf Inf Knowl Manag", "CIKM"], "url": "http://www.cikm.org/"}, "title": "Trustworthy Experimentation Under Telemetry Loss", "abstract": "Failure to accurately measure the outcomes of an experiment can lead to bias and incorrect conclusions. Online controlled experiments (aka AB tests) are increasingly being used to make decisions to improve websites as well as mobile and desktop applications. We argue that loss of telemetry data (during upload or post-processing) can skew the results of experiments, leading to loss of statistical power and inaccurate or erroneous conclusions. By systematically investigating the causes of telemetry loss, we argue that it is not practical to entirely eliminate it. Consequently, experimentation systems need to be robust to its effects. Furthermore, we note that it is nontrivial to measure the absolute level of telemetry loss in an experimentation system. In this paper, we take a top-down approach towards solving this problem. We motivate the impact of loss qualitatively using experiments in real applications deployed at scale, and formalize the problem by presenting a theoretical breakdown of the bias introduced by loss. Based on this foundation, we present a general framework for quantitatively evaluating the impact of telemetry loss, and present two solutions to measure the absolute levels of loss. This framework is used by well-known applications at Microsoft, with millions of users and billions of sessions. These general principles can be adopted by any application to improve the overall trustworthiness of experimentation and data-driven decision making.", "venue": "International Conference on Information and Knowledge Management", "year": 2018, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2018-10-17", "journal": {"name": "Proceedings of the 27th ACM International Conference on Information and Knowledge Management"}, "authors": [{"authorId": "144087058", "name": "J. Gupchup"}, {"authorId": "2909839", "name": "Yasaman Hosseinkashi"}, {"authorId": "144690532", "name": "Pavel A. Dmitriev"}, {"authorId": "2064960498", "name": "D. Schneider"}, {"authorId": "145952419", "name": "Ross Cutler"}, {"authorId": "3280849", "name": "A. Jefremov"}, {"authorId": "2059005561", "name": "Martin Ellis"}], "citations": [{"paperId": "f4b04b832539ea1a43a0a24a57ecef69ad035d5b", "title": "A/B Testing: A Systematic Literature Review"}, {"paperId": "3214299518e70bd80e9ff05519225d6ee68bb463", "title": "Addressing Hidden Imperfections in Online Experimentation"}, {"paperId": "03fe5309d8ea349ed1aba77ba1e66ba5900856b8", "title": "The Cosmos Big Data Platform at Microsoft: Over a Decade of Progress and a Decade to Look Forward"}, {"paperId": "17f41c4d15abb3610397f219de2cd3526713afb3", "title": "Controlled Experimentation in Continuous Experimentation: Knowledge and Challenges"}, {"paperId": "5a7aff45406dd6a0e4ee511661d45048b8fe55a8", "title": "Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments"}, {"paperId": "11e36867f0720101584e7140249c7827377c634c", "title": "Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments"}, {"paperId": "e4fb476d00fccd144c16c83251ba33946fdb6769", "title": "Diagnosing Sample Ratio Mismatch in Online Controlled Experiments: A Taxonomy and Rules of Thumb for Practitioners"}, {"paperId": "dcd88ec67cdc19174062af6bc0a08fb77d47f548", "title": "Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments"}, {"paperId": "da2bbf485435d4ca5cbff1429bc815d7ade28de2", "title": "Experimentation in the Operating System: The Windows Experimentation Platform"}, {"paperId": "7004b3c087b20d91994304259c164067f5633168", "title": "Three Key Checklists and Remedies for Trustworthy Analysis of Online Controlled Experiments at Scale"}, {"paperId": "2640a31a4568d2f333b52bb28936615112ca330c", "title": "A Framework for Tunable Anomaly Detection"}]}
