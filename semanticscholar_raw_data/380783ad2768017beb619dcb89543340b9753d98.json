{"paperId": "380783ad2768017beb619dcb89543340b9753d98", "publicationVenue": null, "title": "Various Approaches Proposed for Eliminating Duplicate Data in a System", "abstract": "Resume The growth of big data processing market led to an increase in the overload of computation data centers, change of methods used in storing the data, communication between the computing units and computational time needed to process or edit the data. Methods of distributed or parallel data processing brought new problems related to computations with data which need to be examined. Unlike the conventional cloud services, a tight connection between the data and the computations is one of the main characteristics of the big data services. The computational tasks can be done only if relevant data are available. Three factors, which influence the speed and efficiency of data processing are - data duplicity, data integrity and data security. We are motivated to study the problems related to the growing time needed for data processing by optimizing these three factors in geographically distributed data centers.", "venue": "", "year": 2021, "fieldsOfStudy": null, "publicationTypes": null, "publicationDate": "2021-10-01", "journal": {"name": "Communications", "volume": "23"}, "authors": [{"authorId": "1395918425", "name": "Roman Ceresn\u00e1k"}, {"authorId": "2877394", "name": "K. Matia\u0161ko"}, {"authorId": "40866509", "name": "A. Dud\u00e1\u0161"}], "citations": []}
