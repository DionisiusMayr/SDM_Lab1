{"paperId": "6f75e8b61f13562237851d8119cb2f9d49e073fb", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Can LLM-Generated Misinformation Be Detected?", "abstract": "The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-25", "journal": {"name": "ArXiv", "volume": "abs/2309.13788"}, "authors": [{"authorId": "2163546329", "name": "Canyu Chen"}, {"authorId": "145800151", "name": "Kai Shu"}], "citations": [{"paperId": "d67d3e31ce219a82eb6315295f60ab663edebc96", "title": "Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations"}, {"paperId": "681ec8743f214b299f801359c8375eae90c833f9", "title": "\"I'm categorizing LLM as a productivity tool\": Examining ethics of LLM use in HCI research practices"}, {"paperId": "038ed1e52bdf92a4db0c91d31d1db28b2c5051fb", "title": "Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges"}, {"paperId": "b78adeffd4ab17a690e537ada53ddb85940f1015", "title": "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games"}, {"paperId": "c25f5776bc550886175361cc25962c401caaa5cc", "title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art"}, {"paperId": "8f02e0ac92a0b34360eb12105f4b8d98ab91d45a", "title": "Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation (preprint)"}, {"paperId": "144399b7708c0e15b8f067f7635df173a7067905", "title": "Bypassing LLM Watermarks with Color-Aware Substitutions"}, {"paperId": "ab8e6df5001dbb9b48445220099425aff536b3e8", "title": "Knowledge Conflicts for LLMs: A Survey"}, {"paperId": "03cfdde24c6b9837ad8933cb535a2c4a7c0fd971", "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild"}, {"paperId": "04cda88826c63dcd7d19597dfad6b7bd2ae41530", "title": "A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization"}, {"paperId": "6610c729cf132a22101660f2a96beafddc709de9", "title": "Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement"}, {"paperId": "a6f7485dfdf45320e82d84bcfdc51bcd52dff18b", "title": "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"}, {"paperId": "44d0c9a483b0af2f3952ae9acdde3d091472bc69", "title": "Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions"}, {"paperId": "b5142d7f7282be6aec15c1d30091eeb442268bbf", "title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models"}, {"paperId": "2df14dafc8486ff51575f8327159da1a021054b5", "title": "Large Language Models for Data Annotation: A Survey"}, {"paperId": "05e0c57f912cec9597021855bac28306c97e36fd", "title": "Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content"}, {"paperId": "de6ddb30b07f192f2be142062c4c6c817e508d96", "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation"}, {"paperId": "00af36ae2b615a8300348386052e38f4ddeb32e3", "title": "DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection"}, {"paperId": "1da88ffd32eec5eac30d8b2235a6065c1bd7896d", "title": "Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale"}, {"paperId": "8a9ede4be8458629d54451d241a8d0ae318d471b", "title": "Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception"}, {"paperId": "6ae2a428d149a71e0d7bf747dee97aad21eb1551", "title": "Can Large Language Models Detect Rumors on Social Media?"}, {"paperId": "6e2704025046be6dc29b71339994422f9a9cacf1", "title": "Building Guardrails for Large Language Models"}, {"paperId": "614639931a1598e8a0eca39b595a792ff42a0059", "title": "What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection"}, {"paperId": "12ed45473dee6d0917f8577157cb86952cb162ce", "title": "Detecting Multimedia Generated by Large AI Models: A Survey"}, {"paperId": "62ac45c894b38a9ec6ca089d1bea292281089d04", "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection"}, {"paperId": "ac2a659cc6f0e3635a9c1351c9963b47817205fb", "title": "Exploiting Novel GPT-4 APIs"}, {"paperId": "9cd0837a6204e62e0819fbd9238a4e41b18482aa", "title": "The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation"}, {"paperId": "3686311263da911309233e93a271f44f2a1f1c3a", "title": "A Survey of Text Watermarking in the Era of Large Language Models"}, {"paperId": "74b0976a3a7b7013fd468a043a940dcf401e66f1", "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions"}, {"paperId": "383c598625110e0a4c60da4db10a838ef822fbcf", "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"}, {"paperId": "da89cdeb0014666f4024f797d0c67cd45d92a7c9", "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity"}, {"paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2", "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"}, {"paperId": "5f017207d4667ee45c1161978a2ece62bcd11ae4", "title": "Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks"}, {"paperId": "893bc4c9d3be06319db7f31b6491110483db5fd1", "title": "A Semantic Invariant Robust Watermark for Large Language Models"}, {"paperId": "080e00c681e0e2aebb72758233a1d2bfe64b6930", "title": "Generative AI in the Classroom: Can Students Remain Active Learners?"}]}
