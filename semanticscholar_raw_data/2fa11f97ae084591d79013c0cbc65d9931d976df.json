{"paperId": "2fa11f97ae084591d79013c0cbc65d9931d976df", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels", "abstract": "Deployed dialogue agents have the potential to integrate human feedback to continuously improve themselves. However, humans may not always provide explicit signals when the chatbot makes mistakes during interactions. In this work, we propose Juicer, a framework to make use of both binary and free-form textual human feedback. It works by: (i) extending sparse binary feedback by training a satisfaction classifier to label the unlabeled data; and (ii) training a reply corrector to map the bad replies to good ones. We find that augmenting training with model-corrected replies improves the final dialogue model, and we can further improve performance by using both positive and negative replies through the recently proposed Director model.", "venue": "arXiv.org", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-10-28", "journal": {"name": "ArXiv", "volume": "abs/2210.15893"}, "authors": [{"authorId": "8299781", "name": "Weiyan Shi"}, {"authorId": "31461304", "name": "Emily Dinan"}, {"authorId": "35752280", "name": "Kurt Shuster"}, {"authorId": "145183709", "name": "J. Weston"}, {"authorId": "2155954521", "name": "Jing Xu"}], "citations": [{"paperId": "647d64679501b9c161d39f9872d255b8dec95def", "title": "DUnE: Dataset for Unified Editing"}, {"paperId": "d0b5d78e562e426519469e1f4ec84ee3b34e6fbe", "title": "What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception"}, {"paperId": "ad8182bfb70b3b67a60571ba720cdfc00b8e1392", "title": "Constructive Large Language Models Alignment with Diverse Feedback"}, {"paperId": "b931b242f40a032b9ae7dae9d9fc10c6ab90695e", "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models"}, {"paperId": "2dffb901382c6055a4d6bc9d07f4e9f6ae0e520e", "title": "Leveraging Implicit Feedback from Deployment Data in Dialogue"}, {"paperId": "16342fd115398bb1bcf2f16baf94f9d4d122d480", "title": "Let Me Teach You: Pedagogical Foundations of Feedback for Language Models"}, {"paperId": "d1fb9013ed41c67b18755f01f0b7c7c5ef0a047f", "title": "System-Level Natural Language Feedback"}, {"paperId": "0b6edce3dde7e502c6b7c6d83bac0230ec912482", "title": "Improving Open Language Models by Learning from Organic Interactions"}, {"paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc", "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"}, {"paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa", "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"}, {"paperId": "ebf35cef5c249d90b40043fffa41f8802c27f132", "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs"}, {"paperId": "c11810fa8887b678facea62da4607c4898360308", "title": "Training Language Models with Language Feedback at Scale"}, {"paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654", "title": "Constitutional AI: Harmlessness from AI Feedback"}, {"paperId": "a96762ae0ac80206f33657d2941beae41c09b16b", "title": "MERCY: Multiple Response Ranking Concurrently in Realistic Open-Domain Conversational Systems"}]}
