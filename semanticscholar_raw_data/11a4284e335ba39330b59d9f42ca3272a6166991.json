{"paperId": "11a4284e335ba39330b59d9f42ca3272a6166991", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "abstract": "Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-09-27", "journal": {"name": "ArXiv", "volume": "abs/2309.15402"}, "authors": [{"authorId": "2162359198", "name": "Zheng Chu"}, {"authorId": "2218647765", "name": "Jingchang Chen"}, {"authorId": "1500384901", "name": "Qianglong Chen"}, {"authorId": "2248673673", "name": "Weijiang Yu"}, {"authorId": "2247838930", "name": "Tao He"}, {"authorId": "2256768984", "name": "Haotian Wang"}, {"authorId": "2247980601", "name": "Weihua Peng"}, {"authorId": "145111960", "name": "Ming Liu"}, {"authorId": "2247852651", "name": "Bing Qin"}, {"authorId": "2238862997", "name": "Ting Liu"}], "citations": [{"paperId": "d0255ce4c897bea6578955de64fc40324f991878", "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey"}, {"paperId": "74847531214ee1bc345b2147942e4ca4570fd56f", "title": "An AIC-based approach for articulating unpredictable problems in open complex environments"}, {"paperId": "7f96bb27a8fca35b1f7d02ee319a64be04114809", "title": "LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments"}, {"paperId": "5c62af6925858d159b2d05fcadbb75ff3a07b70a", "title": "ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis"}, {"paperId": "c54f9094414df460d505a7380fb3dd193860837d", "title": "Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research"}, {"paperId": "064cb9247995d12bc2d20a569c3e179f90955f42", "title": "Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process"}, {"paperId": "d2001a89304cfee821b6b7c1ba92199b68c1fa00", "title": "LLMs with Chain-of-Thought Are Non-Causal Reasoners"}, {"paperId": "6b15183fcf3516fb03cb40f4d29b9ed202b129c3", "title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization"}, {"paperId": "2bd09677fc5d39d8ace60a704b80ecffdb2a0bfe", "title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models"}, {"paperId": "e670b3a7e4e8af79b8cd69a0b303d89917750765", "title": "How Interpretable are Reasoning Explanations from Prompting Large Language Models?"}, {"paperId": "396ea10d3ab89da41d02693d7165c4b98ecbb5f3", "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey"}, {"paperId": "0fa49dc5da32d418d1ebcefe5dd7f6da8f5d4dd8", "title": "Bard, ChatGPT and 3DGPT: a scientometric analysis of generative AI tools and assessment of implications for mechanical engineering education"}, {"paperId": "25243632a6159c19db280e2f0064aa59562a518a", "title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models"}, {"paperId": "7d6168fbd3ed72f9098573007f4b8c2ec9e576b9", "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning"}, {"paperId": "b03460fbae0ea0356ea7d245a7804cba50ebdde7", "title": "Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks"}, {"paperId": "eff9d7ed06f30f121d30ee13802a11f172ef66f4", "title": "Demystifying Chains, Trees, and Graphs of Thoughts"}, {"paperId": "86c0128529a59ca815bfb0d27beb191760da31e7", "title": "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency"}, {"paperId": "0a8a776054a087118f4f9523994ef084b2b2469a", "title": "Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation"}, {"paperId": "78875987dc674fc556873df037cf114f04932e80", "title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment"}, {"paperId": "d13dc7f94991e4cf7fba70c340b1d9d36346f238", "title": "Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought"}, {"paperId": "0251bb95be75d472c8d5b873751615e7fe2feb1d", "title": "A Comprehensive Study of Knowledge Editing for Large Language Models"}, {"paperId": "1bd9466f0bb10d29a16f614943ec7823e13cb210", "title": "Mixed Distillation Helps Smaller Language Model Better Reasoning"}, {"paperId": "5ee871537ae51e7e2e93d2a70fff5d100649a655", "title": "Mathematical Language Models: A Survey"}, {"paperId": "e199b26c75328e5f918d8db1bf72fc7083db7d44", "title": "PROMISE: A Framework for Developing Complex Conversational Interactions (Technical Report)"}, {"paperId": "f37d1ef3c4fd85f608439d239306a3b3302e3add", "title": "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models"}, {"paperId": "6fa0677731184444df0e1fc8070938419cd6da47", "title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"}, {"paperId": "91e3906550821c4624146e6e87db36c3296e773a", "title": "Applications of Large Scale Foundation Models for Autonomous Driving"}, {"paperId": "f2abeec1256f80970827d60f0151c7a19f2dbe7a", "title": "Interpreting User Requests in the Context of Natural Language Standing Instructions"}, {"paperId": "a1cbe3e24cdcb49541ef5da4f82d9ebc45bb6261", "title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving"}, {"paperId": "6233b5863f9a0e8bacce47ce21bc3e81c09497bd", "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning"}, {"paperId": "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8", "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"}, {"paperId": "ee5e79a83b019d5a7e3ad55e6e39696aff67a5f2", "title": "Combating Misinformation in the Age of LLMs: Opportunities and Challenges"}, {"paperId": "ae16932164b3be704671f25af7989f2346a689a5", "title": "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis"}, {"paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"}]}
