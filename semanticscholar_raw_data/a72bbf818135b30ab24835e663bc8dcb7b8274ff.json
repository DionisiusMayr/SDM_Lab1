{"paperId": "a72bbf818135b30ab24835e663bc8dcb7b8274ff", "publicationVenue": {"id": "86c43745-31d9-4c1a-b33f-ce3aa0042dbb", "name": "USENIX Symposium on Operating Systems Design and Implementation", "type": "conference", "alternate_names": ["Oper Syst Des Implement", "Operating Systems Design and Implementation", "OSDI", "USENIX Symp Oper Syst Des Implement"]}, "title": "GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs", "abstract": "As the emerging trend of graph-based deep learning, Graph Neural Networks (GNNs) excel for their capability to generate high-quality node feature vectors (embeddings). However, the existing one-size-fits-all GNN implementations are insufficient to catch up with the evolving GNN architectures, the ever-increasing graph sizes, and the diverse node embedding dimensionalities. To this end, we propose \\textbf{GNNAdvisor}, an adaptive and efficient runtime system to accelerate various GNN workloads on GPU platforms. First, GNNAdvisor explores and identifies several performance-relevant features from both the GNN model and the input graph, and uses them as a new driving force for GNN acceleration. Second, GNNAdvisor implements a novel and highly-efficient 2D workload management, tailored for GNN computation to improve GPU utilization and performance under different application settings. Third, GNNAdvisor capitalizes on the GPU memory hierarchy for acceleration by gracefully coordinating the execution of GNNs according to the characteristics of the GPU memory structure and GNN workloads. Furthermore, to enable automatic runtime optimization, GNNAdvisor incorporates a lightweight analytical model for an effective design parameter search. Extensive experiments show that GNNAdvisor outperforms the state-of-the-art GNN computing frameworks, such as Deep Graph Library ($3.02\\times$ faster on average) and NeuGraph (up to $4.10\\times$ faster), on mainstream GNN architectures across various datasets.", "venue": "USENIX Symposium on Operating Systems Design and Implementation", "year": 2020, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-11", "journal": {"pages": "515-531"}, "authors": [{"authorId": "2115942346", "name": "Yuke Wang"}, {"authorId": "144525636", "name": "Boyuan Feng"}, {"authorId": "2108625274", "name": "Gushu Li"}, {"authorId": "3152086", "name": "Shuangchen Li"}, {"authorId": "143895326", "name": "Lei Deng"}, {"authorId": "1410066063", "name": "Yuan Xie"}, {"authorId": "119663869", "name": "Yufei Ding"}], "citations": [{"paperId": "a48d60f216c9c7714f14bd6196ee8dc4b3070269", "title": "GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System"}, {"paperId": "5d8d16b2518286351136e01da84bdb773726246c", "title": "GeoT: Tensor Centric Library for Graph Neural Network via Efficient Segment Reduction on GPU"}, {"paperId": "4771fd8f52cd038707a3c5d51c9c4e4a7ad59970", "title": "FlGan: GAN-Based Unbiased Federated Learning Under Non-IID Settings"}, {"paperId": "09a02e6a83bc43b5fa3a764ddf761972b2ebc529", "title": "A Survey on Spatio-Temporal Big Data Analytics Ecosystem: Resource Management, Processing Platform, and Applications"}, {"paperId": "610df78324239e5f5ed8810316a2dd9ad83c47ed", "title": "Engineering A Workload-balanced Push-Relabel Algorithm for Massive Graphs on GPUs"}, {"paperId": "f8efcf6aa53ab3f16cf52ecb3c1c9a7e992edd6f", "title": "HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning"}, {"paperId": "0a980180974561d3861ee298503426ddf1c3aa65", "title": "PruneGNN: Algorithm-Architecture Pruning Framework for Graph Neural Network Acceleration"}, {"paperId": "48c35e8cfa7ac7e24c448fb21154151097e4f184", "title": "Celeritas: Out-of-Core Based Unsupervised Graph Neural Network via Cross-Layer Computing 2024"}, {"paperId": "cc2854e988fbf1f1ac2825471c88b0e84e340343", "title": "ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on Multi-Core Processor"}, {"paperId": "85691014466d48647a01d8726c68d0e9258fc667", "title": "Single-GPU GNN Systems: Traps and Pitfalls"}, {"paperId": "ebc06a22fa91e03648bc38e3a02cf444d8d3785e", "title": "CUTTANA: Scalable Graph Partitioning for Faster Distributed Graph Databases and Analytics"}, {"paperId": "314934f8d952aa32f6368a0fe5b7595b38cf06f1", "title": "Synergistic Signals: Exploiting Co-Engagement and Semantic Links via Graph Neural Networks"}, {"paperId": "de2057a4615c6f2d04a5b179bffd0287670aebd3", "title": "Accelerating GNN Training by Adapting Large Graphs to Distributed Heterogeneous Architectures"}, {"paperId": "dcf1ff48cabcb0f7a3c8ffabb0f8afb9e2a97550", "title": "A Unified Engine for Accelerating GNN Weighting/Aggregation Operations, With Efficient Load Balancing and Graph-Specific Caching"}, {"paperId": "0d41b3d4841bd007edc4a2ef7bbdce11ca70d255", "title": "CoGNN: An Algorithm-Hardware Co-Design Approach to Accelerate GNN Inference With Minibatch Sampling"}, {"paperId": "8e17c3c9c8f112959a526a0279759a98dda94b40", "title": "FlexGM: An Adaptive Runtime System to Accelerate Graph Matching Networks on GPUs"}, {"paperId": "2a72d9a7f8a3c4740fc75b8e62a6648189fb8f19", "title": "Barad-dur: Near-Storage Accelerator for Training Large Graph Neural Networks"}, {"paperId": "d43d6e4bd9dca7ebfd0978ebfa29b40d1c4b08c0", "title": "DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks"}, {"paperId": "f0f84ea6f9d0c417614d205827fecbc238f82a58", "title": "TurboGNN: Improving the End-to-End Performance for Sampling-Based GNN Training on GPUs"}, {"paperId": "6988501587a310dca1fe0a6b5d3e188b26ff8f12", "title": "Redundancy-Free High-Performance Dynamic GNN Training with Hierarchical Pipeline Parallelism"}, {"paperId": "a7f4d825e3088da30a1908c02db001417b1faa97", "title": "A Multicore GNN Training Accelerator"}, {"paperId": "3f57f297eb80171f9c2a900d087cfcac943c4c1e", "title": "DGI: An Easy and Efficient Framework for GNN Model Evaluation"}, {"paperId": "68352ea2d6aba76afaaedd51727cd5f4dc7f4435", "title": "Exploiting On-Chip Heterogeneity of Versal Architecture for GNN Inference Acceleration"}, {"paperId": "54a863e3155637681e9330c04c7c1928c2d7455e", "title": "Serving Graph Neural Networks With Distributed Fog Servers for Smart IoT Services"}, {"paperId": "bd14e6ce4eeeed6206f291d8f763719abda3e5b7", "title": "SENSEi: Input-Sensitive Compilation for Accelerating GNNs"}, {"paperId": "894d61c709ec6f61899703458d90b09c663d7b11", "title": "A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware"}, {"paperId": "e89492751fa4ccd016ea2421f8b6b412f8f72b98", "title": "SPADE: A Flexible and Scalable Accelerator for SpMM and SDDMM"}, {"paperId": "902906c2f99f6ac09ff5476a40f64080a7bb24eb", "title": "MetaNMP: Leveraging Cartesian-Like Product to Accelerate HGNNs with Near-Memory Processing"}, {"paperId": "3229291b9b7e9a7200b543089e05a4442cbe6a9b", "title": "Scalable and Efficient Full-Graph GNN Training for Large Graphs"}, {"paperId": "5acf75e1ee88a9695230708c3d6b11512583fefa", "title": "DUCATI: A Dual-Cache Training System for Graph Neural Networks on Giant Graphs with the GPU"}, {"paperId": "8bf5d3643a9d2b637d6f57ac5512e9ca54a51dbf", "title": "TurboMGNN: Improving Concurrent GNN Training Tasks on GPU With Fine-Grained Kernel Fusion"}, {"paperId": "fa1e8a10ec22c5177a0290d4877d150a9b36dafe", "title": "The Evolution of Distributed Systems for Graph Neural Networks and Their Origin in Graph Processing and Deep Learning: A Survey"}, {"paperId": "548449b18b248f08f851d1ac7123b9d793c526c9", "title": "AdaptGear: Accelerating GNN Training via Adaptive Subgraph-Level Kernels on GPUs"}, {"paperId": "2b8c884e3e7a60bc058775445e8e24d25ef91d4a", "title": "Communication-Efficient Graph Neural Networks with Probabilistic Neighborhood Expansion Analysis and Caching"}, {"paperId": "1f63e99817aa069522d9bc1e2c86b1ded0d36cbe", "title": "GraphTensor: Comprehensive GNN-Acceleration Framework for Efficient Parallel Processing of Massive Datasets"}, {"paperId": "2aa619338235a63683809edf92a4b3d99eaead3d", "title": "Fast Sparse GPU Kernels for Accelerated Training of Graph Neural Networks"}, {"paperId": "23366f13155e836eb155fe6eeba589c1bf2eac93", "title": "GraphMetaP: Efficient MetaPath Generation for Dynamic Heterogeneous Graph Models"}, {"paperId": "6eea7fa1e628c47b16335034faf18e2fb3a01844", "title": "Optimizing Irregular Dense Operators of Heterogeneous GNN Models on GPU"}, {"paperId": "87d7c926ed572d9cb83c5e0a8153ef0799a5b501", "title": "MergePath-SpMM: Parallel Sparse Matrix-Matrix Algorithm for Graph Neural Network Acceleration"}, {"paperId": "9dadfd97013f649fbfa5641ef829cc1d575040e4", "title": "uGrapher: High-Performance Graph Operator Computation via Unified Abstraction for Graph Neural Networks"}, {"paperId": "46210e170045df3c0c50a17bb63e6de480d62f9d", "title": "FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training"}, {"paperId": "ebe8a3c68b88e7c2747f92414a59ba6e708f0670", "title": "PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs"}, {"paperId": "d4cdadb0355e7c8bad2a6040687ae4f9a9233937", "title": "DGI: Easy and Efficient Inference for GNNs"}, {"paperId": "823a52b2b770d4e198d4693b27e4cebb4822e5ad", "title": "Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry"}, {"paperId": "1e79e33c77b2d8eaf643af0e1f5003057d7356b2", "title": "Distributed Graph Neural Network Training: A Survey"}, {"paperId": "38621d7f2f792dd8025ef3e7dd0374fb7aacee5a", "title": "CoGNN: Efficient Scheduling for Concurrent GNN Training on GPUs"}, {"paperId": "4d468c7b4345fcd25d40ca67313a743e6e164444", "title": "HGL: Accelerating Heterogeneous GNN Training with Holistic Representation and Optimization"}, {"paperId": "5c03e7f2146a80d9917c9761e8337b810a467e99", "title": "Optimizing Random Access to Hierarchically-Compressed Data on GPU"}, {"paperId": "28173e3a23927c420e621934f01230aa158411fa", "title": "DGS: Communication-Efficient Graph Sampling for Distributed GNN Training"}, {"paperId": "17f3d9b84c21cfc313d5c11f9efba37605cade94", "title": "Software Systems Implementation and Domain-Specific Architectures towards Graph Analytics"}, {"paperId": "d590ad6e83c9aa5d4fcdff11d514f38f5c560107", "title": "An Adaptive Elastic Multi-model Big Data Analysis and Information Extraction System"}, {"paperId": "2f81ccbe5b697ba53cf0fa00dcb6a7fc4b10b092", "title": "Optimizing Aggregate Computation of Graph Neural Networks with on-GPU Interpreter-Style Programming"}, {"paperId": "0205ba7bb590bf9ee5fde1232c003fa413f159e8", "title": "Faith: An Efficient Framework for Transformer Verification on GPUs"}, {"paperId": "0a942d68964ba104cfd49b675c976c1a1dbb65d1", "title": "MG-GCN: A Scalable multi-GPU GCN Training Framework"}, {"paperId": "615ef86f2f817db633bac029fd4bc412ac3102e1", "title": "Analysis and Optimization of GNN-Based Recommender Systems on Persistent Memory"}, {"paperId": "6c7d5db71d5c274222804fefa40f2a4876180fba", "title": "Multi-Node Acceleration for Large-Scale GCNs"}, {"paperId": "c142bfce3b6849ee9d6aa0d5637ab28b170fd507", "title": "SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning"}, {"paperId": "ebf774622c5761bf67a84c87c9c2f71b5ee754ac", "title": "DynaGraph: dynamic graph neural networks at scale"}, {"paperId": "e08e552c62b40c7ee2ec660b365d11956ef1d40d", "title": "NeutronStar: Distributed GNN Training with Hybrid Dependency Management"}, {"paperId": "bda02351d387bc63265f9e5671a69be8d386272c", "title": "Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization"}, {"paperId": "d28559e225dbde4b6da971a09ef6c5bc0ae833b1", "title": "SmartSAGE: training large-scale graph neural networks using in-storage processing architectures"}, {"paperId": "3512b4dac371a92a82b13c30758e389dab2c9b67", "title": "Accelerating Backward Aggregation in GCN Training With Execution Path Preparing on GPUs"}, {"paperId": "740b9d4414a955a74c16f5f3617d2b5dde2e9adf", "title": "GNNLab: a factored system for sample-based GNN training over GPUs"}, {"paperId": "20dc2612c0296952a363f2aa5d78d2178503553f", "title": "Rethinking graph data placement for graph neural network training on multiple GPUs"}, {"paperId": "bff8a216e261d03c2e6938e1a91e807a979624fc", "title": "GROW: A Row-Stationary Sparse-Dense GEMM Accelerator for Memory-Efficient Graph Convolutional Neural Networks"}, {"paperId": "6a5637e617f2ef97fd8f059d6fcb46c567fd08dd", "title": "PaSca: A Graph Neural Architecture Search System under the Scalable Paradigm"}, {"paperId": "36b7419166e470d65a84dad4ce4aa7857d7573fe", "title": "MariusGNN: Resource-Efficient Out-of-Core Training of Graph Neural Networks"}, {"paperId": "3bec8b4017541b9be679eb75836de9c713968e22", "title": "HP-GNN: Generating High Throughput GNN Training Implementation on CPU-FPGA Heterogeneous Platform"}, {"paperId": "94f0823f8db5360972a7a68b453e28ddf9c4e992", "title": "BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing"}, {"paperId": "bfc1fbd304707adf8e55f18b0ac71c70549a52e7", "title": "Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective"}, {"paperId": "a94fd3ea065fb218a64cf52bacb4ef8123690f0a", "title": "Point-X: A Spatial-Locality-Aware Architecture for Energy-Efficient Graph-Based Point-Cloud Deep Learning"}, {"paperId": "8d93f1738790954941105d5631ad777c85fd08c1", "title": "Accelerating Training and Inference of Graph Neural Networks with Fast Sampling and Pipelining"}, {"paperId": "6604a29489fab65cc9b61a369759d96c781d2861", "title": "Sextans: A Streaming Accelerator for General-Purpose Sparse-Matrix Dense-Matrix Multiplication"}, {"paperId": "456b30f0154ff1689cabbd4a5fe05334d41c664c", "title": "GNNIE: GNN inference engine with load-balancing and graph-specific caching"}, {"paperId": "ca973af3058109cf10b6a37bbeebd132f6123dba", "title": "Marius++: Large-Scale Training of Graph Neural Networks on a Single Machine"}, {"paperId": "5013c374658ea532899b3a693aed5239d0e642b3", "title": "Benchmarking GNN-Based Recommender Systems on Intel Optane Persistent Memory"}, {"paperId": "7c6ed0ff29cb7298082527c4cba6f9869097a05b", "title": "Exploring Query Processing on CPU-GPU Integrated Edge Device"}, {"paperId": "dc65a8d92704a8d235822ca90779fc1dbd088bca", "title": "A Deep Learning Dataloader with Shared Data Preparation"}, {"paperId": "fe3d6c9a7e76a175ca90f7ce05beb9965f148f26", "title": "the Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation."}, {"paperId": "40f2b55a2fc836bb2b05d1f6f86e0300ec213e12", "title": "Graphiler: Optimizing Graph Neural Networks with Message Passing Data Flow Graph"}, {"paperId": "97d98a0cf1a3d229148d8d71cb256c63c084720f", "title": "Availability Attacks on Graph Neural Networks"}, {"paperId": "9de73bbcf64e800d300d89bcc8cbb1100616e8c6", "title": "Shugra: A Shuf\ufb02ing-Driven Graph Convolutional Network Accelerator"}, {"paperId": "2145129c795c7b7b36fb98c7aa3b91ccd2d398b8", "title": "This paper is included in the Proceedings of the 2022 USENIX Annual Technical Conference."}, {"paperId": "febbe50d95e27e99c46a0d431dffd371203e57a4", "title": "This paper is included in the Proceedings of the 2023 USENIX Annual Technical Conference."}]}
