{"paperId": "99eee47dad469790042b973bc7cb40cb63a660b1", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker", "abstract": "Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment w.r.t. the authority power for inciting harmfulness, we disclose a lightweight method, termed DeepInception, which can easily hypnotize LLM to be a jailbreaker. Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario. Empirically, our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open and closed-source LLMs like Falcon, Vicuna-v1.5, Llama-2, and GPT-3.5-turbo/4. Our investigation appeals to people to pay more attention to the safety aspects of LLMs and develop a stronger defense against their misuse risks. The code is publicly available at: https://github.com/tmlr-group/DeepInception.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-06", "journal": {"name": "ArXiv", "volume": "abs/2311.03191"}, "authors": [{"authorId": "2144461756", "name": "Xuan Li"}, {"authorId": "1768392566", "name": "Zhanke Zhou"}, {"authorId": "2143475848", "name": "Jianing Zhu"}, {"authorId": "2110069725", "name": "Jiangchao Yao"}, {"authorId": "2244770736", "name": "Tongliang Liu"}, {"authorId": "2261906733", "name": "Bo Han"}], "citations": [{"paperId": "55eed6f9ede6c4187d849224d61e60fda73a54df", "title": "EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models"}, {"paperId": "425d3c381dde8f576b6237a6d443b97880dd6bc2", "title": "Tastle: Distract Large Language Models for Automatic Jailbreak Attack"}, {"paperId": "97c17683dbd3bea6972b93f5ab8ebb208b344ee6", "title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs"}, {"paperId": "8b1378a728ac223309e5e4c6d2006654b2d469bf", "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models"}, {"paperId": "db68cc363587bf82acf5a373b68bbf8a6bc11ac9", "title": "Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue"}, {"paperId": "09e2391ab4c266f96c71cc379c1a7c7ddd40ee33", "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers"}, {"paperId": "6d5fe9fdf1df2dd42d5936adfaa23d3619f9634d", "title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy"}, {"paperId": "8832f073c4d5d7ae26d6b5252b00bf8d8531f2e6", "title": "LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study"}, {"paperId": "3283764bbbd7084c9e6995e20953dbf36b25a226", "title": "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!"}, {"paperId": "0a691e58a36cdcdaaf72294e88420f79e61e85c7", "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs"}, {"paperId": "2c6638f6c817b7dc94365e217138d5b60cc699fa", "title": "A Chinese Dataset for Evaluating the Safeguards in Large Language Models"}, {"paperId": "f2e88c26bc1ebdd4adc5f83ab56cb4276120745d", "title": "A StrongREJECT for Empty Jailbreaks"}, {"paperId": "490e815b3be11ba97631783d9ae946b8f8517fd6", "title": "Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues"}, {"paperId": "2139e414bdf6a5ea7ec4052d3f65a8d49991494b", "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"}, {"paperId": "e79671a83e25288fedd897e1c9e6152f70f7f52e", "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey"}, {"paperId": "b7ef6182f617ef3e7cc9682f562f794115a4c62c", "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability"}, {"paperId": "b0ada492ba48e85016cbbfd95ec7180fb7e79648", "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast"}, {"paperId": "1e718f7aca54bdbfa6223b2c758b6cfc91234c97", "title": "FedImpro: Measuring and Improving Client Update in Federated Learning"}, {"paperId": "f75f401f046d508753d6b207f3f19414f489bd08", "title": "Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia"}, {"paperId": "c44622a46fac5fcd6296358d8c0e06efc93a7767", "title": "A Cross-Language Investigation into Jailbreak Attacks in Large Language Models"}, {"paperId": "4fda99880cdbf8f178f01eb4c8dbdae7f959ea94", "title": "Red-Teaming for Generative AI: Silver Bullet or Security Theater?"}, {"paperId": "b843fd79f0ddfd1a3e5ff3bd182715429e28aa35", "title": "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering"}, {"paperId": "b137709522bc70b42b026cae192de2a45000b22e", "title": "MetaAID 2.5: A Secure Framework for Developing Metaverse Applications via Large Language Models"}, {"paperId": "89641466373aa9ce2976e3f384b0791a7bd0931c", "title": "Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs"}, {"paperId": "801d7ba75fc833aa76ce4863dc1f79e30ee0c23f", "title": "Forward-Backward Reasoning in Large Language Models for Mathematical Verification"}]}
