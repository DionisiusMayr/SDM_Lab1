{"paperId": "fa8ad38863ae7b96570e91a05d7fdf5e6d768c0c", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "RenAIssance: A Survey into AI Text-to-Image Generation in the Era of Large Model", "abstract": "Text-to-image generation (TTI) refers to the usage of models that could process text input and generate high fidelity images based on text descriptions. Text-to-image generation using neural networks could be traced back to the emergence of Generative Adversial Network (GAN), followed by the autoregressive Transformer. Diffusion models are one prominent type of generative model used for the generation of images through the systematic introduction of noises with repeating steps. As an effect of the impressive results of diffusion models on image synthesis, it has been cemented as the major image decoder used by text-to-image models and brought text-to-image generation to the forefront of machine-learning (ML) research. In the era of large models, scaling up model size and the integration with large language models have further improved the performance of TTI models, resulting the generation result nearly indistinguishable from real-world images, revolutionizing the way we retrieval images. Our explorative study has incentivised us to think that there are further ways of scaling text-to-image models with the combination of innovative model architectures and prediction enhancement techniques. We have divided the work of this survey into five main sections wherein we detail the frameworks of major literature in order to delve into the different types of text-to-image generation methods. Following this we provide a detailed comparison and critique of these methods and offer possible pathways of improvement for future work. In the future work, we argue that TTI development could yield impressive productivity improvements for creation, particularly in the context of the AIGC era, and could be extended to more complex tasks such as video generation and 3D generation.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-09-02", "journal": {"name": "ArXiv", "volume": "abs/2309.00810"}, "authors": [{"authorId": "2271095802", "name": "Fengxiang Bie"}, {"authorId": "2270553819", "name": "Yibo Yang"}, {"authorId": "2226690406", "name": "Zhongzhu Zhou"}, {"authorId": "2256976897", "name": "Adam Ghanem"}, {"authorId": "2257093455", "name": "Minjia Zhang"}, {"authorId": "2262242352", "name": "Zhewei Yao"}, {"authorId": "2129511744", "name": "Xiaoxia Wu"}, {"authorId": "2059083875", "name": "Connor Holmes"}, {"authorId": "38147257", "name": "Pareesa Ameneh Golnari"}, {"authorId": "2270218158", "name": "David A. Clifton"}, {"authorId": "2257185770", "name": "Yuxiong He"}, {"authorId": "2257210161", "name": "Dacheng Tao"}, {"authorId": "2243324798", "name": "S. Song"}], "citations": [{"paperId": "21ff703fdfc19bd9a0a2d5120070be427e14a369", "title": "TC4D: Trajectory-Conditioned Text-to-4D Generation"}, {"paperId": "007746ead6ff873738e008fca82dccc6baf76970", "title": "GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning"}]}
