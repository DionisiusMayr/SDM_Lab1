{"paperId": "9a741f33aa4d782639e1f81a7e9c341b58b6ed2a", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices", "abstract": "Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-03-19", "journal": {"name": "ArXiv", "volume": "abs/2403.12503"}, "authors": [{"authorId": "2290848807", "name": "Sara Abdali"}, {"authorId": "2290852185", "name": "Richard Anarfi"}, {"authorId": "103032630", "name": "C. Barberan"}, {"authorId": "2290712336", "name": "Jia He"}], "citations": []}
