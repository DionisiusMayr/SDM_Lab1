{"paperId": "edd7da08668aab41409b073170de7789c8bd7a9f", "publicationVenue": {"id": "ffe5bb5c-04ed-488e-985d-d3a7b39542cf", "name": "IEEE International Conference on Distributed Computing Systems", "type": "conference", "alternate_names": ["International Conference on Distributed Computing Systems", "IEEE Int Conf Distrib Comput Syst", "Int Conf Device Circuit Syst", "ICDCS", "Int Conf Distrib Comput Syst", "International Conference on Devices, Circuits and Systems"], "url": "https://ieeexplore.ieee.org/xpl/conhome/1000213/all-proceedings"}, "title": "Falcon: Towards Computation-Parallel Deep Learning in Heterogeneous Parameter Server", "abstract": "Parameter server paradigm has shown great performance superiority for handling deep learning (DL) applications. One crucial issue in this regard is the presence of stragglers, which significantly retards DL training progress. Previous solutions for solving straggler may not fully exploit the computation capacity of a cluster as evidenced by our experiments. This motivates us to make an attempt at building a new parameter server architecture that mitigates and addresses stragglers in heterogeneous DL from the perspective of computation parallelism. We introduce a novel methodology named straggler projection to give a comprehensive inspection of stragglers and reveal practical guidelines for resolving this problem: (1) reducing straggler emergence frequency via elastic parallelism control and (2) transferring blocked tasks to pioneer workers for fully exploiting cluster computation capacity. Following the guidelines, we propose the abstraction of parallelism as an infrastructure and elaborate the Elastic-Parallelism Synchronous Parallel (EPSP) that supports both enforced-and slack-synchronization schemes. The whole idea has been implemented in a prototype called Falcon which efficiently accelerates the DL training progress with the presence of stragglers. Evaluation under various benchmarks with baseline comparison evidences the superiority of our system. Specifically, Falcon yields shorter convergence time, by up to 61.83%, 55.19%, 38.92% and 23.68% reduction over FlexRR, Sync-opt, ConSGD and DynSGD, respectively.", "venue": "IEEE International Conference on Distributed Computing Systems", "year": 2019, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-07-01", "journal": {"name": "2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)", "pages": "196-206"}, "authors": [{"authorId": "1993679419", "name": "Qihua Zhou"}, {"authorId": "28014924", "name": "Kun Wang"}, {"authorId": "144123438", "name": "Song Guo"}, {"authorId": "1400301067", "name": "Haodong Lu"}, {"authorId": "40108968", "name": "Li Li"}, {"authorId": "1697293", "name": "M. Guo"}, {"authorId": "1759469", "name": "Yanfei Sun"}], "citations": [{"paperId": "4a09fdb268a342198279a74ddfe1ed7150e89e20", "title": "PS+: A Simple yet Effective Framework for Fast Training on Parameter Server"}, {"paperId": "8b7a8af8427975dbaa15e6417baa0078a341f5a5", "title": "Inverse order based optimization method for task offloading and resource allocation in mobile edge computing"}, {"paperId": "0f97975b54687bb1f20683dc62de4c3ee7991be5", "title": "QoE-Based Task Offloading With Deep Reinforcement Learning in Edge-Enabled Internet of Vehicles"}, {"paperId": "7b39b5f1e7b0822a627b1b2c3a37dbdeaa383f0e", "title": "GSSP: Eliminating Stragglers Through Grouping Synchronous for Distributed Deep Learning in Heterogeneous Cluster"}, {"paperId": "36462c2979252e6aeea17f3e4eaa6c1df862987e", "title": "Falcon: Addressing Stragglers in Heterogeneous Parameter Server Via Multiple Parallelism"}, {"paperId": "18c891d2b1598c818710fda44b29bf1bd2557153", "title": "BaPipe: Exploration of Balanced Pipeline Parallelism for DNN Training"}, {"paperId": "7af8e01a8e262d4f4b7400b04d0e70e3aca6cef5", "title": "Elan: Towards Generic and Efficient Elastic Training for Deep Learning"}, {"paperId": "8ce0bf1b322e127e2b0c21c5476f9097843a4b91", "title": "Towards Mitigating Straggler with Deep Reinforcement Learning in Parameter Server"}, {"paperId": "2dc3554ac4405767c1911129a39ae75837fe87de", "title": "Exploiting Computation Reuse in Cloud-Based Deep Learning via Input Reordering"}, {"paperId": "33cb916f17f1124e0e749830a850f8998dbd2eac", "title": "Edge QoE: Computation Offloading With Deep Reinforcement Learning for Internet of Things"}, {"paperId": "b766b139443c49dd35b5995ad2c70b61e96f51dd", "title": "Intelligent VNF Orchestration and Flow Scheduling via Model-Assisted Deep Reinforcement Learning"}, {"paperId": "3c2c2093a9211e0c17d291cfbeb9682fdaf4de76", "title": "Fast Coflow Scheduling via Traffic Compression and Stage Pipelining in Datacenter Networks"}, {"paperId": "47eea9d53b84ffbd127ff93f7996594a2444be6d", "title": "Topology-Aware Job Scheduling for Machine Learning Cluster"}, {"paperId": "81d5467dbcb37233864d7416251929702fb4ab1f", "title": "Efficient Gradient Descent via Value Staleness Analysis for Heterogeneous Deep Learning Systems"}, {"paperId": "cf8f41da8e970e5773e27ebb3a8933123f1ce589", "title": "Metric Learning-Based Multi-Instance Multi-Label Classification With Label Correlation"}]}
