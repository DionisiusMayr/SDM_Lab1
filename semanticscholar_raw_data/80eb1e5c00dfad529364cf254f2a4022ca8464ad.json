{"paperId": "80eb1e5c00dfad529364cf254f2a4022ca8464ad", "publicationVenue": {"id": "e2742a49-1b55-4a31-b319-7455fa5e12d4", "name": "IEEE Transactions on Services Computing", "type": "journal", "alternate_names": ["IEEE Trans Serv Comput"], "issn": "1939-1374", "url": "http://www.computer.org/tsc", "alternate_urls": ["http://ieeexplore.ieee.org/servlet/opac?punumber=4629386"]}, "title": "NBSync: Parallelism of Local Computing and Global Synchronization for Fast Distributed Machine Learning in WANs", "abstract": "Recently, due to privacy concerns, distributed machine learning in Wide-Area Networks (DML-WANs) attracts increasing attention and has been widely deployed to promote the widespread application of intelligence services that rely on geographically distributed data. DML-WANs is essentially performing collaboratively federated learning over a combination of servers at both edge and cloud on a large spatial scale. However, efficient model training is challenging for DML-WANs because it is blocked by the high overhead of model parameter synchronization between computing servers over WANs. The reason is that there has a sequential dependency between local model computing and global model synchronization of traditional DML-WANs training methods intrinsically producing a sequential blockage between them, e.g., FedAvg. When the computing heterogeneity and the low WAN bandwidth coexist, a long block of global model synchronization prolongs the training time and leads to low utilization of local computing. Despite many efforts on alleviating synchronization overhead with novel communication technologies and synchronization methods, they still use traditional training patterns with sequential dependency and thereby have very limited improvements, such as FedAsync and ESync. In this article, we propose NBSync, a novel training algorithm for DML-WANs, which greatly speeds up the model training by the parallelism of local computing and global synchronization. NBSync employs a well-designed pipelining scheme, which can properly relax the sequential dependency of local computing and global synchronization and process them in parallel so as to overlap their operating overhead in the time dimension. NBSync also realizes flexible, differentiated and dynamical local computing for workers to maximize the overlap ratio in dynamically heterogeneous training environments. Convergence analysis shows that the convergence rate of NBSync training process is asymptotically equal to that of SSGD, and NBSync has a better convergence efficiency. We implemented the prototype of NBSync based on a popular parameter server system, i.e., MXNET's PS-LITE library, and evaluate its performance on a DML-WANs testbed. Experimental results show that NBSync speeds up training about 1.43\u00d7\u20132.79\u00d7 than state-of-the-art distributed training algorithms (DTAs) in DML-WANs scenarios where computing heterogeneity and low WAN bandwidth coexist.", "venue": "IEEE Transactions on Services Computing", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-01", "journal": {"name": "IEEE Transactions on Services Computing", "pages": "4115-4127", "volume": "16"}, "authors": [{"authorId": "1739176388", "name": "Huaman Zhou"}, {"authorId": "41018841", "name": "Zonghang Li"}, {"authorId": "49514984", "name": "Hongfang Yu"}, {"authorId": "2064538", "name": "Long Luo"}, {"authorId": "143847421", "name": "Gang Sun"}], "citations": []}
