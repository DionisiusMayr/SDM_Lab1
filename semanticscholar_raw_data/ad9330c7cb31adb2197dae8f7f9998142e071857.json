{"paperId": "ad9330c7cb31adb2197dae8f7f9998142e071857", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Zero-shot cross-lingual transfer in instruction tuning of large language model", "abstract": "Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.", "venue": "arXiv.org", "year": 2024, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-22", "journal": {"name": "ArXiv", "volume": "abs/2402.14778"}, "authors": [{"authorId": "2258716783", "name": "Nadezhda Chirkova"}, {"authorId": "2841761", "name": "Vassilina Nikoulina"}], "citations": [{"paperId": "223cab066bd3b0352e01954736eaa1517f6db687", "title": "Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large Language Models?"}]}
