{"paperId": "ea6c0620f4d56faa76a8f99d8963ad77fa6fbdb8", "publicationVenue": {"id": "41bf9ed3-85b3-4c90-b015-150e31690253", "name": "Conference on Empirical Methods in Natural Language Processing", "type": "conference", "alternate_names": ["Empir Method Nat Lang Process", "Empirical Methods in Natural Language Processing", "Conf Empir Method Nat Lang Process", "EMNLP"], "url": "https://www.aclweb.org/portal/emnlp"}, "title": "Copyright Violations and Large Language Models", "abstract": "Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than {\\em verbatim} reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text. We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations. Code is at \\url{https://github.com/coastalcph/CopyrightLLMs}.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-10-20", "journal": {"name": "ArXiv", "volume": "abs/2310.13771"}, "authors": [{"authorId": "2106433317", "name": "Antonia Karamolegkou"}, {"authorId": "2261336713", "name": "Jiaang Li"}, {"authorId": "2116635928", "name": "Li Zhou"}, {"authorId": "2261279990", "name": "Anders Sogaard"}], "citations": [{"paperId": "4c641b4b177024943c5b3cc527bf58632e9a4f41", "title": "A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish"}, {"paperId": "35f4207ab52c6b710dcb510db65ab7a834ae7615", "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models"}, {"paperId": "ef22a02c7d490d831261ef559a45362aac53fa2b", "title": "Autonomous Intelligent Systems: From Illusion of Control to Inescapable Delusion"}, {"paperId": "0044140fdd4547a380b0b82052ae0b6ffd95216c", "title": "Eight Methods to Evaluate Robust Unlearning in LLMs"}, {"paperId": "a932b662645ab4a348c44c73bb81876cb415ae95", "title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data"}, {"paperId": "d0b02eb4f0d3efd884b49450efc88145bdf49abc", "title": "Rethinking Machine Unlearning for Large Language Models"}, {"paperId": "d349f4e924f6cdcbafce75f159437920495cd8a1", "title": "Black-Box Access is Insufficient for Rigorous AI Audits"}, {"paperId": "24185cef6cdbf428fd8cdda56a7ee578142f446f", "title": "Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications"}, {"paperId": "811a5ecd5161f1354e89e3d143e471fa6762efc7", "title": "On Copyright Risks of Text-to-Image Diffusion Models"}]}
