{"paperId": "1104d766527dead44a40532e8a89444d9cef5c65", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models", "abstract": "The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and they have persisted online for over 100 days. Our work sheds light on the severe and evolving threat landscape of jailbreak prompts. We hope our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-08-07", "journal": {"name": "ArXiv", "volume": "abs/2308.03825"}, "authors": [{"authorId": "2047395411", "name": "Xinyue Shen"}, {"authorId": "2111435173", "name": "Z. Chen"}, {"authorId": "144588806", "name": "M. Backes"}, {"authorId": "2117688523", "name": "Yun Shen"}, {"authorId": "1698138", "name": "Yang Zhang"}], "citations": [{"paperId": "ed6d26a21184878a5cbb4320a01105b5329e2ada", "title": "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"}, {"paperId": "2c4d2d889a1f0ff9598de829a001df11a95d3294", "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack"}, {"paperId": "a421692950ceeeec6a78d58a675dcbb05ff5c1b3", "title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs"}, {"paperId": "1528919e218a7408c6cfb78db5d90bd6a7759a0b", "title": "Exploring the Privacy Protection Capabilities of Chinese Large Language Models"}, {"paperId": "038ed1e52bdf92a4db0c91d31d1db28b2c5051fb", "title": "Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges"}, {"paperId": "e8f12eb28184dfd0b2e3de58636ca0e566db4a6d", "title": "AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue"}, {"paperId": "2c030ad4be327dc3447e23ad68c303714c55cf14", "title": "Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models"}, {"paperId": "0659e2a187f43c5fa86632f3003f8b0d13dac329", "title": "Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization"}, {"paperId": "21f8977648e25ce8b95020e6f01988af99209c82", "title": "A Safe Harbor for AI Evaluation and Red Teaming"}, {"paperId": "a8a0e0574b18787f03bd25e99124d0f4421922e6", "title": "AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks"}, {"paperId": "a6f7485dfdf45320e82d84bcfdc51bcd52dff18b", "title": "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"}, {"paperId": "0044140fdd4547a380b0b82052ae0b6ffd95216c", "title": "Eight Methods to Evaluate Robust Unlearning in LLMs"}, {"paperId": "1f9f25aad947030fe3206114fa2ac75e8b590515", "title": "Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing"}, {"paperId": "d0746668f85fcdbe00306bd7486a24f8750207d5", "title": "How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries"}, {"paperId": "4ebfb0589ba587d6912661c1fe1082db705476af", "title": "Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs"}, {"paperId": "61567da465bf0121d5b02fc6894e4cca59f7b5d3", "title": "Is the System Message Really Important to Jailbreaks in Large Language Models?"}, {"paperId": "d9f198541267870ae71087f120ea543ebc38d1c6", "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models"}, {"paperId": "f2e88c26bc1ebdd4adc5f83ab56cb4276120745d", "title": "A StrongREJECT for Empty Jailbreaks"}, {"paperId": "e10691827473d0d4e940112adf02a6eb9040de3a", "title": "Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization"}, {"paperId": "b7ef6182f617ef3e7cc9682f562f794115a4c62c", "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability"}, {"paperId": "5c204b2421d05b83d3c96a6c515cc03143073935", "title": "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models"}, {"paperId": "2cb1933e7159a7cd2cd759c322e1973e28868cc7", "title": "Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs"}, {"paperId": "02fe1c5eeb30cdf0a574b9c3e185f3a4b3b9d594", "title": "Comprehensive Assessment of Jailbreak Attacks Against LLMs"}, {"paperId": "aa6a03f3368cbb4a413f7e11650fb8a6a2b71de1", "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications"}, {"paperId": "b82ccc66c14f531a444c74d2a9a9d86a86a8be99", "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal"}, {"paperId": "c7830808c6342d673ebc318a179195018d3b5d11", "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models"}, {"paperId": "f1d384440d0f16b50d615ffbcc6234f65bb3ebd0", "title": "Conversation Reconstruction Attack Against GPT Models"}, {"paperId": "2d83b615f989c8d1e9860a8a2d82628c95e40d22", "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models"}, {"paperId": "6e2704025046be6dc29b71339994422f9a9cacf1", "title": "Building Guardrails for Large Language Models"}, {"paperId": "78fa00a8cdd113e63ed18285fcd1bb6e8b042785", "title": "An Early Categorization of Prompt Injection Attacks on Large Language Models"}, {"paperId": "88d59e31575f5b3dd88a2c2033b55f628c2adbc9", "title": "Weak-to-Strong Jailbreaking on Large Language Models"}, {"paperId": "de4dfb773ab455081e5fb1862e08f581c58d43bc", "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems"}, {"paperId": "ac2a659cc6f0e3635a9c1351c9963b47817205fb", "title": "Exploiting Novel GPT-4 APIs"}, {"paperId": "6d465be006615460d41060f9f5068d51fc1f46b1", "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models"}, {"paperId": "89641466373aa9ce2976e3f384b0791a7bd0931c", "title": "Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs"}, {"paperId": "383c598625110e0a4c60da4db10a838ef822fbcf", "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"}, {"paperId": "1909ad17042a0d2af93898d427df0cef9d4483e3", "title": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits"}, {"paperId": "b78b5ce5f21f46d8149824463f8eebd6103d49aa", "title": "FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"}, {"paperId": "937205cf51b7ee46aa2983c129b7f5d596ea8293", "title": "Can LLMs Follow Simple Rules?"}, {"paperId": "e25b4bdfc3f5a8293ea6cd687a0203e446594188", "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game"}, {"paperId": "60a6c0800753d05094ebac1169532081b4b52406", "title": "Large Language Models and Computer Security"}, {"paperId": "512b3097ffcd6afcde6b58da1e656d5d9a635678", "title": "BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B"}, {"paperId": "158b28d2a20914405dd9bad5b3df45075994fa44", "title": "From Chatbots to PhishBots? - Preventing Phishing scams created using ChatGPT, Google Bard and Claude"}, {"paperId": "a1cdae1444507912841305bc2fea2faa3b518f63", "title": "Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks"}, {"paperId": "34113ce993100c4cbb3b05c3414ce80da38380be", "title": "Privacy in Large Language Models: Attacks, Defenses and Future Directions"}, {"paperId": "ac27dd71af3ee93e1129482ceececbae7dd0d0e8", "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"}, {"paperId": "1a9f394b5b7f5bcdecee487174a3f4fc65d30e33", "title": "Multilingual Jailbreak Challenges in Large Language Models"}, {"paperId": "8cf9b49698fdb1b754df2556576412a7b44929f6", "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"}, {"paperId": "84b7c486c56bd3880cb8eb01de9ae90ba3ebdaed", "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models"}, {"paperId": "764fc56883bf83392cac99a7b5a264ac9fe2cdc5", "title": "Low-Resource Languages Jailbreak GPT-4"}, {"paperId": "ba015c5d3f5b44e36363b90070bb3301d21ae57e", "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?"}, {"paperId": "be2b0125f3161739c685e9f86d9fd49f9f6d99c8", "title": "Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI"}, {"paperId": "6f75e8b61f13562237851d8119cb2f9d49e073fb", "title": "Can LLM-Generated Misinformation Be Detected?"}, {"paperId": "84a36e19f9394f22b34f79756fa9628a795e02ea", "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset"}, {"paperId": "cd29c25c489562b409a60f83365f93f33ee1a0a1", "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"}, {"paperId": "9be0dea0d6b892a2162490fb02712efaf10c0c87", "title": "Demystifying RCE Vulnerabilities in LLM-Integrated Apps"}, {"paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e", "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models"}, {"paperId": "ac1788e9a168a6455beb6316f316950842297c11", "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities"}, {"paperId": "5690e35b8beab92a80055fe2530c29c24e495379", "title": "On the Adversarial Robustness of Multi-Modal Foundation Models"}, {"paperId": "6eb46737bf0ef916a7f906ec6a8da82a45ffb623", "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"}, {"paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce", "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models"}, {"paperId": "b5069352383579c6464d8e5ec34eab693c45f59a", "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets"}, {"paperId": "eced3be90dddcf2c1a2bcd8bd5ac609328ba0541", "title": "Das Bewusstsein der Maschinen \u2013 die Mechanik des Bewusstseins"}]}
