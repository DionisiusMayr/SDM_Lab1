{"paperId": "3f6da3a49aaf7a1600e12a326a36f226ee53b159", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Pipeline Parallelism for Inference on Heterogeneous Edge Computing", "abstract": "Deep neural networks with large model sizes achieve state-of-the-art results for tasks in computer vision (CV) and natural language processing (NLP). However, these large-scale models are too compute- or memory-intensive for resource-constrained edge devices. Prior works on parallel and distributed execution primarily focus on training -- rather than inference -- using homogeneous accelerators in data centers. We propose EdgePipe, a distributed framework for edge systems that uses pipeline parallelism to both speed up inference and enable running larger (and more accurate) models that otherwise cannot fit on single edge devices. EdgePipe achieves these results by using an optimal partition strategy that considers heterogeneity in compute, memory, and network bandwidth. Our empirical evaluation demonstrates that EdgePipe achieves $10.59\\times$ and $11.88\\times$ speedup using 16 edge devices for the ViT-Large and ViT-Huge models, respectively, with no accuracy loss. Similarly, EdgePipe improves ViT-Huge throughput by $3.93\\times$ over a 4-node baseline using 16 edge devices, which independently cannot fit the model in memory. Finally, we show up to $4.16\\times$ throughput improvement over the state-of-the-art PipeDream when using a heterogeneous set of devices.", "venue": "arXiv.org", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-10-28", "journal": {"name": "ArXiv", "volume": "abs/2110.14895"}, "authors": [{"authorId": "2113665991", "name": "Yang Hu"}, {"authorId": "2747057", "name": "Connor Imes"}, {"authorId": "2145743848", "name": "Xuanang Zhao"}, {"authorId": "2965493", "name": "Souvik Kundu"}, {"authorId": "2658716", "name": "P. Beerel"}, {"authorId": "2468342", "name": "S. Crago"}, {"authorId": "10979204", "name": "J. Walters"}], "citations": [{"paperId": "fdbe9b2fd738a25a4e3cb344d831d3709dec0b7b", "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization"}, {"paperId": "8a040d0d99fa248c415f0aa90a083123320bf21a", "title": "Offloading Machine Learning to Programmable Data Planes: A Systematic Survey"}, {"paperId": "178cfb0234e3d39fd6ea737c5e17838b02436a5a", "title": "Model-Distributed Inference in Multi-Source Edge Networks"}, {"paperId": "2f7c95f834b4e41d9f66843c109e1d57dccef49a", "title": "STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining"}, {"paperId": "e6b0603dd865b512cedddabc2acc6e8002125679", "title": "HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments"}, {"paperId": "a8b01c9b98bcd1df5145f9595a4a7572decb22ab", "title": "Adaptive and Resilient Model-Distributed Inference in Edge Computing Systems"}, {"paperId": "583669169d429bced61d3789cb75d13ae1409fa0", "title": "A Pipelining-Based Heterogeneous Scheduling and Energy-Throughput Optimization Scheme for CNNs Leveraging Apache TVM"}, {"paperId": "7849991e628f4a6b28bcebc6a676589f884383fc", "title": "Turbocharge Interactive NLP at the Edge"}, {"paperId": "bf2cb4d4ff212e351997faf8aefb40a1e34462a5", "title": "Efficient NLP Inference at the Edge via Elastic Pipelining"}]}
