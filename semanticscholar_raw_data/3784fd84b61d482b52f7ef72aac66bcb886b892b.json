{"paperId": "3784fd84b61d482b52f7ef72aac66bcb886b892b", "publicationVenue": {"id": "1901e811-ee72-4b20-8f7e-de08cd395a10", "name": "arXiv.org", "alternate_names": ["ArXiv"], "issn": "2331-8422", "url": "https://arxiv.org"}, "title": "Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models", "abstract": "Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}. To address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.", "venue": "arXiv.org", "year": 2023, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-06", "journal": {"name": "ArXiv", "volume": "abs/2310.03965"}, "authors": [{"authorId": "28822585", "name": "Junchi Yu"}, {"authorId": "2256994353", "name": "Ran He"}, {"authorId": "2248206514", "name": "Rex Ying"}], "citations": [{"paperId": "a41d4a3b005c8ec4f821e6ee96672d930ca9596c", "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering"}, {"paperId": "47f71a0aaf4a043d9f2c5d6d117acb73f7ad013a", "title": "Retrieval-Augmented Thought Process as Sequential Decision Making"}, {"paperId": "98aa313fbe30734eb3bb50683204765bcbc607eb", "title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning"}, {"paperId": "eff9d7ed06f30f121d30ee13802a11f172ef66f4", "title": "Demystifying Chains, Trees, and Graphs of Thoughts"}, {"paperId": "503c85a9df91de5dace92d6c5ade8627701f08ac", "title": "Large language model empowered participatory urban planning"}, {"paperId": "6514abaf6bdd1c31ac5dc4ad40760ff8422c6a4e", "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation"}, {"paperId": "5ee871537ae51e7e2e93d2a70fff5d100649a655", "title": "Mathematical Language Models: A Survey"}, {"paperId": "11a4284e335ba39330b59d9f42ca3272a6166991", "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"}]}
