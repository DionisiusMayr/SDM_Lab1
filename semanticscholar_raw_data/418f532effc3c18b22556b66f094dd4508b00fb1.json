{"paperId": "418f532effc3c18b22556b66f094dd4508b00fb1", "publicationVenue": {"id": "914d4734-285a-4cd8-bba1-eec0db26a512", "name": "International Conference on Supercomputing", "type": "conference", "alternate_names": ["International Conference Computer, Communication and Computational Sciences", "ICS", "INFORMS Comput Soc", "Int Conf Supercomput", "INFORMS Computing Society", "Int Conf Comput Commun Comput Sci"], "url": "http://portal.acm.org/proceedings/ics/"}, "title": "Enabling energy-efficient DNN training on hybrid GPU-FPGA accelerators", "abstract": "DNN training consumes orders of magnitude more energy than inference and requires innovative use of accelerators to improve energy-efficiency. However, despite having complementary features, GPUs and FPGAs have been mostly used independently for the entire training process, thus neglecting the opportunity in assigning individual but distinct operations to the most suitable hardware. In this paper, we take the initiative to explore new opportunities and viable solutions in enabling energy-efficient DNN training on hybrid accelerators. To overcome fundamental challenges including avoiding training throughput loss, enabling fast design space exploration, and efficient scheduling, we propose a comprehensive framework, Hype-training, that utilizes a combination of offline characterization, performance modeling, and online scheduling of individual operations. Experimental tests using NVIDIA V100 GPUs and Intel Stratix 10 FPGAs show that, Hype-training is able to exploit a mixture of GPUs and FPGAs at a fine granularity to achieve significant energy reduction, by 44.3% on average and up to 59.7%, without any loss in training throughput. Hype-training can also enforce power caps more effectively than state-of-the-art power management mechanisms on GPUs.", "venue": "International Conference on Supercomputing", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "Proceedings of the ACM International Conference on Supercomputing"}, "authors": [{"authorId": "2116554220", "name": "Xin He"}, {"authorId": "1491252080", "name": "Jiawen Liu"}, {"authorId": "2051649010", "name": "Zhen Xie"}, {"authorId": "2149051756", "name": "Hao Chen"}, {"authorId": "2321965", "name": "Guoyang Chen"}, {"authorId": "49039449", "name": "Weifeng Zhang"}, {"authorId": "48937589", "name": "Dong Li"}], "citations": [{"paperId": "afccd91c911a75814479dd2eda3ee8dae4089fa4", "title": "Novel adaptive quantization methodology for 8-bit floating-point DNN training"}, {"paperId": "8e3def9a7762b9a97e394b3d5b7e75b1f1802ce8", "title": "Two-Level Scheduling Algorithms for Deep Neural Network Inference in Vehicular Networks"}, {"paperId": "82eed40bf760a1fac01c99281ab86a6ce185a72e", "title": "PROFET: PROFiling-based CNN Training Latency ProphET for GPU Cloud Instances"}, {"paperId": "f15e78f86cdc57397c63f27fcefedaba8be3be6e", "title": "Spatial- and time- division multiplexing in CNN accelerator"}, {"paperId": "60f4350d44cba05d1e9adf990bda69257156473f", "title": "Flame: A Self-Adaptive Auto-Labeling System for Heterogeneous Mobile Processors"}, {"paperId": "ae4b8cd0a210ae7a42c11bfd54f16d0226c2225f", "title": "FARNN: FPGA-GPU Hybrid Acceleration Platform for Recurrent Neural Networks"}, {"paperId": "33b85beadd27bde621e0caa7d5558847ce0c68e9", "title": "This paper is included in the Proceedings of the 2022 USENIX Annual Technical Conference."}]}
