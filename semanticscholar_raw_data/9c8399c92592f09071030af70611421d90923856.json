{"paperId": "9c8399c92592f09071030af70611421d90923856", "publicationVenue": {"id": "d13e941e-4cac-4f1d-bdca-77d927e31f1b", "name": "ACM Symposium on Cloud Computing", "type": "conference", "alternate_names": ["System-on-Chip Conference", "ACM Symp Cloud Comput", "Syst Conf", "Symp Cloud Comput", "Annual IEEE International System-on-Chip Conference", "Symposium on Cloud Computing", "Annu IEEE Int Syst Conf", "SoCC"], "url": "http://www.ieee-socc.org/"}, "title": "tf.data service: A Case for Disaggregating ML Input Data Processing", "abstract": "Machine learning (ML) computations commonly execute on expensive specialized hardware, such as GPUs and TPUs, which provide high FLOPs and performance-per-watt. For cost efficiency, it is essential to keep these accelerators highly utilized. This requires preprocessing input data at the rate at which the accelerators can ingest and perform ML computations on the data. To avoid data stalls, the host CPU and RAM required for input data processing per accelerator core used for ML computations varies across jobs. Hence, the traditional approach of processing input data on ML accelerator hosts with a fixed hardware ratio leads to either under-utilizing the accelerators or the host CPU and RAM. In this paper, we address these concerns by building a disaggregated ML data processing system. We present tf.data service, an open-source disaggregated input data processing service built on top of tf.data in TensorFlow. We show that disaggregating data preprocessing has three key advantages for large-scale ML training jobs. First, the service can horizontally scale-out to right-size CPU/RAM host resources for data processing in each job, saving 32\u00d7 training time and 26\u00d7 cost, on average. Second, the service can share ephemeral preprocessed data results across jobs, to optimize CPU usage and reduce redundant computations. Finally, the service supports coordinated reads, a technique that avoids stragglers due to different input sizes in distributed training, reducing training time by 2.2\u00d7, on average. Our design is inspired by lessons learned from deploying tf.data service in production, including relaxing data visitation guarantees without impacting model accuracy.", "venue": "ACM Symposium on Cloud Computing", "year": 2022, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2022-10-26", "journal": {"name": "Proceedings of the 2023 ACM Symposium on Cloud Computing"}, "authors": [{"authorId": "31728717", "name": "Andrew Audibert"}, {"authorId": "40930325", "name": "Yangrui Chen"}, {"authorId": "2997351", "name": "D. Graur"}, {"authorId": "2285439911", "name": "Ana Klimovic"}, {"authorId": "38300863", "name": "Ji\u0159\u00ed \u0160im\u0161a"}, {"authorId": "1780447", "name": "C. Thekkath"}], "citations": [{"paperId": "fb0207353fe923efd26a4fedafde56cd8eda1173", "title": "Characterization of Large Language Model Development in the Datacenter"}, {"paperId": "398bb60aed5d51d2053349f4149c5f44257ff850", "title": "cedar: Composable and Optimized Machine Learning Input Data Pipelines"}, {"paperId": "2ec26b3f436c12511cd8deedd3cb0c1f13593a25", "title": "Lovelock: Towards Smart NIC-hosted Clusters"}, {"paperId": "e3f0f242e1cd93cb13d9ec552e50d254e4b2d627", "title": "GoldMiner: Elastic Scaling of Training Data Pre-Processing Pipelines for Deep Learning"}, {"paperId": "423d83256f500fa413da9d9d1af474078754ff05", "title": "Orion: Interference-aware, Fine-grained GPU Sharing for ML Applications"}]}
