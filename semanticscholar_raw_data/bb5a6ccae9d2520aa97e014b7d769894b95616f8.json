{"paperId": "bb5a6ccae9d2520aa97e014b7d769894b95616f8", "publicationVenue": null, "title": "Heterogeneity-Aware Distributed Machine Learning Training via Partial Reduce", "abstract": "All-reduce is the key communication primitive used in distributed data-parallel training due to the high performance in the homogeneous environment. However, All-reduce is sensitive to stragglers and communication delays as deep learning has been increasingly deployed on the heterogeneous environment like cloud. In this paper, we propose and analyze a novel variant of all-reduce, called partial-reduce, which provides high heterogeneity tolerance and performance by decomposing the synchronous all-reduce primitive into parallel-asynchronous partial-reduce operations. We provide theoretical guarantees, proving that partial-reduce converges to a stationary point at the similar sub-linear rate as distributed SGD. To enforce the convergence of the partial-reduce primitive, we further propose a dynamic staleness-aware distributed averaging algorithm and implement a novel group generation mechanism to prevent possible update isolation in heterogeneous environments. We build a prototype system in the real production cluster and validate its performance under different workloads. The experiments show that it is 1.21x-2x faster than other state-of-the-art baselines.", "venue": "SIGMOD Conference", "year": 2021, "fieldsOfStudy": ["Computer Science"], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2021-06-09", "journal": {"name": "Proceedings of the 2021 International Conference on Management of Data"}, "authors": [{"authorId": "1720763480", "name": "Xupeng Miao"}, {"authorId": "2113588952", "name": "Xiaonan Nie"}, {"authorId": "2237813", "name": "Yingxia Shao"}, {"authorId": "2109540175", "name": "Zhi Yang"}, {"authorId": "3452475", "name": "Jiawei Jiang"}, {"authorId": "2492241", "name": "Lingxiao Ma"}, {"authorId": "144585959", "name": "B. Cui"}], "citations": [{"paperId": "81b2834f3f884785e94a8c4c1b0bf66f858e0d97", "title": "The Image Calculator: 10x Faster Image-AI Inference by Replacing JPEG with Self-designing Storage Format"}, {"paperId": "10cb1af640c855c17dd9e66d34be91a92b0d3c72", "title": "ASHL: An Adaptive Multi-Stage Distributed Deep Learning Training Scheme for Heterogeneous Environments"}, {"paperId": "203278b671b1894d603451b41017e2ad227a3bd3", "title": "FusionFlow: Accelerating Data Preprocessing for Machine Learning with CPU-GPU Cooperation"}, {"paperId": "3adca9d49eab1a4db6538a995af3636d10e120c3", "title": "BladeDISC: Optimizing Dynamic Shape Machine Learning Workloads via Compiler Approach"}, {"paperId": "44154f6631e4c381b3a0381431ece6db26d1d5c3", "title": "Personalized Federated Learning via ADMM with Moreau Envelope"}, {"paperId": "70f23d12b37a9436ebde16e99d9fdd540ef5c7b6", "title": "Dynamic Worker Classification Scheme for Addressing Straggler Problem in Distributed Deep Learning Environments"}, {"paperId": "68f86181502bd523e6faeebde704873cdf154464", "title": "Flash-LLM: Enabling Low-Cost and Highly-Efficient Large Generative Model Inference With Unstructured Sparsity"}, {"paperId": "b8913ddb4652158e7fe523a9f42a2254796dca52", "title": "Convergence Analysis of Decentralized ASGD"}, {"paperId": "001ef084f3255f0bf6d89038dddde73cdd329d29", "title": "DeepBoot: Dynamic Scheduling System for Training and Inference Deep Learning Tasks in GPU Cluster"}, {"paperId": "5d2f9e0b2eb1119d2955515f4806af9e1f961c82", "title": "Communication-efficient ADMM-based distributed algorithms for sparse training"}, {"paperId": "76ebc99fd8d22b63b1695d399642358fc0cbaae8", "title": "Robust Fully-Asynchronous Methods for Distributed Training over General Architecture"}, {"paperId": "3e5741ee9cfd23c79d2af2e209ebb1b57da96e2a", "title": "Improving Automatic Parallel Training via Balanced Memory Workload Optimization"}, {"paperId": "e61462184a6dce9259e76c9069c5747a420b3c0d", "title": "SDPipe: A Semi-Decentralized Framework for Heterogeneity-aware Pipeline-parallel Training"}, {"paperId": "25f17c2ab2bd57c482980e350e39e6aba8fc31c0", "title": "SparDL: Distributed Deep Learning Training with Efficient Sparse Communication"}, {"paperId": "d7e00702bbb5a0cccc97033f0405b634ae9e2d3c", "title": "Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent"}, {"paperId": "3822cb0a089a66f2ad88e7960fcae6ebdc4e4427", "title": "DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining"}, {"paperId": "34e14fdf011987ecf8abac4dc18c776e1554c51c", "title": "FSP: Towards Flexible Synchronous Parallel Frameworks for Distributed Machine Learning"}, {"paperId": "1357a06dfcbe11f2ea57b67f49580f044906a01d", "title": "Hetu: a highly efficient automatic parallel distributed deep learning system"}, {"paperId": "531b64253e65bc49f029ee9bc77dbf498e074e33", "title": "Joint Dynamic Grouping and Gradient Coding for Time-Critical Distributed Machine Learning in Heterogeneous Edge Networks"}, {"paperId": "e2df6ae1b3485449364ce2a5356ab09600fc3632", "title": "Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism"}, {"paperId": "9e9d8a432748c97b56b5d217e9e2928ae732bf52", "title": "Poster: Selective Reduce for Heterogeneous Distributed Training"}, {"paperId": "d9f280deeb39686fde956a4b92389983fdb0304e", "title": "Efficient Partial Reduce Across Clouds"}, {"paperId": "67f5f2f960965d5d51f41469785125e083ae4317", "title": "MD-Roofline: A Training Performance Analysis Model for Distributed Deep Learning"}, {"paperId": "9ed73c7bcfe6cfeeeb8b63b3b6d92e991170b475", "title": "HET-GMP: A Graph-based System Approach to Scaling Large Embedding Model Training"}, {"paperId": "004149bf36ae931f5af717f388ea6753cf5ba4c2", "title": "BlindFL: Vertical Federated Machine Learning without Peeking into Your Data"}, {"paperId": "a013c498b400a5f8c7593068a90a30a259a4e565", "title": "N-FedAvg: Novel Federated Average Algorithm Based on FedAvg"}, {"paperId": "74cb59f7980fd298da53e46d95a4c2319be3bb07", "title": "Fast Parameter Synchronization for Distributed Learning with Selective Multicast"}, {"paperId": "ba377c911b1d601cbe6a7f10f89e5b6175adcd98", "title": "TSPLIT: Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting"}, {"paperId": "67c03d7a477059dc20faa02e3b45ca7055433615", "title": "HET-KG: Communication-Efficient Knowledge Graph Embedding Training via Hotness-Aware Cache"}, {"paperId": "39271db2458f6fb635e40372049cb1e203c7e05c", "title": "SANCUS: Staleness-Aware Communication-Avoiding Full-Graph Decentralized Training in Large-Scale Graph Neural Networks"}, {"paperId": "f39f94bbcbfda2d64d1b3ad1fe004e7a44290fa2", "title": "Finding Materialized Models for Model Reuse"}, {"paperId": "0759c30c37c2a2b868d0f768a9823a768db1e306", "title": "HET: Scaling out Huge Embedding Model Training via Cache-enabled Distributed Framework"}, {"paperId": "5b3f41652c4630477eba8bb19534109c37dc17a3", "title": "Decoupling the All-Reduce Primitive for Accelerating Distributed Deep Learning"}]}
